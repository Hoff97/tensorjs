{"version":3,"sources":["../../../../lib/util/shape.ts","../../../lib/types.ts","../../../../../lib/ops/util/conv.ts","../../../../../lib/ops/cpu/basic.ts","../../../../../lib/ops/util/convTranspose.ts","../../../../../lib/ops/util/pool.ts","../../../../../lib/ops/cpu/pool.ts","../../../../../lib/ops/cpu/pad.ts","../../../../../lib/tensor/cpu/tensor.ts","../../../../../lib/ops/cpu/setValues.ts","../../../../../lib/ops/cpu/matMul.ts","../../../../../lib/ops/cpu/gemm.ts","../../../../../lib/ops/cpu/sum.ts","../../../../../lib/ops/cpu/sumSquare.ts","../../../../../lib/ops/cpu/product.ts","../../../../../lib/ops/cpu/max.ts","../../../../../lib/ops/cpu/min.ts","../../../../../lib/ops/cpu/reduceMean.ts","../../../../../lib/ops/cpu/reduceMeanSquare.ts","../../../../../lib/ops/cpu/reduceLogSum.ts","../../../../../lib/ops/cpu/reduceLogSumExp.ts","../../../../../lib/ops/cpu/conv.ts","../../../../../lib/ops/cpu/convTranspose.ts","../../../../../lib/ops/cpu/averagePool.ts","../../../../../lib/ops/cpu/concat.ts","../../../../../lib/ops/cpu/transpose.ts","../../../../../lib/ops/cpu/repeat.ts","../../../../../lib/ops/cpu/expand.ts","../../../../../lib/ops/cpu/gather.ts","../../../../../lib/ops/cpu/slice.ts","../../../../../lib/ops/cpu/upsample.ts","../../../../../lib/ops/cpu/normalize.ts","../../../../../lib/util/datastructs/dict.ts","../../../../lib/util/math.ts","../../../../../lib/tensor/gpu/memory.ts","../../../../../lib/tensor/gpu/gl.ts","../../../../../lib/ops/gpu/operation.ts","../../../../../../lib/ops/gpu/matMul/matmul.ts","../../../../../../lib/ops/gpu/unary/unaryOperation.ts","../../../../../../lib/ops/gpu/unary/exp.ts","../../../../../../lib/ops/gpu/conv/conv.ts","../../../../../../lib/ops/gpu/unary/abs.ts","../../../../../../lib/ops/gpu/binary/binaryOperation.ts","../../../../../../lib/ops/gpu/binary/add.ts","../../../../../../lib/ops/gpu/binary/multiply.ts","../../../../../../lib/ops/gpu/binary/subtract.ts","../../../../../../lib/ops/gpu/binary/divide.ts","../../../../../../lib/ops/gpu/conv/averagePool.ts","../../../../../../lib/ops/gpu/pool/pool.ts","../../../../../../lib/ops/gpu/pool/reduceMean.ts","../../../../../../lib/ops/gpu/pool/reduceMeanSquare.ts","../../../../../../lib/ops/gpu/pool/sumSquare.ts","../../../../../../lib/ops/gpu/pool/sum.ts","../../../../../../lib/ops/gpu/pool/product.ts","../../../../../../lib/ops/gpu/pool/max.ts","../../../../../../lib/ops/gpu/pool/min.ts","../../../../../../lib/ops/gpu/unary/ceil.ts","../../../../../../lib/ops/gpu/unary/clip.ts","../../../../../../lib/ops/gpu/unary/floor.ts","../../../../../../lib/ops/gpu/util/concat.ts","../../../../../../lib/ops/gpu/util/copy.ts","../../../../../../lib/ops/gpu/util/expand.ts","../../../../../../lib/ops/gpu/util/gather.ts","../../../../../../lib/ops/gpu/matMul/gemm.ts","../../../../../../lib/ops/gpu/binary/power.ts","../../../../../../lib/ops/gpu/unary/sqrt.ts","../../../../../../lib/ops/gpu/unary/log.ts","../../../../../../lib/ops/gpu/util/transpose.ts","../../../../../../lib/ops/gpu/util/repeat.ts","../../../../../../lib/ops/gpu/conv/pad.ts","../../../../../../lib/ops/gpu/util/slice.ts","../../../../../../lib/ops/gpu/conv/upsample.ts","../../../../../../lib/ops/gpu/conv/normalize.ts","../../../../../lib/ops/gpu/dispatcher.ts","../../../../../../lib/ops/gpu/unary/sign.ts","../../../../../../lib/ops/gpu/unary/negate.ts","../../../../../../lib/ops/gpu/util/clipBackward.ts","../../../../../../lib/ops/gpu/conv/convTranspose.ts","../../../../../../lib/ops/gpu/unary/sigmoid.ts","../../../../../../lib/ops/gpu/unary/addMultiplyScalar.ts","../../../../../../lib/ops/gpu/util/setValues.ts","../../../../../../lib/ops/gpu/unary/sin.ts","../../../../../../lib/ops/gpu/unary/cos.ts","../../../../../../lib/ops/gpu/unary/tan.ts","../../../../../../lib/ops/gpu/pool/reduceLogSum.ts","../../../../../../lib/ops/gpu/pool/reduceLogSumExp.ts","../../../../../../lib/ops/gpu/unary/hardSigmoid.ts","../../../../../../lib/ops/gpu/unary/powerScalar.ts","../../../../../../lib/ops/gpu/unary/round.ts","../../../../../lib/tensor/gpu/tensor.ts","../../../../../lib/tensor/wasm/tensor.ts","../../../../../../lib/ops/sparse/aggregate/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/max/max.ts","../../../../../../../lib/ops/sparse/aggregate/max/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/max/wasm.ts","../../../../../../../lib/ops/sparse/aggregate/min/min.ts","../../../../../../../lib/ops/sparse/aggregate/min/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/min/wasm.ts","../../../../../../../lib/ops/sparse/aggregate/product/product.ts","../../../../../../../lib/ops/sparse/aggregate/product/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/product/wasm.ts","../../../../../../../lib/ops/sparse/aggregate/reduceLogSum/reduceLogSum.ts","../../../../../../../lib/ops/sparse/aggregate/reduceLogSum/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/reduceLogSum/wasm.ts","../../../../../../../lib/ops/sparse/aggregate/reduceLogSumExp/reduceLogSumExp.ts","../../../../../../../lib/ops/sparse/aggregate/reduceLogSumExp/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/reduceLogSumExp/wasm.ts","../../../../../../../lib/ops/sparse/aggregate/reduceMean/reduceMean.ts","../../../../../../../lib/ops/sparse/aggregate/reduceMean/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/reduceMean/wasm.ts","../../../../../../../lib/ops/sparse/aggregate/reduceMeanSquare/reduceMeanSquare.ts","../../../../../../../lib/ops/sparse/aggregate/reduceMeanSquare/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/reduceMeanSquare/wasm.ts","../../../../../../../lib/ops/sparse/aggregate/sum/sum.ts","../../../../../../../lib/ops/sparse/aggregate/sum/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/sum/wasm.ts","../../../../../../../lib/ops/sparse/aggregate/sumSquare/sumSquare.ts","../../../../../../../lib/ops/sparse/aggregate/sumSquare/cpu.ts","../../../../../../../lib/ops/sparse/aggregate/sumSquare/wasm.ts","../../../../../../lib/ops/sparse/binary/cpu.ts","../../../../../../../lib/ops/sparse/binary/add/add.ts","../../../../../../../lib/ops/sparse/binary/add/cpu.ts","../../../../../../../lib/ops/sparse/binary/add/wasm.ts","../../../../../../../lib/ops/sparse/binary/divide/divide.ts","../../../../../../../lib/ops/sparse/binary/divide/cpu.ts","../../../../../../../lib/ops/sparse/binary/divide/wasm.ts","../../../../../../../lib/ops/sparse/binary/multiply/multiply.ts","../../../../../../../lib/ops/sparse/binary/multiply/cpu.ts","../../../../../../../lib/ops/sparse/binary/multiply/wasm.ts","../../../../../../../lib/ops/sparse/binary/subtract/subtract.ts","../../../../../../../lib/ops/sparse/binary/subtract/cpu.ts","../../../../../../../lib/ops/sparse/binary/subtract/wasm.ts","../../../../../../lib/ops/sparse/concat/gpu.ts","../../../../../../lib/ops/sparse/concat/concat.ts","../../../../../../lib/ops/sparse/concat/cpu.ts","../../../../../../lib/ops/sparse/concat/wasm.ts","../../../../../../lib/ops/sparse/matMul/matMul.ts","../../../../../../lib/ops/sparse/matMul/cpu.ts","../../../../../../lib/ops/sparse/matMul/wasm.ts","../../../../../../lib/ops/sparse/repeat/gpu.ts","../../../../../../lib/ops/sparse/repeat/repeat.ts","../../../../../../lib/ops/sparse/repeat/cpu.ts","../../../../../../lib/ops/sparse/repeat/wasm.ts","../../../../../../lib/ops/sparse/reshape/gpu.ts","../../../../../../lib/ops/sparse/reshape/reshape.ts","../../../../../../lib/ops/sparse/reshape/cpu.ts","../../../../../../lib/ops/sparse/reshape/wasm.ts","../../../../../lib/tensor/sparse/tensor.ts","../../../../../../lib/autograd/ops/unary/absBack.ts","../../../../../../lib/autograd/ops/unary/expBack.ts","../../../../../../lib/autograd/ops/unary/logBack.ts","../../../../../../lib/autograd/ops/matMul/matMulBack.ts","../../../../../../lib/autograd/ops/unary/negateBack.ts","../../../../../../lib/autograd/ops/unary/sqrtBack.ts","../../../../../../lib/autograd/ops/util/concatBack.ts","../../../../../../lib/autograd/ops/unary/clipBack.ts","../../../../../../lib/autograd/ops/util/repeatBack.ts","../../../../../../lib/autograd/ops/util/expandBack.ts","../../../../../../lib/autograd/ops/util/reshapeBack.ts","../../../../../../lib/autograd/ops/binary/addBack.ts","../../../../../../lib/autograd/ops/binary/subtractBack.ts","../../../../../../lib/autograd/ops/binary/multiplyBack.ts","../../../../../../lib/autograd/ops/conv/convBack.ts","../../../../../../lib/autograd/ops/binary/divideBack.ts","../../../../../../lib/autograd/ops/binary/powerBack.ts","../../../../../../lib/autograd/ops/matMul/gemmBack.ts","../../../../../../lib/autograd/ops/util/transposeBack.ts","../../../../../../lib/autograd/ops/reduce/sumBack.ts","../../../../../../lib/autograd/ops/reduce/sumSquareBack.ts","../../../../../../lib/autograd/ops/unary/addMultiplyScalarBack.ts","../../../../../../lib/autograd/ops/reduce/meanBack.ts","../../../../../../lib/autograd/ops/reduce/meanSquareBack.ts","../../../../../../lib/autograd/ops/util/sliceBack.ts","../../../../../../lib/autograd/ops/conv/averagePoolBack.ts","../../../../../../lib/autograd/ops/conv/padBack.ts","../../../../../../lib/autograd/ops/reduce/productBack.ts","../../../../../../lib/autograd/ops/unary/sigmoidBack.ts","../../../../../../lib/autograd/ops/unary/sinBack.ts","../../../../../../lib/autograd/ops/unary/cosBack.ts","../../../../../../lib/autograd/ops/unary/tanBack.ts","../../../../../../lib/autograd/ops/reduce/logSumBack.ts","../../../../../../lib/autograd/ops/reduce/logSumExpBack.ts","../../../../../../lib/autograd/ops/unary/powerScalarBack.ts","../../../../lib/autograd/variable.ts","../../../../lib/util/convert.ts","../../../../lib/model/module.ts","../../../../lib/onnx/node.ts","../../../../lib/onnx/util.ts","../../../../lib/onnx/definitions.ts","../../../../../lib/onnx/nodes/constant.ts","../../../../../../lib/onnx/nodes/conv/conv.ts","../../../../../lib/onnx/optimizations/optimization.ts","../../../../../lib/onnx/optimizations/convBatchnorm.ts","../../../../../lib/onnx/optimizations/convRelu.ts","../../../../../lib/onnx/optimizations/convRelu6.ts","../../../../../lib/onnx/optimizations/default.ts","../../../../../../lib/onnx/nodes/binary/binaryNode.ts","../../../../../../lib/onnx/nodes/binary/add.ts","../../../../../lib/onnx/nodes/batchNormalization.ts","../../../../../lib/onnx/nodes/cast.ts","../../../../../../lib/onnx/nodes/unary/unaryNode.ts","../../../../../../lib/onnx/nodes/unary/ceil.ts","../../../../../lib/onnx/nodes/clip.ts","../../../../../../lib/onnx/nodes/nary/concat.ts","../../../../../lib/onnx/nodes/constantOfShape.ts","../../../../../../lib/onnx/nodes/binary/div.ts","../../../../../../lib/onnx/nodes/unary/exp.ts","../../../../../lib/onnx/nodes/expand.ts","../../../../../../lib/onnx/nodes/unary/floor.ts","../../../../../lib/onnx/nodes/gather.ts","../../../../../lib/onnx/nodes/gemm.ts","../../../../../../lib/onnx/nodes/conv/instanceNormalization.ts","../../../../../lib/onnx/nodes/matMul.ts","../../../../../../lib/onnx/nodes/binary/mul.ts","../../../../../../lib/onnx/nodes/conv/pad.ts","../../../../../../lib/onnx/nodes/reduce/reduceNode.ts","../../../../../../lib/onnx/nodes/reduce/reduceMax.ts","../../../../../../lib/onnx/nodes/reduce/reduceMean.ts","../../../../../../lib/onnx/nodes/reduce/reduceSum.ts","../../../../../../lib/onnx/nodes/reduce/reduceSumSquare.ts","../../../../../lib/onnx/nodes/relu.ts","../../../../../lib/onnx/nodes/reshape.ts","../../../../../lib/onnx/nodes/shape.ts","../../../../../lib/onnx/nodes/slice.ts","../../../../../lib/onnx/nodes/softmax.ts","../../../../../../lib/onnx/nodes/binary/sub.ts","../../../../../lib/onnx/nodes/tile.ts","../../../../../lib/onnx/nodes/transpose.ts","../../../../../lib/onnx/nodes/unsqueeze.ts","../../../../../lib/onnx/nodes/upsample.ts","../../../../../../lib/onnx/nodes/conv/globalAveragePool.ts","../../../../../../lib/onnx/nodes/unary/abs.ts","../../../../../../lib/onnx/nodes/unary/log.ts","../../../../../../lib/onnx/nodes/unary/sqrt.ts","../../../../../../lib/onnx/nodes/unary/sign.ts","../../../../../../lib/onnx/nodes/unary/tan.ts","../../../../../../lib/onnx/nodes/unary/cos.ts","../../../../../../lib/onnx/nodes/unary/sin.ts","../../../../../../lib/onnx/nodes/unary/sigmoid.ts","../../../../../../lib/onnx/nodes/reduce/reduceMin.ts","../../../../../../lib/onnx/nodes/reduce/reduceProd.ts","../../../../../../lib/onnx/nodes/reduce/reduceLogSum.ts","../../../../../../lib/onnx/nodes/reduce/reduceLogSumExp.ts","../../../../../../lib/onnx/nodes/reduce/reduceL2.ts","../../../../../../lib/onnx/nodes/reduce/reduceL1.ts","../../../../../../lib/onnx/nodes/binary/pow.ts","../../../../../../lib/onnx/nodes/unary/identity.ts","../../../../../../lib/onnx/nodes/unary/hardSigmoid.ts","../../../../../../lib/onnx/nodes/unary/neg.ts","../../../../../../lib/onnx/nodes/unary/reciprocal.ts","../../../../../lib/onnx/nodes/squeeze.ts","../../../../../lib/onnx/nodes/size.ts","../../../../../lib/onnx/nodes/leakyRelu.ts","../../../../../lib/onnx/nodes/elu.ts","../../../../../lib/onnx/nodes/prelu.ts","../../../../../lib/onnx/nodes/selu.ts","../../../../../lib/onnx/nodes/flatten.ts","../../../../../lib/onnx/nodes/softplus.ts","../../../../../lib/onnx/nodes/softsign.ts","../../../../../../lib/onnx/nodes/nary/sum.ts","../../../../../../lib/onnx/nodes/nary/mean.ts","../../../../../lib/onnx/nodes/celu.ts","../../../../../../lib/onnx/nodes/unary/round.ts","../../../../../lib/onnx/nodes/range.ts","../../../../lib/onnx/resolve.ts","../../../../lib/onnx/model.ts","../../../../lib/model/basic.ts","../../../../../lib/model/optimizer/optimizer.ts","../../../../../../lib/model/optimizer/adam/updateMoments.ts","../../../../../../lib/model/optimizer/adam/updateParams.ts","../../../../../../lib/model/optimizer/adam/Adam.ts","../../../../../../../lib/model/functional/bce/back/gpu.ts","../../../../../../../lib/model/functional/bce/back/back.ts","../../../../../../../lib/model/functional/bce/back/cpu.ts","../../../../../../lib/model/functional/bce/gpu.ts","../../../../../../lib/model/functional/bce/bce.ts","../../../../../../lib/model/functional/bce/cpu.ts","inference.ts","App.tsx","reportWebVitals.ts","index.tsx"],"names":["getSize","shape","strides","i","outputDimSize","result","poolResultShape","n","j","primeFactors","num","this","e","r","input","b","a","permutation","sigmoid","multiply","value","backend","inputs","attributes","outputs","constants","onnxVersion","mode","dimIn","loadModel","name","fetch","res","arrayBuffer","buffer","model","tjs","OnnxModel","toGPU","featureDim","wait","t","Promise","resolve","reject","setTimeout","countDown","cb","models","size","prune","App","props","undefined","classifier","mean","varSqrt","meanMobilenet","gpu","GPUTensor","stdMobilenet","data","trainingResultsCollected","videoTensor","audio","numIts","state","stage","numIterations","threshold","sound","notification","numTrainSamples","numValidationSamples","setModel","Audio","getVideo","then","x","checkStorage","localStorage","getItem","classifierValues","parsedClassifier","JSON","parse","Linear","weights","Variable","create","bias","setState","prediction","reshaped","prepareVideo","forward","results","delete","process","find","optimize","video","document","querySelector","navigator","mediaDevices","getUserMedia","height","width","stream","srcObject","warmup","fromData","slice","sliced","transposed","transpose","scaled","subtract","normalized","divide","reshape","numSamples","Array","fill","console","log","progress","handleResult","tensors","res1","processed","processResult","oldResults","setValues","trainClassifier","reduceMean","shifted","variance","sumSquare","sqrt","trainData1","trainData2","trainData","concat","valData1","valData2","valData","normalizeResults","shiftedValData","normalizedValData","trainX","noGrad","trainYs","trainY","valX","valYs","prepareData","optimizer","Adam","pred","loss","bce","getValues","predTrain","from","correctTrain","getCorrect","length","predVal","sigmoidVal","predValArr","correctVal","backward","step","zeroGrads","saveModelSettings","params","getParameters","paramValues","param","push","setItem","stringify","meanValues","stdValues","Notification","permission","requestPermission","logits","v","Math","exp","play","predVals","ys","correct","className","Paper","elevation","style","padding","renderSettings","Typography","variant","component","autoPlay","id","renderState","renderGeneralSettings","renderModelSettings","Accordion","AccordionSummary","aria-controls","AccordionDetails","Grid","container","spacing","item","xs","gutterBottom","Slider","aria-labelledby","min","max","onChange","ev","valueLabelDisplay","FormControlLabel","control","Checkbox","checked","target","color","label","toggleNotification","FormControl","InputLabel","Select","labelId","event","map","MenuItem","marks","Button","onClick","prepareTraining","LinearProgress","round","backgroundColor","borderRadius","overflow","margin","React","Component","reportWebVitals","onPerfEntry","Function","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","getElementById"],"mappings":"y4BAAM,SAAUA,EAAQC,GAA0C,IAAZ,EAAY,uDAAD,EAC7D,GAAmB,IAAjB,EAAM,OACJ,OAAG,EAIP,IADA,IAAE,EAAO,EACF,EAAI,EAAG,EAAI,EAAM,OAAQ,GAAK,EACrC,GAAQ,EAAM,GAEhB,OAAO,EAGH,SAAU,EAAe,GAC3B,IAAI,EAAO,EAAM,OAEjB,GAAW,IAAT,EACE,MAAG,GAEP,GAAW,IAAT,EACE,OAAa,IAAb,EAAM,GACD,CAAC,GAED,CAAC,GAIV,IAAIC,EAAU,IAAI,MAAM,GACxB,EAAM,EAAO,GAAK,EACI,IAApB,EAAM,EAAO,KACf,EAAQ,EAAO,GAAK,GAGpB,IADA,IAAE,EAAa,EACR,EAAI,EAAO,EAAG,GAAK,EAAG,GAAK,EAC9B,EAAS,EAAM,EAAI,GAAK,EACX,IAAb,EAAM,GACFA,EAAE,GAAK,EAEb,EAAQ,GAAK,EAIf,OAAK,EAGH,SAAU,EACd,EACA,EACA,GAGE,IADA,IAAE,EAAK,EACAC,EAAI,EAAG,EAAI,EAAM,OAAQ,GAAK,EAAG,CACxC,GAAI,IACE,EAAM,GAAK,GAAM,EAAM,IAAM,EAAM,IAAmB,IAAb,EAAM,IACjD,MAAM,IAAI,MAAM,iBAGpB,GAAM,EAAM,GAAK,EAAQ,GAEzB,OAAK,EAGH,SAAU,EACd,EACA,GAME,IAJA,IAAE,EAAM,EACJ,EAAO,EAAQ,OACf,EAAQ,IAAI,MAAM,GAEf,EAAI,EAAG,EAAI,EAAM,OAAQ,GAAK,EACrC,EAAM,GAAK,KAAK,MAAM,EAAM,EAAQ,IACpC,GAAO,EAAQ,GAGf,OAAK,EAGH,SAAU,EACd,EACA,GAEE,GAAE,EAAE,SAAW,EAAE,OACjB,OAAO,EAGP,IAAG,IAAI,EAAI,EAAG,EAAI,EAAE,OAAQ,GAAK,EAC7B,KAAE,KAAO,EAAE,GACb,OAAO,EAIT,OAAK,EAoBH,SAAU,EAAe,EAAiB,GAC9C,IAAK,IAAI,EAAI,EAAM,OAAS,EAAG,GAAK,IAClC,EAAM,IAAM,EACR,EAAM,IAAM,EAAM,IAFe,IAGnC,EAAM,GAAK,E,0SC/FJ,EAA0B,CACrC,QAAS,aACT,QAAS,aACT,QAAS,aACT,MAAO,WACP,MAAO,WACP,KAAM,UACN,OAAQ,YACR,OAAQ,YACR,MAAO,YA2CqB,E,WAM1B,SAAF,EAAY,GAAW,oBACjB,KAAC,MAAQ,E,oDAkED,EAAsB,G,mIAC7B,EAAc,KAAK,WAAY,EAAO,Y,0CAClC,G,OAGI,O,SAAM,KAAK,Y,OACX,OADP,E,gBACa,EAAO,Y,UAApB,E,YAEU,IAAZ,E,iBACO,EAAI,E,aAAG,EAAI,EAAK,Q,sBACnB,KAAK,IAAI,EAAK,GAAK,EAAK,IAAM,G,2CACzB,G,QAFsB,GAAK,E,gDAM7B,EAAI,E,aAAG,EAAI,EAAK,Q,oBACnB,EAAK,KAAO,EAAK,G,2CACZ,G,QAFsB,GAAK,E,kDAOjC,G,iFAGS,GAChB,IAAI,EAEE,EAAK,KAAK,WAChB,QAAa,IAAT,EAAoB,CACtB,EAAK,GACL,IAAK,IAAI,EAAI,EAAG,EAAI,EAAG,OAAQ,IAC7B,EAAG,KAAK,QAEL,GAAM,aAAgB,MAEtB,CACL,EAAK,EACL,IAAK,IAAI,EAAI,EAAG,EAAI,EAAG,OAAQ,IACzB,EAAG,GAAK,IACV,EAAG,IAAM,KAAK,WAAW,aAL7B,EAAK,CAAC,GASR,OAAO,I,0BAmBL,EAA0B,GAC5B,IAAM,EAAK,KAAK,QAAQ,GAExB,OADA,EAAW,IAAY,EAChB,KAAK,SAAS,EAAI,K,gCAWjB,EAA0B,GAClC,IAAM,EAAK,KAAK,QAAQ,GAExB,OADA,EAAW,IAAY,EAChB,KAAK,eAAe,EAAI,K,8BAmBzB,EAA0B,GAChC,IAAM,EAAK,KAAK,QAAQ,GAExB,OADA,EAAW,IAAY,EAChB,KAAK,aAAa,EAAI,K,0BAmB3B,EAA0B,GAC5B,IAAM,EAAK,KAAK,QAAQ,GAExB,OADA,EAAW,IAAY,EAChB,KAAK,SAAS,EAAI,K,0BAmBvB,EAA0B,GAC5B,IAAM,EAAK,KAAK,QAAQ,GAExB,OADA,EAAW,IAAY,EAChB,KAAK,SAAS,EAAI,K,iCAYhB,EAA0B,GACnC,IAAM,EAAK,KAAK,QAAQ,GAGxB,OAFA,EAAW,IAAY,EAEhB,KAAK,gBAAgB,EAAI,K,mCAcrB,EAA0B,GACrC,IAAM,EAAK,KAAK,QAAQ,GAGxB,OAFA,EAAW,IAAY,EAEhB,KAAK,kBAAkB,EAAG,OAAQ,K,sCAc3B,EAA0B,GACxC,IAAM,EAAK,KAAK,QAAQ,GAGxB,OAFA,EAAW,IAAY,EAEhB,KAAK,qBAAqB,EAAI,K,uCAYtB,EAA0B,GACzC,IAAM,EAAK,KAAK,QAAQ,GAGxB,OAFA,EAAW,IAAY,EAEhB,KAAK,sBAAsB,EAAI,K,2BAmBtC,EACA,EACA,EACA,EACA,EACA,EACA,GAEA,IACM,EADK,KAAK,WACI,OAAS,EAW7B,OATA,EAAY,GAAa,IAAI,MAAM,GAAU,KAAK,GAClD,EAAQ,GAAS,EACX,EAAC,GAAQ,IAAI,MAAiB,EAAX,GAAc,KAAK,GACtC,EAAI,GAAW,IAAI,MAAM,GAAU,KAAK,QAE3B,IAAf,IACM,EAAK,MAGR,KAAK,UACV,EACA,EACA,EACA,EACA,EACA,EACA,K,oCAgBF,EACA,EACA,EACA,EACA,GAEA,IACM,EADK,KAAK,WACI,OAAS,EAO7B,OALA,EAAY,GAAa,IAAI,MAAM,GAAU,KAAK,GAClD,EAAQ,GAAS,EACjB,EAAO,GAAQ,IAAI,MAAiB,EAAX,GAAc,KAAK,GAC5C,EAAU,GAAW,IAAI,MAAM,GAAU,KAAK,GAEvC,KAAK,mBAAmB,EAAQ,EAAW,EAAO,EAAM,K,0BAoC7D,EAAgB,EAAgB,GAOlC,YANa,IAAT,IACF,EAAO,iBAEK,IAAV,IACF,EAAQ,GAEH,KAAK,SAAS,EAAM,EAAM,K,kCAajC,EACA,EACA,EACA,GAEA,IACM,EADK,KAAK,WACI,OAAS,EAM7B,OAJA,EAAO,GAAQ,IAAI,MAAiB,EAAX,GAAc,KAAK,GAC5C,EAAU,GAAW,IAAI,MAAM,GAAU,KAAK,GAC9C,EAAa,IAAc,EAEpB,KAAK,iBAAiB,EAAa,EAAM,EAAS,K,8BAWnD,EAA0B,GAGhC,IAFM,IAAF,EAAS,EACT,GAAY,EACP,EAAI,EAAG,EAAI,EAAM,OAAQ,KACd,IAAd,EAAM,GACR,EAAW,EAEX,GAAU,EAAM,GAQpB,QAJa,IAAT,IACF,GAAO,IAGS,IAAd,EAAiB,CACnB,IACM,EAAW,EADC,KAAK,YAEjB,EAAM,YAAO,GAInB,OAFA,EAAO,GAAY,EAAW,EAEvB,KAAK,aAAa,EAAQ,GAEnC,OAAO,KAAK,aAAa,EAAO,K,kCA8JhC,EACA,GAEA,GAAI,EAAc,EAAQ,GACxB,MAAO,CAAC,EAAQ,EAAQ,GAE1B,GAAI,EAAO,OAAS,EAAO,OAAQ,OACjC,EAAM,YAAO,GACb,IAAM,EAAU,EAAO,OAAS,EAAO,QACtC,KAAoB,QAApB,oBAA+B,IAAI,MAAM,GAAS,KAAK,UACnD,GAAI,EAAO,OAAS,EAAO,OAAQ,OACxC,EAAM,YAAO,GACL,IAAF,EAAU,EAAO,OAAS,EAAO,QACtC,KAAoB,QAApB,oBAA+B,IAAI,MAAM,GAAS,KAAK,KAI1D,IADM,MAAc,IAAI,MAAM,EAAO,QAAQ,KAAK,GACzC,EAAI,EAAG,EAAI,EAAO,OAAQ,IACjC,EAAY,GAAK,KAAK,IAAI,EAAO,GAAI,EAAO,IAG9C,MAAO,CAAC,EAAQ,EAAQ,K,kCAUd,GACJ,IAAF,EAAY,KAAK,WACjB,EAAY,EAAO,WACjB,GAAF,EAAc,EAAW,GAC3B,MAAO,CAAC,KAAM,EAAQ,GAGlB,IAAF,EAAmB,KACjB,GAAF,EAAU,OAAS,EAAU,OAAQ,OAC/B,EAAC,YAAO,GAChB,IAAM,EAAU,EAAU,OAAS,EAAU,QAC5C,KAAuB,QAAvB,oBAAkC,IAAI,MAAM,GAAS,KAAK,KAC3D,EAAK,KAAK,QAAQ,GAAW,QACxB,GAAI,EAAU,OAAS,EAAU,OAAQ,OAC9C,EAAS,YAAO,GAChB,IAAM,EAAU,EAAU,OAAS,EAAU,QAC5C,KAAuB,QAAvB,oBAAkC,IAAI,MAAM,GAAS,KAAK,KAC3D,EAAS,EAAO,QAAQ,GAAW,GAIrC,IADM,MAAc,IAAI,MAAM,EAAU,QAAQ,KAAK,GAC5C,EAAI,EAAG,EAAI,EAAU,OAAQ,IACpC,EAAY,GAAK,KAAK,IAAI,EAAU,GAAI,EAAU,IAEpD,MAAO,CAAC,EAAI,EAAQ,K,0BAuBlB,EAAsB,EAAgB,QAC1B,IAAV,IACF,EAAQ,QAEG,IAAT,IACF,EAAO,GAL4C,MAOrB,KAAK,YAAY,GAPI,mBAO9C,EAP8C,KAO1C,EAP0C,KAOpC,EAPoC,KAQrD,OAAO,KAAK,SACV,EACA,EACA,EACA,EACA,K,+BAwBK,EAAsB,EAAgB,QAC/B,IAAV,IACF,EAAQ,QAEG,IAAT,IACF,EAAO,GALiD,MAQ1B,KAAK,YAAY,GARS,mBAQnD,EARmD,KAQ/C,EAR+C,KAQzC,EARyC,KAS1D,OAAO,KAAK,cACV,EACA,EACA,EACA,EACA,K,+BAwBK,EAAsB,QACf,IAAV,IACF,EAAQ,GAFiC,MAIX,KAAK,YAAY,GAJN,mBAIpC,EAJoC,KAIhC,EAJgC,KAI1B,EAJ0B,KAK3C,OAAO,KAAK,cACV,EACA,EACA,EACA,K,qCAIW,GACb,OAAO,KAAK,kBAAkB,EAAO,K,gCAG7B,GACR,OAAO,KAAK,kBAAkB,EAAG,K,6BAyB5B,EAAsB,QACb,IAAV,IACF,EAAQ,GAF+B,MAKT,KAAK,YAAY,GALR,mBAKlC,EALkC,KAK9B,EAL8B,KAKxB,EALwB,KAMzC,OAAO,KAAK,YACV,EACA,EACA,EACA,K,4BAwBE,GAAoB,MACQ,KAAK,YAAY,GADzB,mBACjB,EADiB,KACb,EADa,KACP,EADO,KAExB,OAAO,KAAK,WACV,EACA,EACA,K,gCAkBM,GACR,QAAoB,IAAhB,EAA2B,CAC7B,IACM,EADQ,KAAK,WACA,OACnB,EAAc,GACd,IAAK,IAAI,EAAI,EAAG,EAAI,EAAM,IACxB,EAAY,KAAK,EAAO,EAAI,GAGhC,OAAO,KAAK,eAAe,K,8BASrB,GACN,IAAM,EAAM,KAAK,IAAI,GAAM,GACrB,EAAa,KAAK,SAAS,GAC3B,EAAM,EAAW,MAEjB,EAAM,EAAI,IAAI,GAAM,GACpB,EAAS,EAAI,OAAO,GAO1B,OALA,EAAI,SACJ,EAAW,SACX,EAAI,SACJ,EAAI,SAEG,I,2BAkBP,EACA,EACA,EACA,EACA,EACA,GAOA,GALA,EAAa,IAAc,EAC3B,EAAa,IAAc,EAC3B,OAAkB,IAAV,EAAsB,EAAQ,EACtC,OAAgB,IAAT,EAAqB,EAAO,OAEzB,IAAN,EAAiB,CACnB,IAAM,EAAS,KAAK,WAChB,EAAS,EAAE,WACT,EAAQ,EAAO,OACf,EAAQ,EAAO,OAEjB,EAAQ,IACV,EAAM,sBAAO,IAAI,MAAM,EAAQ,GAAO,KAAK,IAArC,YAA4C,IAClD,EAAI,EAAE,QAAQ,GAAQ,IAI1B,OAAO,KAAK,UAAU,EAAG,EAAY,EAAY,EAAO,EAAM,K,4BAyB9D,EACA,EACA,EACA,GAEA,IAAM,EAAQ,KAAK,WACb,EAAO,EAAM,OACnB,QAAa,IAAT,EAAoB,CACtB,EAAO,GACP,IAAK,IAAI,EAAI,EAAG,EAAI,EAAM,IACxB,EAAK,KAAK,QAGZ,EAAO,EAAK,KAAI,YAAC,OAAK,EAAI,EAAI,EAAI,EAAO,UAE7B,IAAV,IACF,EAAQ,IAAI,MAAM,GAAM,KAAK,IAE/B,EAAM,YAAO,GACb,EAAI,YAAO,GACX,IAAK,IAAI,EAAI,EAAG,EAAI,EAAK,OAAQ,IAAK,CACpC,IAAM,EAAK,EAAM,EAAK,IAClB,EAAO,GAAK,EACd,EAAO,IAAM,EACJ,EAAO,IAAM,IAClB,EAAM,GAAK,EACb,EAAO,GAAK,EAEZ,EAAO,GAAK,EAAK,GAGjB,EAAK,GAAK,EACZ,EAAK,IAAM,EACF,EAAK,IAAM,IACpB,EAAK,GAAK,GAGd,OAAO,KAAK,WAAW,EAAQ,EAAM,EAAM,K,gCAiF3C,IADK,EACC,EAAK,KAAK,WACV,EAAW,GAFZ,cAGW,GAHX,IAGL,2BAAoB,KAAT,EAAS,QACR,IAAN,GACF,EAAS,KAAK,IALb,8BAQL,OAAO,KAAK,QAAQ,K,8BAGd,QACO,IAAT,IACF,EAAO,GAET,IAAM,EAAK,KAAK,WACZ,EAAO,IACT,GAAQ,EAAG,QAEb,IAAM,EAAW,CACf,EAAQ,EAAG,MAAM,EAAG,GAAO,GAC3B,EAAQ,EAAG,MAAM,GAAO,IAE1B,OAAO,KAAK,QAAQ,O,KCnqClB,SAAUC,EACd,EACA,EACA,EACA,EACA,EACA,GAEE,IAAI,EAAU,GAAY,EAAS,GAAK,EACxC,OAAK,KAAK,OAAO,EAAS,EAAU,EAAU,GAAW,EAAS,GAGhE,SAAU,EACd,EACA,EACA,EACA,EACA,EACA,GAGA,IADA,IAAM,EAAmB,GAChB,EAAI,EAAG,EAAI,EAAQ,OAAQ,IAClC,EAAO,KACL,EACE,EAAQ,GACR,EAAQ,GACR,EAAS,GACT,EAAS,GACT,EAAU,GACV,EAAQ,KAId,OAAO,ECxBH,SAAU,EACd,EACA,GAIE,IAFA,IAAI,EAAS,IAAI,EAAU,EAAE,WAAO,EAAW,EAAE,OAE1C,EAAI,EAAG,EAAI,EAAO,KAAM,GAAK,EAChCC,EAAG,IAAI,EAAG,EAAG,EAAE,IAAI,KAGvB,OAAK,EAGH,SAAU,EACd,EACA,EACA,EACA,GAEE,IHkEE,SACJ,EACA,GAEE,GAAE,EAAE,SAAW,EAAE,OACb,OAAG,EAGP,IAAG,IAAI,EAAI,EAAG,EAAI,EAAE,OAAQ,GAAK,EAC7B,KAAE,KAAO,EAAE,IAAe,IAAT,EAAE,IAAqB,IAAT,EAAE,GACnC,OAAO,EAIX,OAAO,EGhFF,CAAiB,EAAE,MAAO,EAAE,OAC/B,MAAM,IAAI,MACR,2EAQJ,IAJE,IAAI,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEjD,EAAQ,IAAI,MAAM,EAAY,QAAQ,KAAK,GAExC,EAAI,EAAG,EAAI,EAAO,KAAM,GAAK,EACpC,EAAO,IAAI,EAAO,EAAG,EAAE,IAAI,GAAQ,EAAE,IAAI,KAEzC,EAAe,EAAO,GAGxB,OAAO,EA+FH,SAAU,EACd,EACA,EACA,GAEA,OAAO,EAAoB,GAAG,YAAE,OAAI,EAAK,EAAS,KCnI9C,SAAU,EACd,EACA,EACA,EACA,EACA,EACA,GAGA,IADA,IApBA,EACA,EACA,EACA,EACA,EAgBM,EAAmB,GAChB,EAAI,EAAG,EAAI,EAAQ,OAAQ,IAClC,EAAO,MAtBT,EAwBM,EAAQ,GAvBd,EAwBM,EAAQ,GAvBd,EAwBM,EAAS,GAvBf,EAwBM,EAAS,GAvBf,EAwBM,EAAU,GACV,EAAQ,IApBG,EAAS,GAAK,EAAU,GAFtB,GAAY,EAAS,GAAK,GAEmB,IAwBhE,OAAO,EClCH,SAAUC,EACd,EACA,EACA,GAKE,IAHA,IAAI,EAAc,GACd,EAAW,GACX,EAAkB,GACf,EAAI,EAAG,EAAI,EAAW,OAAQ,IAChC,EAAK,SAAS,IAIb,IACF,EAAY,KAAK,GACjB,EAAM,KAAK,IAEb,EAAS,KAAK,EAAW,MAPnB,EAAM,KAAK,EAAW,IACtB,OAAK,IAcf,OAJ2B,IAAvB,EAAY,QACd,EAAY,KAAK,GAGZ,CAAC,EAAa,GCdjB,SAAU,EACd,EACA,EACA,EACA,EACA,GAaE,IAXA,IAAI,EAAa,EAAE,WACf,EAAY,EAAQ,GAHS,EAIN,EAAgB,EAAY,EAAM,GAJ5B,mBAI5B,EAJ4B,KAIf,EAJe,KAK7B,EAAa,EAAQ,GACrB,EAAgB,EAAe,GAE/B,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OACjD,EAAc,IAAI,MAAM,GAAY,MAAK,GAEzC,EAAkB,IAAI,MAAM,EAAW,QAAQ,KAAK,GACpD,EAAqB,IAAI,MAAM,EAAY,QAAQ,KAAK,GACrD,EAAI,EAAG,EAAI,EAAW,IAAK,CAC9B,IAAC,IAAI,EAAI,EAAG,EAAI,EAAM,OAAQ,IAChC,EAAS,GAAK,EAAM,EAAM,IAE5B,IAAM,EAAY,EAAW,EAAU,GAEnC,EAAY,GACd,EAAO,IAAI,EAAU,EAAU,EAAE,IAAI,GAAI,EAAO,IAAI,MAEpD,EAAY,IAAa,EACzB,EAAO,IAAI,EAAU,EAAU,EAAE,IAAI,MAGvC,EAAe,EAAO,GAGxB,GAAI,EACF,IAAK,IAAI,EAAI,EAAG,EAAI,EAAO,KAAM,IAC/B,EAAO,IAAI,EAAG,EAAY,EAAO,IAAI,KAIzC,OAAO,ECbT,SAAS,EACP,EACA,EACA,EACA,EACA,GAEE,GAAE,EACE,OAAG,EAAE,IAAI,GAGb,IAAI,EAAO,EAAE,MAAM,OAEnB,GAAW,aAAT,EACF,OAAO,EACF,GAAa,SAAT,EACT,IAAK,IAAI,EAAI,EAAG,EAAI,EAAM,IACpB,EAAM,GAAK,EACb,EAAM,GAAK,EACF,EAAM,IAAM,EAAE,MAAM,KAC7B,EAAM,GAAK,EAAE,MAAM,GAAK,QAI5B,IAAK,IAAI,EAAI,EAAG,EAAI,EAAM,IACpB,EAAM,GAAK,EACb,EAAM,IAAM,EAAM,GACT,EAAM,IAAM,EAAE,MAAM,KAC7B,EAAM,GAAK,EAAI,EAAE,MAAM,GAAK,EAAM,GAAK,GAK7C,OAAO,EAAE,IAAI,GCCf,IAAa,EAAb,YAAE,qBAAF,iBAmCI,SAAF,EACE,EACA,EACA,GAAY,kCAER,EAAJ,YAAM,GAAU,YAPX,SAAU,EASf,EAAK,MAAQ,EACb,EAAK,QAAU,EAAe,GAC1B,EAAC,KAAO,EAAQ,GAIhB,EAAK,YAFM,IAAX,EACE,aAAkB,MACN,IAAI,EAAwB,EAAK,OAC7C,GAGY,EAGF,IAAI,EAAwB,EAAK,OAC7C,EAAK,MAlBG,EAtCd,uBAAE,IAAJ,YAAI,MAAJ,WA8DI,OAAO,QAAQ,QAAQ,KAAK,UA9DhC,CAAI,IAAJ,WAAI,MAAJ,WAkEI,OAAO,KAAK,QAlEhB,CAAI,IAAJ,eAAI,MAAJ,SAqEe,GACX,OAAO,IAAI,EACT,KAAK,MACL,IAAI,EAAwB,KAAK,OAAO,KAAK,MAAM,KACjD,GAEF,KAAK,SA3EX,CAAI,IAAJ,iBAAI,MAAJ,SA+EiB,GACb,OAAO,IAAI,EAAU,CAAC,GAAI,CAAC,GAAQ,KAAK,SAhF5C,CAAI,IAAJ,OAAI,MAAJ,SAmF4B,GAGxB,OAAO,IAAI,EAAU,KAAK,MAAO,MAAM,KAAK,KAAK,QAAS,KAtF9D,CAAI,IAAJ,SAAI,MAAJ,WA2FI,KAAK,YAAS,EACd,KAAK,SAAU,IA5FnB,CAAI,IAAJ,OAAI,MAAJ,SA+FO,QACc,IAAb,IACF,EAAQ,YAAO,KAAK,QAKtB,IAHA,IAAM,EAAS,IAAI,EAAwB,KAAK,OAC9C,KAAK,MAEE,EAAI,EAAG,EAAI,KAAK,KAAM,IAC7B,EAAO,GAAK,KAAK,OAAO,GAE1B,OAAO,IAAI,EAAgB,EAAU,EAAQ,KAAK,SAzGtD,CAAI,IAAJ,MAAI,MAAJ,SA4GM,GACF,IAAI,EAOJ,OALE,EADE,MAAM,QAAQ,GACV,EAAW,EAAO,KAAK,QAAS,KAAK,OAErC,EAGD,KAAK,OAAO,KApHvB,CAAI,IAAJ,MAAI,MAAJ,SAuHM,EAA0B,GAC5B,IAAI,EAEF,EADE,MAAM,QAAQ,GACV,EAAW,EAAO,KAAK,SAEvB,EAGR,KAAK,OAAO,GAAO,IA/HvB,CAAI,IAAJ,YAAI,MAAJ,SAkIY,EAAsB,GAC9B,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,yCAElB,OC1ME,SACJ,EACA,EACA,GAME,IAJA,IAAI,EAAS,IAAI,EAAU,EAAE,WAAO,EAAW,EAAE,OAE7C,EAAQ,IAAI,MAAM,EAAE,MAAM,QAAQ,KAAK,GAEpC,EAAI,EAAG,EAAI,EAAO,KAAM,GAAK,EAAG,CAGnC,IAFA,OAAM,EACJ,EAAM,IAAI,MAAM,EAAO,QAAQ,KAAK,GACjC,EAAI,EAAG,EAAI,EAAO,OAAQ,IAAK,CACtC,GAAI,EAAM,GAAK,EAAO,IAAM,EAAM,IAAM,EAAO,GAAK,EAAE,MAAM,GAAI,CAC9D,GAAM,EACN,MAEA,EAAI,GAAK,EAAM,GAAK,EAAO,GAI3B,EACF,EAAO,IAAI,EAAG,EAAE,IAAI,IAEpB,EAAO,IAAI,EAAG,EAAE,IAAI,IAGtB,EAAe,EAAO,EAAE,OAG1B,OAAO,ED4KE,CAAU,KAAM,EAAQ,KAtInC,CAAI,IAAJ,MAAI,MAAJ,WA0II,OLlKK,EKkKM,MLlKiB,YAAE,OAAI,KAAK,IAAI,QKwB/C,CAAI,IAAJ,MAAI,MAAJ,WA8II,OLlKK,EKkKM,MLlKiB,YAAE,OAAI,KAAK,IAAI,QKoB/C,CAAI,IAAJ,OAAI,MAAJ,WAkJI,OLlKK,EKkKO,MLlKgB,YAAE,OAAI,KAAK,KAAK,QKgBhD,CAAI,IAAJ,MAAI,MAAJ,WAsJI,OLlKK,EKkKM,MLlKiB,YAAE,OAAI,KAAK,IAAI,QKY/C,CAAI,IAAJ,MAAI,MAAJ,WA0JI,OLlKK,EKkKM,MLlKiB,YAAE,OAAI,KAAK,IAAI,QKQ/C,CAAI,IAAJ,MAAI,MAAJ,WA8JI,OLlKK,EKkKM,MLlKiB,YAAE,OAAI,KAAK,IAAI,QKI/C,CAAI,IAAJ,MAAI,MAAJ,WAkKI,OLlKK,EKkKM,MLlKiB,YAAE,OAAI,KAAK,IAAI,QKA/C,CAAI,IAAJ,OAAI,MAAJ,WAsKI,OLlKK,EKkKO,MLlKgB,YAAE,OAAI,KAAK,KAAK,QKJhD,CAAI,IAAJ,OAAI,MAAJ,WA0KI,OLlKK,EKkKO,MLlKgB,YAAE,OAAI,KAAK,KAAK,QKRhD,CAAI,IAAJ,OAAI,MAAJ,WA8KI,OLlKK,EKkKO,MLlKgB,YAAE,OAAI,KAAK,KAAK,QKZhD,CAAI,IAAJ,OAAI,MAAJ,WAkLI,OLlKK,EKkKO,MLlKgB,YAAE,OAAI,KAAK,KAAK,QKhBhD,CAAI,IAAJ,OAAI,MAAJ,WAsLI,OLlKK,EKkKO,MLlKgB,YAAE,OAAI,KAAK,KAAK,QKpBhD,CAAI,IAAJ,OAAI,MAAJ,WA0LI,OLlKK,EKkKO,MLlKgB,YAAE,OAAI,KAAK,KAAK,QKxBhD,CAAI,IAAJ,QAAI,MAAJ,WA8LI,OLlKK,EKkKQ,MLlKe,YAAE,OAAI,KAAK,MAAM,QK5BjD,CAAI,IAAJ,QAAI,MAAJ,WAkMI,OLlKK,EKkKQ,MLlKe,YAAE,OAAI,KAAK,MAAM,QKhCjD,CAAI,IAAJ,QAAI,MAAJ,WAsMI,OLlKK,EKkKQ,MLlKe,YAAE,OAAI,KAAK,MAAM,QKpCjD,CAAI,IAAJ,QAAI,MAAJ,WA0MI,OLlKK,EKkKQ,MLlKe,YAAE,OAAI,KAAK,MAAM,QKxCjD,CAAI,IAAJ,OAAI,MAAJ,WA8MI,OLlKK,EKkKO,MLlKgB,YAAE,OAAI,KAAK,KAAK,QK5ChD,CAAI,IAAJ,QAAI,MAAJ,WAkNI,OLlKK,EKkKQ,MLlKe,YAAE,OAAI,KAAK,MAAM,QKhDjD,CAAI,IAAJ,SAAI,MAAJ,WAsNI,OL9JK,EK8JS,ML9Jc,YAAE,OAAK,OKxDvC,CAAI,IAAJ,cAAI,MAAJ,SAyNc,EAAe,GACzB,OL/JE,SACJ,EACA,EACA,GAEA,OAAO,EAAoB,GAAG,YAAE,OAAI,KAAK,IAAI,EAAI,GAAS,KK0JjD,CAAY,KAAM,EAAO,KA1NpC,CAAI,IAAJ,iBAAI,MAAJ,SA6NiB,GACb,OAAO,EAAkB,KAAM,EAAO,KA9N1C,CAAI,IAAJ,YAAI,MAAJ,SAiOY,GACF,OAAC,EAAkB,KAAM,EAAG,KAlOtC,CAAI,IAAJ,oBAAI,MAAJ,SAqOoB,EAAgB,GAChC,OAAO,EAAkB,KAAM,EAAQ,KAtO3C,CAAI,IAAJ,OAAI,MAAJ,WA0OU,OLtLD,EKsLO,MLtLgB,YAAE,OAAK,EAAK,GAAK,EAAW,IAAP,EAAW,EAAI,OKpDpE,CAAI,IAAJ,OAAI,MAAJ,SA6OO,EAAc,GACjB,OLrJE,SACJ,EACA,EACA,GAEA,IAAI,EAAI,SAAC,GAAD,OAAgB,GASxB,YARY,IAAR,QAA6B,IAAR,EACvB,EAAI,SAAC,GAAD,OAAgB,KAAK,IAAI,EAAK,KAAK,IAAI,EAAK,UAC/B,IAAR,EACT,EAAI,SAAC,GAAD,OAAgB,KAAK,IAAI,EAAK,SACjB,IAAR,IACT,EAAI,SAAC,GAAD,OAAgB,KAAK,IAAI,EAAK,KAG7B,EAAoB,EAAG,GKuIrB,CAAK,KAAM,EAAK,KA9O3B,CAAI,IAAJ,eAAI,MAAJ,SAiPe,EAAoB,EAAc,GAC7C,KAAM,aAAgB,GACpB,MAAM,IAAI,MAAM,2CAElB,OLnFE,SACJ,EACA,EACA,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,YAAY,IAAR,GAAqB,EAAI,QAGjB,IAAR,GAAqB,EAAI,EAFpB,EAKF,IAET,GKgEO,CAAa,KAAM,EAAM,KAAK,WAAY,EAAK,KArP1D,CAAI,IAAJ,UAAI,MAAJ,WAyPI,OL7KK,EK6KU,ML7Ka,YAAE,OAAI,GAAK,EAAI,KAAK,KAAK,SK5EzD,CAAI,IAAJ,cAAI,MAAJ,SA4Pc,EAAe,GACzB,OL9KE,SACJ,EACA,EACA,GAEA,OAAO,EAAoB,GAAG,YAAE,OAC9B,KAAK,IAAI,EAAG,KAAK,IAAI,EAAG,EAAQ,EAAK,OKwK9B,CAAY,KAAM,EAAO,KA7PpC,CAAI,IAAJ,WAAI,MAAJ,SAiQI,EACA,EACA,EACA,EACA,GAEM,kBAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,yCAEZ,OLhKJ,SACJ,EACA,EACA,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAI,GAAL,OAAY,EAAK,EAAQ,EAAK,IAC9B,GKqJO,CAAI,EAAI,EAAQ,EAAa,EAAO,KA1Q/C,CAAI,IAAJ,gBAAI,MAAJ,SA8QI,EACA,EACA,EACA,EACA,GAEA,KAAM,aAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,8CAElB,OL9JE,SACJ,EACA,EACA,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAI,GAAL,OAAY,EAAK,EAAQ,EAAK,IAC9B,GKmJO,CAAS,EAAI,EAAQ,EAAa,EAAO,KAvRpD,CAAI,IAAJ,gBAAI,MAAJ,SA2RI,EACA,EACA,EACA,GAEA,KAAM,aAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,yCAElB,OL3JE,SACJ,EACA,EACA,EACA,GAEA,OAAO,EAAqB,EAAG,GAAG,SAAC,EAAI,GAAL,OAAY,EAAK,EAAK,IAAO,GKqJtD,CAAS,EAAI,EAAQ,EAAa,KAnS7C,CAAI,IAAJ,cAAI,MAAJ,SAuSI,EACA,EACA,EACA,GAEA,KAAM,aAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,yCAElB,OL9JE,SACJ,EACA,EACA,EACA,GAEA,OAAO,EAAqB,EAAG,GAAG,SAAC,EAAI,GAAL,OAAa,EAAK,EAAM,IAAO,GKwJxD,CAAO,EAAI,EAAQ,EAAa,KA/S3C,CAAI,IAAJ,aAAI,MAAJ,SAmTI,EACA,EACA,GAEA,KAAM,aAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,mDAElB,OLhKE,SACJ,EACA,EACA,GAEA,OAAO,EAAqB,EAAG,GAAG,SAAC,EAAI,GAAL,OAAY,KAAK,IAAI,EAAI,KAAK,GK2JvD,CAAM,EAAI,EAAQ,KA1T7B,CAAI,IAAJ,SAAI,MAAJ,SA6TS,GACL,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,yCAGlB,OEvYE,SACJ,EACA,GAEE,GAAqB,IAAnB,EAAE,MAAM,QAAmC,IAAnB,EAAE,MAAM,OAClC,MAAM,IAAI,MAAM,+CAGhB,GAAE,EAAE,MAAM,KAAO,EAAE,MAAM,GACzB,MAAM,IAAI,MACR,6EAUJ,IANE,IAAI,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,GAEZ,EAAS,IAAI,EAAU,CAAC,EAAG,QAAI,EAAW,EAAE,OAEzC,EAAI,EAAG,EAAI,EAAG,GAAK,EAC1B,IAAK,IAAI,EAAI,EAAG,EAAI,EAAG,GAAK,EAAG,CAE7B,IADA,IAAI,EAAM,EACD,EAAI,EAAG,EAAI,EAAG,GAAK,EAC1B,GAAO,EAAE,IAAI,CAAC,EAAG,IAAM,EAAE,IAAI,CAAC,EAAG,IAEnC,EAAO,IAAI,CAAC,EAAG,GAAI,GAIvB,OAAO,EFyWE,CAAO,KAAM,KAlUxB,CAAI,IAAJ,YAAI,MAAJ,SAsUI,EACA,EACA,EACA,EACA,EACA,GAEA,KACI,aAAa,SAAoB,IAAN,GAAmB,aAAa,IAE7D,MAAM,IAAI,MAAM,qCAElB,OGtZE,SACJ,EACA,EACA,EACA,EACA,EACA,EACA,GAEE,IAAI,EAAO,EAAE,MAAM,OAEf,EAAI,EAAa,EAAE,MAAM,EAAO,GAAK,EAAE,MAAM,EAAO,GACpD,EAAI,EAAa,EAAE,MAAM,EAAO,GAAK,EAAE,MAAM,EAAO,GACpD,EAAI,EAAa,EAAE,MAAM,EAAO,GAAK,EAAE,MAAM,EAAO,GAEpD,EAAa,EAAI,EACjB,EAAa,EAAI,EACjB,EAAa,EAAI,EAEjB,EAAS,EAAa,EAAI,EAC1B,EAAS,EAAa,EAAI,EAC1B,EAAS,EAAa,EAAI,EAC1B,EAAS,EAAa,EAAI,EAE5B,EAAS,EACT,EAAS,EACT,EAAa,OACP,IAAN,IACF,EAAS,EAAE,QAAQ,EAAO,GAC1B,EAAS,EAAE,QAAQ,EAAO,GAEP,EAAQ,EAAE,MAAM,MAAM,EAAG,EAAO,IAClC,EAGI,KAFnB,EAAa,EAAE,MAAM,EAAO,GAAK,EAAE,MAAM,EAAO,MAG9C,EAAa,GAGf,EAAa,GAIf,IAAI,EAAa,EAAE,MAAM,MAAM,EAAG,EAAO,GACvC,EAAY,EAAQ,GACN,IAAd,IACE,EAAQ,GAMZ,IAJA,IAAI,EAAW,sBAAO,GAAP,CAAmB,EAAG,IAEjC,EAAI,IAAI,EAAU,OAAa,EAAW,EAAE,OAEzC,EAAI,EAAG,EAAI,EAAW,IAM7B,IALI,IAAE,EAAQ,EAAI,EACZ,EAAQ,EAAI,EACZ,EAAQ,EAAI,EACZ,EAAQ,EAAI,EAET,EAAI,EAAG,EAAI,EAAG,IACrB,IAAK,IAAI,EAAI,EAAG,EAAI,EAAG,IAAK,CAG1B,IAFA,IAAI,EAAS,EAEJ,EAAI,EAAG,EAAI,EAAG,IACrB,GACE,EAAE,IAAI,EAAQ,EAAI,EAAS,EAAI,GAC/B,EAAE,IAAI,EAAQ,EAAI,EAAS,EAAI,GAGnC,GAAS,OACC,IAAN,IACF,GAAU,EAAO,EAAE,IAAI,EAAQ,EAAI,EAAS,EAAI,IAGlD,EAAE,IAAI,EAAQ,EAAI,EAAI,EAAG,GAK/B,OAAO,EHwUE,CACL,KACA,EACA,EACA,EACA,EACA,EACA,KAzVN,CAAI,IAAJ,WAAI,MAAJ,SA6VW,EAAgB,GACvB,OIjaE,SACJ,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,QAAW,IAAN,EAAkB,EAAI,KAEpC,GJsZO,CAAI,KAAM,EAAM,KA9V3B,CAAI,IAAJ,iBAAI,MAAJ,SAiWiB,EAAgB,GAC7B,OKraE,SACJ,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,EAAI,QAAW,IAAN,EAAkB,EAAI,KAExC,GL0ZO,CAAU,KAAM,EAAM,KAlWjC,CAAI,IAAJ,eAAI,MAAJ,SAqWe,EAAgB,GAC3B,OMzaE,SACJ,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,QAAW,IAAN,EAAkB,EAAI,KAEpC,GN8ZO,CAAQ,KAAM,EAAM,KAtW/B,CAAI,IAAJ,WAAI,MAAJ,SAyWW,EAAgB,GACvB,OO7aE,SACJ,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,KAAK,IAAI,OAAS,IAAN,EAAkB,EAAI,KAE3C,GPkaO,CAAI,KAAM,EAAM,KA1W3B,CAAI,IAAJ,WAAI,MAAJ,SA6WW,EAAgB,GACvB,OQjbE,SACJ,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,KAAK,IAAI,OAAS,IAAN,EAAkB,EAAI,KAE3C,GRsaO,CAAI,KAAM,EAAM,KA9W3B,CAAI,IAAJ,kBAAI,MAAJ,SAiXkB,EAAgB,GAC9B,OSrbE,SACJ,EACA,EACA,GAGA,IADA,IAAI,EAAW,EACN,EAAI,EAAG,EAAI,EAAK,OAAQ,IAC/B,GAAY,EAAE,MAAM,EAAK,IAG3B,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,QAAW,IAAN,EAAkB,EAAI,KAEpC,GACA,SAAC,GAAD,OAAe,EAAI,KToaZ,CAAW,KAAM,EAAM,KAlXlC,CAAI,IAAJ,wBAAI,MAAJ,SAqXwB,EAAgB,GACpC,OUzbE,SACJ,EACA,EACA,GAGA,IADA,IAAI,EAAW,EACN,EAAI,EAAG,EAAI,EAAK,OAAQ,IAC/B,GAAY,EAAE,MAAM,EAAK,IAG3B,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,EAAI,QAAW,IAAN,EAAkB,EAAI,KAExC,GACA,SAAC,GAAD,OAAe,EAAI,KVwaZ,CAAiB,KAAM,EAAM,KAtXxC,CAAI,IAAJ,oBAAI,MAAJ,SAyXoB,EAAgB,GAChC,OW7bE,SACJ,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,QAAW,IAAN,EAAkB,EAAI,KAEpC,GACA,SAAC,GAAD,OAAe,KAAK,IAAI,MXibjB,CAAa,KAAM,EAAM,KA1XpC,CAAI,IAAJ,uBAAI,MAAJ,SA6XuB,EAAgB,GACnC,OYjcE,SACJ,EACA,EACA,GAEA,OAAO,EACL,EACA,GACA,SAAC,EAAG,GACF,OAAO,KAAK,IAAI,SAAY,IAAN,EAAkB,EAAI,KAE9C,GACA,SAAC,GAAD,OAAe,KAAK,IAAI,MZqbjB,CAAgB,KAAM,EAAM,KA9XvC,CAAI,IAAJ,YAAI,MAAJ,SAkYI,EACA,EACA,EACA,EACA,EACA,EACA,GAEA,KACI,aAAkB,SACV,IAAT,KAAwB,aAAgB,GAEzC,MAAM,IAAI,MAAM,yDAElB,OaldE,SACJ,EACA,EACA,EACA,EACA,EACA,EACA,EACA,GAEE,IAAI,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,MAAM,GAClB,EAAI,EAAE,MAAM,MAAM,GAClB,EAAI,EAAE,MAAM,GACZ,EAAK,EAAI,EAET,EAAa,EAAQ,GAErB,EAAI,EACR,EACA,EACA,EAAK,MAAM,EAAG,EAAK,OAAS,GAC5B,EAAK,MAAM,EAAK,OAAS,GACzB,EACA,GAEI,EAAa,EAAQ,GACvB,EAAc,CAAC,EAAG,GACpB,EAAY,EAAY,OAAO,GAO/B,IALA,IAAI,EAAI,IAAI,EAAU,OAAa,EAAW,EAAE,OAE5C,EAAW,EAAE,OAGV,EAAI,EAAG,EAAI,EAAG,IAEjB,IAAC,IAAI,EAAI,EAAG,EAAI,EAAG,IAAK,CACpB,GAAF,EAAM,CACA,IAAF,EAAI,EAAQ,EAAK,IAAI,CAAC,IAAiB,EAEvC,EAAgB,IAAI,MAAM,EAAE,QAAQ,KAAK,GACvC,EAAM,QAAQ,EAAG,GAEjB,IAAH,IAAI,EAAM,EAAG,EAAM,EAAY,IACxB,EAAR,IAAI,EAAe,GAEX,EAAK,EAAe,EAAE,OAI9B,IAAD,IAAI,EAAK,EAAG,EAAK,EAAI,IAAM,CACtB,IAAF,GAAK,EAAI,EAAK,GAAM,EAEpB,EAAgB,IAAI,MAAM,EAAE,QAAQ,KAAK,GACvC,EAAM,QAAQC,EAAG,GACjB,IAAH,IAAI,EAAM,EAAG,EAAM,EAAY,IAAO,CACzC,IAAI,EAAS,EAAE,IAAI,GAEb,EAA0B,IAAI,MAAM,EAAE,QAAQ,KAAK,GACzD,EAAc,QAAQ,EAAG,GACzB,IAAK,IAAI,EAAM,EAAG,EAAM,EAAY,IAAO,CAIzC,IAHA,IAAM,EAAU,CAAC,EAAG,GAEhB,GAAO,EACF,EAAO,EAAG,EAAO,EAAU,IAAQ,CAC1C,IAAM,EAA4B,IAAnB,EAAQ,OAAe,EAAI,EAAQ,GAC5C,EAAsB,IAAhB,EAAK,OAAe,EAAI,EAAK,GACnC,EAAgC,IAArB,EAAU,OAAe,EAAI,EAAU,GAElD,EACJ,EAAc,EAAO,GAAK,EAC1B,EACA,EAAc,EAAO,GAAK,EAE5B,GAAI,EAAK,GAAK,GAAM,EAAE,GAAO,CAC3B,GAAO,EACP,MAGF,EAAQ,KAAK,GAGV,IAGH,GAFW,EAAE,IAAI,GACN,EAAE,IAAI,IAInB,EAAe,EAAe,EAAE,OAGlC,EAAE,IAAI,EAAe,GAErB,EAAe,EAAe,EAAE,QAIpC,GAAmB,OAAf,EAAqB,CACvB,IAAM,EAAgB,IAAI,MAAM,EAAE,QAAQ,KAAK,GAC/C,EAAc,QAAQ,EAAG,GAEzB,IAAK,IAAI,EAAM,EAAG,EAAM,EAAY,IAAO,CACzC,IAAI,EAAS,EAAE,IAAI,GACA,SAAf,EACF,EAAS,KAAK,IAAI,EAAG,GACG,UAAf,IACT,EAAS,KAAK,IAAI,KAAK,IAAI,EAAG,GAAS,IAEzC,EAAE,IAAI,EAAe,GAErB,EAAe,EAAe,EAAE,SAMxC,OAAO,Eb4VE,CACL,KACA,EACA,EACA,EACA,EACA,EACA,EACA,KAxZN,CAAI,IAAJ,qBAAI,MAAJ,SA6ZI,EACA,EACA,EACA,EACA,GAEA,KAAM,aAAkB,GACtB,MAAM,IAAI,MACR,mEAIJ,Oc5eE,SACJ,EACA,EACA,EACA,EACA,EACA,GAEE,IAAI,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,MAAM,GAClB,EAAI,EAAE,MAAM,MAAM,GAClB,EAAI,EAAE,MAAM,GACZ,EAAK,EAAI,EAET,EAAa,EAAQ,GAErB,EAAI,EACR,EACA,EACA,EAAK,MAAM,EAAG,EAAK,OAAS,GAC5B,EAAK,MAAM,EAAK,OAAS,GACzB,EACA,GAEI,EAAa,EAAQ,GACvB,EAAc,CAAC,EAAG,GACpB,EAAY,EAAY,OAAO,GAO/B,IALA,IAAI,EAAI,IAAI,EAAU,OAAa,EAAW,EAAE,OAE5C,EAAW,EAAE,OAGV,EAAI,EAAG,EAAI,EAAG,IAEjB,IAAC,IAAI,EAAI,EAAG,EAAI,EAAG,IACf,IAAD,IAAI,EAAK,EAAG,EAAK,EAAI,IAAM,CACtB,IAAF,GAAK,EAAI,EAAK,GAAM,EAEpB,EAAgB,IAAI,MAAM,EAAE,QAAQ,KAAK,GACvC,EAAM,QAAQ,EAAG,GACjB,IAAH,IAAI,EAAM,EAAG,EAAM,EAAY,IAAO,CAC/B,IAAN,EAAS,EAAE,IAAI,GAEb,EAA0B,IAAI,MAAM,EAAE,QAAQ,KAAK,GAC/C,EAAI,QAAQ,EAAG,GACf,IAAL,IAAI,EAAM,EAAG,EAAM,EAAY,IAAO,CAI7B,IAHA,IAAN,EAAU,CAAC,EAAG,GAEhB,GAAO,EACF,EAAO,EAAG,EAAO,EAAU,IAAQ,CAC5B,IAAR,EAA4B,IAAnB,EAAQ,OAAe,EAAI,EAAQ,GAC5C,EAAsB,IAAhB,EAAK,OAAe,EAAI,EAAK,GACnC,EAAgC,IAArB,EAAU,OAAe,EAAI,EAAU,GAEpD,EACF,EAAc,EAAO,GACrB,EACA,EAAc,EAAO,GAAK,EAI5B,GAAY,IAFA,EAAK,EAEF,CACb,GAAO,EACP,MAIF,IAFA,GAAU,GAED,GAAK,GAAM,EAAE,GAAO,CAC3B,GAAO,EACP,MAGF,EAAQ,KAAK,GAGf,IAAK,EAAM,CAET,IADA,IAAM,EAAqB,CAAC,EAAG,GACtB,EAAI,EAAG,EAAI,EAAU,IAC5B,EAAmB,KAAK,EAAE,GAAK,EAAc,EAAI,GAAK,GAIxD,GAFW,EAAE,IAAI,GACN,EAAE,IAAI,GAInB,EAAe,EAAe,EAAE,OAGlC,EAAE,IAAI,EAAe,GAErB,EAAe,EAAe,EAAE,QAMxC,OAAO,Ed0YE,CAAc,KAAM,EAAQ,EAAW,EAAO,EAAM,KAza/D,CAAI,IAAJ,WAAI,MAAJ,SA4aW,EAAgB,EAAe,GACtC,ODjfE,SACJ,EACA,EACA,EACA,GAKE,IAHA,IAAI,EAAO,EAAE,MAAM,OAEf,EAAW,YAAO,EAAE,OACjB,EAAI,EAAG,EAAI,EAAM,IACpB,EAAQ,IAAM,EAAK,GAAK,EAAK,EAAI,GAOrC,IAJA,IAAI,EAAI,IAAI,EAAU,OAAa,EAAW,EAAE,OAE5C,EAAK,IAAI,MAAM,GAAM,KAAK,GAC1B,EAAU,IAAI,MAAM,GAAM,KAAK,GAC5B,EAAI,EAAG,EAAI,EAAE,KAAM,IAAK,CAE/B,IADA,IAAI,GAAa,EACR,EAAI,EAAG,EAAI,EAAM,IACxB,EAAQ,GAAK,EAAG,GAAK,EAAK,IACtB,EAAQ,GAAK,GAAK,EAAQ,IAAM,EAAE,MAAM,MAC1C,GAAa,GAIb,EAAF,IAAI,EAAG,EAAY,EAAG,EAAS,EAAM,EAAO,IAE9C,EAAe,EAAI,GAGnB,OAAK,ECkdE,CAAI,KAAM,EAAM,EAAM,KA7ajC,CAAI,IAAJ,mBAAI,MAAJ,SAibI,EACA,EACA,EACA,GAEA,OezfE,SACJ,EACA,EACA,EACA,EACA,GAEE,IAAI,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,MAAM,GAElB,EAAW,EAAE,OAEb,EAAa,EAAQ,GAErB,EAAI,EACR,EACA,EACA,EAAK,MAAM,EAAG,EAAK,OAAS,GAC5B,EAAK,MAAM,EAAK,OAAS,GACzB,IAAI,MAAM,GAAU,KAAK,GACzB,GAEI,EAAa,EAAQ,GACvB,EAAc,CAAC,EAAG,GACpB,EAAY,EAAY,OAAO,GAK/B,IAHA,IAAI,EAAI,IAAI,EAAU,OAAa,EAAW,EAAE,OAGzC,EAAI,EAAG,EAAI,EAAGA,IAEjB,IAAC,IAAI,EAAI,EAAG,EAAI,EAAG,IAAK,CACpB,MAAgB,IAAI,MAAM,EAAE,QAAQ,KAAK,GACzC,EAAQ,QAAQ,EAAG,GACnB,IAAD,IAAI,EAAM,EAAG,EAAM,EAAY,IAAO,CAOjC,IANA,IAAJ,EAAS,EAEP,EAA0B,IAAI,MAAM,EAAE,QAAQ,KAAK,GAErD,EAAQ,EAEH,EAAM,EAAG,EAAM,EAAY,IAAO,CAIzC,IAHA,IAAM,EAAU,CAAC,EAAG,GAEhB,GAAO,EACF,EAAO,EAAG,EAAO,EAAU,IAAQ,CAC1C,IAAM,EAA4B,IAAnB,EAAQ,OAAe,EAAI,EAAQ,GAC5C,EAAsB,IAAhB,EAAK,OAAe,EAAI,EAAK,GAEnC,EACJ,EAAc,EAAO,GAAK,EAAS,EAAM,EAAc,GAEzD,GAAI,EAAK,GAAK,GAAM,EAAE,GAAO,CAC3B,GAAO,EACP,MAGF,EAAQ,KAAK,GAGV,IAEH,GADW,EAAE,IAAI,IAId,IAAQ,IACX,GAAS,GAGX,EAAe,EAAe,GAGhC,GAAkB,EAElB,EAAE,IAAI,EAAe,GAErB,EAAe,EAAe,EAAE,QAKtC,OAAO,EfuaE,CAAY,KAAM,EAAa,EAAM,EAAS,KAtbzD,CAAI,IAAJ,eAAI,MAAJ,SAybe,EAAiB,GAC5B,OAAI,EACK,KAAK,KAAK,GAEV,IAAI,EAAU,EAAO,KAAK,OAAQ,KAAK,SA7bpD,CAAI,IAAJ,SAAI,MAAJ,SAicS,EAAsB,GAC3B,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,4CAKlB,OAHI,EAAO,IACT,GAAQ,KAAK,MAAM,QgB3gBnB,SACJ,EACA,EACA,GAEE,IAAI,EAAc,YAAI,EAAE,OACxB,EAAU,IAAS,EAAE,MAAM,GAc3B,IAZA,IAAI,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEnD,EAAS,EACT,EAAS,EAET,EAAK,EAEH,EAAY,EAAO,QAAQ,GAAQ,EAAE,MAAM,GAC3C,EAAY,EAAO,QAAQ,GAAQ,EAAE,MAAM,GAE3C,EACJ,EAAO,MAAQ,EAAO,EAAI,EAAO,QAAQ,EAAO,GAAK,EAAO,MACrD,EAAI,EAAG,EAAI,EAAY,IAAK,CACnC,IAAK,IAAI,EAAI,EAAG,EAAI,EAAW,IAC7B,EAAO,IAAI,EAAI,EAAE,IAAI,IACrB,IACA,IAGF,IAAK,IAAI,EAAI,EAAG,EAAI,EAAW,IAC7B,EAAO,IAAI,EAAI,EAAE,IAAI,IACrB,IACA,IAIJ,OAAO,EhB2eE,CAAO,KAAM,EAAQ,KAxchC,CAAI,IAAJ,iBAAI,MAAJ,SA2ciB,GACb,OiBhhBE,SACJ,EACA,GAME,IAJA,IAAI,EAAO,EAAE,MAAM,OAEf,EAAc,IAAI,MAAM,GACxB,EAAc,IAAI,MAAM,GACrB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAY,GAAK,EAAE,MAAM,EAAY,IACjC,EAAQ,EAAY,IAAM,EAO9B,IAJA,IAAI,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEjD,EAAgB,EAAO,QACvB,EAAgB,IAAI,MAAM,GACvB,EAAI,EAAG,EAAI,EAAM,IACpB,EAAU,GAAK,EAAc,EAAY,IAI/C,IADE,IAAI,EAAQ,IAAI,MAAM,GAAM,KAAK,GAC1B,EAAI,EAAG,EAAI,EAAE,KAAM,IAAK,CAE/B,IADA,IAAI,EAAQ,EACH,EAAI,EAAG,EAAI,EAAM,IACxB,GAAS,EAAM,GAAK,EAAc,GAGpC,EAAO,IAAI,EAAO,EAAE,IAAI,IAExB,EAAe,EAAO,EAAE,OAG1B,OAAO,EjB+eE,CAAU,KAAM,KA5c3B,CAAI,IAAJ,SAAI,MAAJ,SA+cS,GACL,OkBphBE,SACJ,EACA,GAKE,IAHA,IAAI,EAAO,EAAE,MAAM,OAEf,EAAc,IAAI,MAAM,GACrB,EAAI,EAAG,EAAI,EAAM,IACpB,EAAQ,GAAK,EAAE,MAAM,GAAK,EAAQ,GAMtC,IAHA,IAAIF,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEjD,EAAQ,IAAI,MAAM,GAAM,KAAK,GAC1B,EAAI,EAAG,EAAI,EAAO,KAAM,IAAK,CAEpC,IADA,IAAM,EAAU,IAAI,MAAM,GACjB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAQ,GAAK,EAAM,GAAK,EAAE,MAAM,GAGlC,EAAO,IAAI,EAAG,EAAE,IAAI,IAEpB,EAAe,EAAO,EAAO,OAG/B,OAAO,ElB2fE,CAAO,KAAM,KAhdxB,CAAI,IAAJ,SAAI,MAAJ,SAmdS,GAAwB,MAEO,KAAK,YAAY,KAAK,MAAO,GAFpC,mBAEtB,EAFsB,KAER,GAFQ,WAG7B,OAAI,EAAc,KAAK,MAAO,GACrB,KAAK,OmB3hBZ,SACJ,EACA,GAOA,IALE,IAAI,EAAO,EAAE,MAAM,OAEf,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEjD,EAAQ,IAAI,MAAM,GAAM,KAAK,GAC1B,EAAI,EAAG,EAAI,EAAO,KAAM,IAC/B,EAAO,IAAI,EAAG,EAAE,IAAI,IAEpB,EAAe,EAAO,EAAO,OAG/B,OAAO,EnB8gBE,CAAO,KAAK,QAAQ,GAAQ,GAA2B,KAzdlE,CAAI,IAAJ,SAAI,MAAJ,SA4dS,EAAc,GACnB,OoBjiBE,SACJ,EACA,EACA,GAOE,IALA,IAAI,EAAI,EAAE,MAAM,OACZ,EAAI,EAAQ,MAAM,OAElB,EAAa,EAAI,EAAI,EACrB,EAAc,IAAI,MAAM,GACrB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAY,GAAK,EAAE,MAAM,GAEzB,IAAG,IAAI,EAAI,EAAG,EAAI,EAAG,IACrB,EAAY,EAAI,GAAQ,EAAQ,MAAM,GAEtC,IAAG,IAAI,EAAI,EAAO,EAAG,EAAI,EAAG,IACxB,EAAQ,EAAI,EAAI,GAAK,EAAE,MAAM,GAQnC,IALE,IAGE,EACA,EAJEA,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEjD,EAAQ,IAAI,MAAM,GAAY,KAAK,GAGhC,EAAI,EAAG,EAAI,EAAO,KAAM,IAAK,CACpC,EAAW,EAAM,MAAM,EAAM,EAAO,GACpC,IAAM,EAAO,EAAQ,IAAI,GACzB,EAAO,sBAAO,EAAM,MAAM,EAAG,IAAtB,CAA6B,GAA7B,YAAsC,EAAM,MAAM,EAAO,KAEhE,EAAO,IAAI,EAAG,EAAE,IAAI,IAEpB,EAAe,EAAO,GAGxB,OAAO,EpB8fE,CAAO,KAAM,EAAM,KA7d9B,CAAI,IAAJ,aAAI,MAAJ,SAieI,EACA,EACA,EACA,GAEA,OqB1iBE,SACJ,EACA,EACA,EACA,EACA,GAME,IAJA,IAAI,EAAO,EAAE,MAAM,OAEf,EAAW,YAAO,EAAE,OACtB,EAAO,EACF,EAAI,EAAG,EAAI,GAAQ,EAAO,EAAK,OAAQ,IAC1CF,IAAM,EAAK,KACb,EAAY,GAAK,KAAK,MAAM,EAAK,GAAQ,EAAO,IAAS,EAAM,IACzD,KASV,IALE,IAGE,EAHE,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEjD,EAAQ,IAAI,MAAM,GAAM,KAAK,GAG1B,EAAI,EAAG,EAAI,EAAO,KAAM,IAAK,CACpC,EAAO,IAAI,MAAM,GACjB,IAAK,IAAI,EAAI,EAAG,EAAI,EAAK,OAAQ,IAC/B,EAAK,EAAK,IAAM,EAAM,EAAK,IAAM,EAAM,GAAK,EAAO,GAGrD,EAAO,IAAI,EAAG,EAAE,IAAI,IAEpB,EAAe,EAAO,GAGxB,OAAO,ErBwgBE,CAAM,KAAM,EAAQ,EAAM,EAAM,KAte3C,CAAI,IAAJ,WAAI,MAAJ,SAyeW,GACP,OsB9iBE,SACJ,EACA,GAKE,IAHA,IAAI,EAAO,EAAE,MAAM,OAEf,EAAW,YAAO,EAAE,OACjB,EAAI,EAAG,EAAI,EAAM,IACpB,EAAQ,GAAK,KAAK,MAAM,EAAY,GAAK,EAAO,IAOtD,IAJE,IAAIE,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEjD,EAAQ,IAAI,MAAM,GAAM,KAAK,GAC7B,EAAO,IAAI,MAAM,GACd,EAAI,EAAG,EAAI,EAAO,KAAM,IAAK,CACpC,IAAK,IAAI,EAAI,EAAG,EAAI,EAAMG,IACxB,EAAK,GAAK,KAAK,MAAM,EAAM,GAAK,EAAO,IAGzC,EAAO,IAAI,EAAG,EAAE,IAAI,IAEpB,EAAe,EAAO,GAGxB,OAAO,EtBqhBE,CAAS,KAAM,KA1e1B,CAAI,IAAJ,YAAI,MAAJ,SA8eI,EACA,EACA,EACA,EACA,GAEA,KACI,aAAgB,MAChB,aAAoB,MACpB,aAAiB,MACjB,aAAgB,GAElB,MAAM,IAAI,MAAM,uCAElB,OuBhkBE,SACJ,EACA,EACA,EACA,EACA,EACA,GASA,IAPE,IAAI,EAAO,EAAE,MAAM,OAEf,EAAW,YAAO,EAAE,OAEpB,EAAS,IAAI,EAAU,OAAa,EAAW,EAAE,OAEjD,EAAQ,IAAI,MAAM,GAAM,KAAK,GAC1B,EAAI,EAAG,EAAI,EAAO,KAAM,IAAK,CACpC,IAAI,GACD,EAAE,IAAI,GAAS,EAAK,IAAI,IACzB,KAAK,KAAK,EAAS,IAAI,GAAS,GAElC,EAAM,EAAM,EAAM,IAAI,GAAS,EAAK,IAAI,GAExC,EAAO,IAAI,EAAG,GAEd,EAAe,EAAO,GAGxB,OAAO,EvBqiBE,CAAU,KAAM,EAAM,EAAU,EAAS,EAAO,MA5f3D,EAAI,IAAJ,QAAI,MAAJ,SACe,EAAe,EAAe,GAGzC,IAFA,IAAM,EAAO,KAAK,IAAI,KAAK,MAAM,EAAQ,GAAS,GAAQ,GACpD,EAAS,IAAI,aAAa,GACvB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAO,GAAK,EAAQ,EAAI,EAE1B,OAAO,IAAI,EAAU,CAAC,GAAO,OAPjC,GAA+D,G,gCwBtElD,EAAb,WAKI,SAAF,EAAoB,GAA4B,oBAA5B,gBAFb,gBAAa,EAGd,KAAC,KAAO,GANd,uBAAE,IAAJ,qBAAI,MAAJ,SASqB,GACX,QAAY,IAAd,EAAM,IAAmB,CACnB,IAAF,EAAI,KAAK,SAAS,EAAM,KACtB,YAAa,IAAjB,KAAK,KAAK,IAAoB,KAAK,KAAK,GAAG,OAAS,EAC/C,CACL,CACE,IAAK,EAAM,IACX,MAAO,KAAK,KAAK,GAAG,KAAK,KAAK,GAAG,OAAS,KAIzC,GACF,QAAkB,IAAd,EAAM,IAAmB,CAC1B,IAAF,EAAI,KAAK,SAAS,EAAM,KACtB,YAAa,IAAjB,KAAK,KAAK,IAAoB,KAAK,KAAK,GAAG,OAAS,EAC/C,CACL,CACE,IAAK,EAAM,IACX,MAAO,KAAK,KAAK,GAAG,KAAK,KAAK,GAAG,OAAS,KAIzC,GAET,MAAO,KAjCX,CAAI,IAAJ,cAAI,MAAJ,SAoCc,GACV,IAAM,EAAI,KAAK,SAAS,QACH,IAAjB,KAAK,KAAK,KACZ,KAAK,KAAK,GAAG,MACb,KAAK,gBAxCX,CAAI,IAAJ,SAAI,MAAJ,SA4CS,EAAQ,GACb,IAAM,EAAI,KAAK,SAAS,QACH,IAAjB,KAAK,KAAK,KACZ,KAAK,KAAK,GAAK,IAEjB,KAAK,KAAK,GAAG,KAAK,GAClB,KAAK,iBAlDT,KCFM,SAAUC,EAAaC,GACzB,OAAK,EAAoB,GAGvB,SAAU,EACd,GAEa,IADb,EACa,uDADM,GACnB,IAAa,yDAEX,IAAG,OAAO,UAAU,GAAW,OAAO,EAEtC,IAAIA,EAAM,KAAK,IAAI,GACnB,GAAE,EAAM,EAAG,OAAO,EAClB,IAAI,EAAO,KAAK,KAAK,GAEnB,EAAI,EACN,GAAE,EAAM,GAEJ,GADA,KACS,CACL,EAAF,EAEE,IADA,IAAF,EAAM,EACH,EAAM,GAAK,EAAI,GAEpB,GAAK,EAEL,EAAM,EAAI,EAOd,GAFA,EAAE,GAAK,EAAO,EAAI,EAEf,EAGE,EAAO,KAAK,OAHN,CACX,IAAM,EAAQ,EAAO,QAAQ,GACzB,EAAQ,GAAG,EAAO,KAAK,GAG7B,OAAO,IAAM,EAAM,EAAS,EAAoB,EAAM,EAAG,EAAQ,GAM7D,SAAU,IAGZ,IAFA,IAAE,EAAK,EACL,EAAK,EACK,IAAP,GAAU,EAAK,KAAK,SAC3B,KAAc,IAAP,GAAU,EAAK,KAAK,SAI3B,MAAO,CAFI,KAAK,MAAM,EAAI,KAAK,IAAI,IAAO,KAAK,IAAI,EAAI,KAAK,GAAK,GACtD,KAAK,MAAM,EAAI,KAAK,IAAI,IAAO,KAAK,IAAI,EAAI,KAAK,GAAK,IC7B5D,ICfI,EACA,GACA,GDaE,GAAY,CACrB,QAAO,QACP,QAAO,aACP,MAAK,QACL,MAAK,QACL,KAAI,QACJ,OAAM,QACN,OAAM,QACN,MAAK,SAKI,GAAb,WAWI,SAAF,EACE,EACQ,EACR,GAAsB,oBADV,KAAJ,yBAJH,sBAAmB,EAOpB,KAAC,MAAQ,GAET,KAAC,KAAO,EACR,KAAC,QAAU,EAEX,KAAC,cAAgB,GAAiB,EArBxC,uBAAE,IAAJ,eAAI,MAAJ,SAwBe,GAEL,OAAC,GAAU,KA1BrB,CAAI,IAAJ,aAAI,MAAJ,SA6Ba,GACH,MAAQ,YAAV,EACK,UAOA,YAtCb,CAAI,IAAJ,WAAI,MAAJ,SA0CW,EAAc,GACrB,IAAM,EAAQ,KAAK,WAAW,GAE1B,EAAa,EAAO,KAAK,cACvB,EAhDW,EAgDD,KAAK,KAAK,EAhDT,GAiDb,EAAU,IACZ,EAAa,GAGf,IAAM,OACkB,IAAtB,KAAK,MAAM,GACP,KAAK,MAAM,GAAO,mBAAmB,CACnC,IAAK,EACL,IAAK,IAEP,GAEA,GAAiB,IAAnB,EAAQ,OAAc,CACxB,IAAM,EAAc,KAAK,KAAK,EA9Df,GA6DS,EAEA,KAAK,eAAe,GAArC,EAFiB,EAEjB,MAAO,EAFU,EAEV,OAUR,EAA2B,CAC/B,MAAO,EACP,OAAQ,EACR,KAAM,EAAQ,EA5ED,EA6Eb,YAZkB,KAAK,KAAK,YAAY,CACxC,MAAO,EACP,OAAQ,EACR,cAAc,EACd,YAAa,OACb,UAAW,GAAU,KAQrB,GAAI,KAAK,UACT,MAAO,GAKT,OAFA,KAAK,mBAEE,EAEP,IAAM,EAAQ,EAAQ,GAKtB,OAJA,KAAK,MAAM,GAAO,YAAY,EAAM,KAEpC,EAAM,MAAM,MAAQ,EAEb,EAAM,QAzFnB,CAAI,IAAJ,0BAAI,MAAJ,SA6F0B,EAAc,GAC9B,MAAQ,KAAK,WAAW,GAE1B,EAAa,EAAOC,KAAK,cACvB,EAnGW,EAmGD,KAAK,KAAK,EAnGT,GAoGb,EAAU,IACZ,EAAa,GAGf,IAAM,OACkB,IAAtB,KAAK,MAAM,GACP,KAAK,MAAM,GAAO,mBAAmB,CACnC,IAAK,EACL,IAAK,IAEP,GAEN,GAAuB,IAAnB,EAAQ,OAAc,CACxB,IAAM,EAAc,KAAK,KAAK,EAjHf,GAkHf,OAAO,KAAK,eAAe,GAE3B,IAAM,EAAQ,EAAQ,GAEtB,MAAO,CACL,MAAO,EAAM,MAAM,MACnB,OAAQ,EAAM,MAAM,UAtH5B,CAAI,IAAJ,aAAI,MAAJ,SA2Ha,GACH,MAAQ,KAAK,WAAW,EAAM,YAEV,IAAtBA,KAAK,MAAM,KACb,KAAK,MAAM,GAAS,KAAK,0BAErB,KAAD,MAAM,GAAO,OAAO,EAAM,KAAM,KAjIzC,CAAI,IAAJ,kBAAI,MAAJ,SAoIkB,EAAkB,GAM1B,IALA,MAAc,KAAK,KAAK,EAAO,OAvIpB,GAsI8B,EAEvB,KAAK,eAAe,GAArC,EAFwC,EAExC,MAAO,EAFiC,EAEjC,OACR,EAAY,EAAQ,EAzIT,EA2IX,EAAO,IAAI,MAAM,GACd,EAAI,EAAG,EAAI,EAAO,OAAQ,IACjC,EAAK,GAAK,EAAO,GAEb,IAAD,IAAI,EAAI,EAAO,OAAQ,EAAI,EAAW,IACzC,EAAK,GAAK,EAGZ,IAAM,EAAU,KAAK,KAAK,QAAQ,CAChC,MAAO,EACP,OAAQ,EACR,OAAQ,OACR,KAAM,GAAU,GAEhB,KAAM,IAGF,EAAc,KAAK,KAAK,YAAY,CACxC,MAAO,EACP,MAAO,EACP,OAAQ,EACR,cAAc,IAKhB,OAFM,KAAD,mBAEE,CACL,MAAO,EACP,OAAQ,EACR,KAAM,EACN,YAAa,EACb,GAAI,KAAK,UACT,MAAO,KAzKb,CAAI,IAAJ,uBAAI,MAAJ,SA8KI,EACA,EACA,GAEA,IAAM,EAAY,EAAQ,EApLT,EAsLX,EAAU,KAAK,KAAK,QAAQ,CAChC,MAAO,EACP,OAAQ,EACR,OAAQ,OACR,KAAM,GAAU,KAGZ,EAAc,KAAK,KAAK,YAAY,CACxC,MAAO,EACP,MAAO,EACP,OAAQ,EACR,cAAc,IAKhB,OAFA,KAAK,mBAEE,CACL,MAAO,EACP,OAAQ,EACR,KAAM,EACN,YAAa,EACb,GAAI,KAAK,UACT,MAAO,KA1Mb,CAAI,IAAJ,sBAAI,MAAJ,SA8MsB,EAAyB,GAC3C,IAAM,EAAc,KAAK,KAAK,YAAY,CACxC,MAAO,EACP,MAAO,EAAQ,MACf,OAAQ,EAAQ,OAChB,cAAc,EACd,UAAW,GAAU,KAKvB,OAFA,KAAK,mBAEE,CACL,MAAO,EAAQ,MACf,OAAQ,EAAQ,OAChB,KAAM,EAAQ,MAAQ,EAAQ,OA9Nf,EA+Nf,YAAa,EACb,GAAI,KAAK,UACT,MAAO,KA/Nb,CAAI,IAAJ,iBAAI,MAAJ,SAmOyB,GAIrB,IAHA,IAAM,EAAU,EAAa,GACzB,EAAQ,EACR,EAAS,EACJ,EAAI,EAAG,EAAI,EAAQ,OAAQ,GAAK,EACvC,GAAS,EAAQ,GACb,EAAI,EAAI,EAAQ,SAClB,GAAU,EAAQ,EAAI,IAI1B,MAAO,CAAC,QAAO,YA9OnB,CAAI,IAAJ,gBAAI,MAAJ,WAkPI,IAAI,EAAM,EACV,IAAK,IAAM,KAAS,KAAK,MACvB,GAAO,KAAK,MAAM,GAAO,WAE3B,OAAO,MAtPX,KC9BM,GAAS,SAAS,cAAc,UAQlC,EAAU,GAAO,WAAW,QAAS,CACjC,8BAA0B,IAG9B,GAAG,IAAK,CACJ,KACA,WAAQ,CACV,oBACA,2BACA,4BAIJ,GAAmB,IAAI,GAAmB,IAAI,WAC5C,OAAO,IAAI,GAAK,SAAC,GAAD,OAAiB,QChB9B,IAqBe,GAAtB,WAwBI,SAAF,EACE,EACU,EACV,EACA,GAAgB,oBAFN,aAnBF,aAAuB,IAAI,IAY7B,iBAAc,EAEZ,kBAAc,OASJ,IAAd,IACF,EAAY,SAEE,IAAZ,IACF,EAvDwB,IAyD1B,KAAK,UAAY,EACb,KAAC,QAAU,EAEX,KAAC,qBAAuB,EAvC9B,uBAAE,IAAJ,kBAAI,MAAJ,SA0CkB,GACd,IAAI,EAAiB,EACjB,EAAa,EAEjB,IAAK,IAAM,KAAO,EAChB,GAAI,EAAI,WAAW,SAAU,CAC3B,IAAM,EAAU,EAAI,MAAM,QAAQ,QAClC,KAAK,QAAQ,IAAb,eAAyB,IACzB,KAAK,QAAQ,IAAb,cAAwB,IACxB,KAAK,QAAQ,IAAb,cAAwB,IACxB,KAAK,QAAQ,IAAb,iBAA2B,IAC3B,SAEA,KAAK,QAAQ,IAAI,GACZ,EAAI,WAAW,UAAa,EAAI,WAAW,WAC9C,IAMJ,EAAiB,IAAM,KAAK,kBAAkB,QAC9C,IAAe,KAAK,kBAAkB,SAEtC,KAAK,aAAc,EACnB,KAAK,YAAc,EAAI,eAnE7B,CAAI,IAAJ,iBAAI,MAAJ,SA0EiB,GACb,OAAO,KAAK,QAAQ,IAAI,GAAQ,GAAK,YA3EzC,CAAI,IAAJ,MAAI,MAAJ,SAiFM,GACI,IAD6B,IAAlB,EAAkB,uDAAZ,KAAK,QACrB,EAAI,OAAS,GACV,EAAJ,MAAM,GAEN,OAAC,IArFX,CAAI,IAAJ,UAAI,MAAJ,SAwFU,GAEA,IAF0C,IAAlB,EAAkB,uDAAZ,KAAK,QACnC,EAAS,MAAM,KAAK,GACnB,EAAO,OAAS,GACb,EAAD,MAAM,GAET,OAAC,IA7FX,CAAI,IAAJ,eAAI,MAAJ,WAqGU,MAAC,KArGX,CAAI,IAAJ,0BAAI,MAAJ,WAwGyB,WACf,EAAW,KAAK,kBAGhB,OAFN,EAAS,KAAK,UAEd,kBACI,EACC,KAAI,YACH,0BACM,WAAN,EAAiB,GAAjB,4BAA2C,EAA3C,KADA,qBAEA,EAAK,eAAe,OAAS,GAF7B,oBAE2C,EAF3C,sBAGA,EAAK,eAAe,QAAU,GAH9B,qBAG6C,EAH7C,sBAIA,EAAK,eAAe,SAAW,GAJ/B,sBAI+C,EAJ/C,sBAKA,EAAK,eAAe,UAAY,GALhC,uBAKiD,EALjD,YAKsD,EAAK,QAL3D,uBAMA,EAAK,eAAe,QAAU,GAN9B,qBAM6C,EAN7C,YAMkD,EAAK,QANvD,uBAOA,EAAK,eAAe,OAAS,GAP7B,oBAO2C,EAP3C,kBAUD,KAAK,MAbV,6CAgBI,KAAK,kBA5Hb,CAAI,IAAJ,6BAAI,MAAJ,SA+H6B,GACnB,MAAW,KAAK,kBAChB,EAAG,KAAK,UAER,IAJ6B,EAI/B,EAAQ,GAJuB,cAKjB,GALiB,IAK3B,IAAR,uBAA4B,KAAjB,EAAiB,QAChB,GAAN,eAAQ,KAAS,EAAM,CACzB,IAAM,EAAQ,EAAI,eAAS,IACrB,EAAU,EAAe,GACzB,EAAO,EAAQ,GACf,EAAO,EAAM,OAEnB,GAAS,KAAK,aAAL,eAA0B,GAAO,GAC1C,GAAS,KAAK,aAAL,iBAA4B,GAAO,GAC5C,GAAK,gBAAa,EAAb,cAAsB,EAAtB,KACL,GAAK,gBAAa,EAAb,cAAsB,EAAtB,OAf0B,8BAkBnC,IAAK,IAAM,KAAK,EAAM,CACpB,IAAK,EAAE,WAAW,SAChB,GAAI,MAAM,QAAQ,EAAK,IACrB,GAAS,KAAK,aAAa,EAAG,EAAK,SAKjC,GADW,QAFA,KAAK,WAAW,GAGtB,YAAS,EAAT,cAAgB,EAAK,GAArB,KAEA,YAAS,EAAT,cAAiB,EAAK,GAAc,YAAY,IAAhD,KAMb,OAAO,IAjKX,CAAI,IAAJ,aAAI,MAAJ,SAoKa,GACT,IAAM,EAAM,KAAK,kBAAkB,MAAK,YAAC,OAAI,EAAE,OAAS,KACxD,YAAY,IAAR,GACK,EAAI,KAAO,EAAI,KAEjB,QAzKX,CAAI,IAAJ,eAAI,MAAJ,SA6Ke,EAAc,EAAe,EAAc,QAC1C,IAAR,IACF,EAAM,KAAK,SAGb,IAAM,EAAO,KAAK,WAAW,QAEjB,IAAR,IACW,QAAT,EACF,EAAM,KACY,UAAT,IACT,EAAM,SAIV,IADA,IAAI,EAAM,GACD,EAAI,EAAG,EAAI,EAAK,IACnB,EAAI,EAAO,OACA,QAAT,EACF,GAAG,aAAU,EAAV,YAAkB,EAAlB,eAA0B,EAAO,GAAjC,KACe,UAAT,IACT,GAAG,aAAU,EAAV,YAAkB,EAAlB,eAA2B,EAAO,GAAc,YAAY,IAA5D,MAGL,GAAG,aAAU,EAAV,YAAkB,EAAlB,eAA0B,EAA1B,KAGP,OAAO,IAvMX,CAAI,IAAJ,mBAAI,MAAJ,WA2MU,MAAN,gqBAuB2B,KAAK,QAvBhC,0BAuByD,KAAK,QAvB9D,+DAyBwB,KAAK,QAzB7B,4xBAoD6B,KAAK,QApDlC,0BAoD2D,KAAK,QApDhE,sLA3MJ,CAAI,IAAJ,sBAAI,MAAJ,WAqQqB,WAGX,OAFW,KAAK,kBAGnB,KAAI,YACK,MAAR,2BACS,EADT,wBAC0B,EAAK,QAD/B,6DAEsC,EAFtC,kBAEiD,EAFjD,mBAE6D,EAF7D,aAEmE,EAFnE,4BAMD,KAAK,QAhRZ,CAAI,IAAJ,4BAAI,MAAJ,SAmR4B,GACxB,IAAM,EAAa,KAAK,kBAAkB,GAEpC,EAAgB,KAAK,0BACrB,EAAW,KAAK,2BAA2B,GAE3C,EAAgB,KAAK,mBACrB,EAAmB,KAAK,sBAiB9B,MAfY,sEAEA,KAAK,kBAFL,0BAIV,EAJU,mBAMV,EANU,iBAOV,EAPU,4CAUR,EAVQ,0BAaV,KAzSN,CAAI,IAAJ,cAAI,MAAJ,SA8Sc,GACJ,IADc,EACd,EAAwB,GAExB,EAA+B,KAAK,kBAHtB,cAIM,GAJN,IAIZ,IAAR,uBAA+C,KAApC,EAAoC,aACd,IAA3B,EAAK,EAAY,OACnB,EAAa,KAAK,IANF,8BAUpB,IAAM,EAAW,KAAK,kBACtB,EAAS,KAAK,UAXM,oBAYE,GAZF,IAYpB,2BAAgC,KAArB,EAAqB,QAC9B,EAAa,KAAK,CAAC,KAAM,SAEO,IAA5B,EAAI,eAAS,MACf,EAAa,KAAK,CAAC,KAAI,cAAS,KAChC,EAAa,KAAK,CAAC,KAAI,iBAAY,GAAW,OAAQ,KAAK,UAC3D,EAAa,KAAK,CAAC,KAAI,eAAU,GAAW,OAAQ,KAAK,UACzD,EAAa,KAAK,CAAC,KAAI,cAAS,WAEF,IAA5B,EAAI,eAAS,KACf,EAAa,KAAK,CAAC,KAAI,eAAU,UAEF,IAA7B,EAAI,gBAAU,KAChB,EAAa,KAAK,CAAC,KAAI,gBAAW,MAzBlB,8BA+Bd,IADN,IAAM,EAAgB,GACtB,MAA0B,EAA1B,eAAwC,CAAnC,IAAM,EAAW,KACpB,QAA+B,IAA3B,EAAK,EAAY,MACnB,QAA2B,IAAvB,EAAY,OACd,IAAK,IAAI,EAAI,EAAG,EAAI,EAAY,OAAQ,IAAK,CAC3C,IAAM,EAAI,UAAM,EAAY,KAAlB,YAA0B,EAA1B,KACV,EAAS,GAAQ,GAAG,KAAK,QAG3B,EAAS,EAAY,MAAQ,GAAG,KAAK,EAAY,MAKvD,OAAO,IA1VX,CAAI,IAAJ,aAAI,MAAJ,SA6Va,EAAiB,EAAgB,GAC1C,IAAM,EAAI,UAAM,EAAN,YAAa,KAAK,eAC5B,0BACM,EADN,cACgB,EADhB,sCAEsB,KAAK,QAF3B,+BAGQ,EAHR,iCAIM,EAJN,kDAMU,EANV,kCAOQ,EAPR,iDASQ,EATR,iBASuB,EATvB,YAS+B,EAT/B,2BAUQ,EAVR,cAUkB,EAVlB,cAU4B,EAV5B,eAU0C,EAV1C,2DA/VJ,CAAI,IAAJ,YAAI,MAAJ,SA+WY,EAAe,GACvB,YAAa,IAAT,EACF,wCACwB,KAAK,QAD7B,+BAEM,EAFN,wBAKA,wCACwB,KAAK,QAD7B,uCAEc,EAFd,4BAGQ,EAHR,qDAKQ,EALR,uCAtXN,CAAI,IAAJ,iBAAI,MAAJ,SAiYiB,EAAe,GACtB,MAAN,6BACe,KAAK,QADpB,2CAEQ,EAFR,iCAGM,EAHN,kCAIU,EAJV,kBAIyB,EAJzB,6BAKQ,EALR,mFAlYJ,CAAI,IAAJ,uBAAI,MAAJ,SAgZuB,EAAe,EAAe,GAC3C,MAAN,oCACsB,KAAK,QAD3B,+BAEQ,EAFR,gCAGM,EAHN,kCAIU,EAJV,kBAIyB,EAJzB,6BAKQ,EALR,qFASe,EATf,wDAjZJ,CAAI,IAAJ,iBAAI,MAAJ,WA8aI,2QAUgB,KAAK,QAVrB,uBAWM,KAAK,WAAW,gBAAiB,QAAS,OAXhD,mHAiBQ,KAAK,WAAW,gBAAiB,QAAS,OAjBlD,2HAuBU,KAAK,WAAW,gBAAiB,QAAS,OAvBpD,mIA6BY,KAAK,WAAW,gBAAiB,QAAS,OA7BtD,uIA9aJ,CAAI,IAAJ,kBAAI,MAAJ,WAwdU,MAAgB,YAAf,KAAK,MAAsB,UAAY,UAxdlD,CAAI,IAAJ,iBAAI,MAAJ,SA2diB,GACP,MAAa,KAAK,0BAA0B,GAE5C,EAAW,KAAK,YAAY,GAwBlC,OAtBe,GAAG,CAChB,KAAM,EACN,KAAI,yEAEU,KAAK,kBAFf,gMASJ,WAAY,CACV,SAAU,EAAE,GAAI,EAAG,GAAI,EAAG,EAAG,IAE/B,SAAU,EACV,YAAa,GAAG,KAAK,eACrB,MAAO,CACL,QAAQ,GAEV,MAAO,MAnfb,CAAI,IAAJ,UAAI,MAAJ,SA8fU,GACN,KAAK,gBAAgB,GAErB,KAAK,YAAc,KAAK,eAAe,KAjgB3C,CAAI,IAAJ,UAAI,MAAJ,SAqgBI,EACA,EAEA,QAEyB,IAArB,KAAK,aACP,KAAK,QAAQ,IAGf,IAAM,EAAa,EAAQ,GAErB,EAAS,KAAK,UAAU,SAAS,EAAY,KAAK,OAExD,IAAK,IAAM,KAAK,EAAc,CAE5B,GADU,EAAa,GACjB,OAAO,KAAO,EAAO,GACzB,MAAM,IAAI,MAAJ,+IAKV,IAAM,EAAiD,GACvD,IAAK,IAAM,KAAQ,EACjB,EAAc,GAAQ,EAAa,GAAM,OAAO,YAMlD,QAJe,IAAX,IACF,EAAS,KAGN,KAAK,YAAa,CACrB,IAAK,IAAM,KAAQ,EACZ,KAAK,QAAQ,IAAb,eAAyB,MAC5B,EAAM,cAAQ,IAAU,EAAa,GAAM,KAC3C,EAAM,iBAAW,IAAU,KAAK,IAC9B,EAAe,EAAa,GAAM,QAEpC,EAAM,eAAS,IAAU,KAAK,QAAQ,EAAa,GAAM,OACzD,EAAM,cAAQ,IAAU,EAAa,GAAM,MAAM,QAG9C,KAAK,QAAQ,IAAb,eAAyB,MAC5B,EAAM,eAAS,IAAU,EAAa,GAAM,OAAO,OAGhD,KAAK,QAAQ,IAAb,gBAA0B,MAC7B,EAAM,gBAAU,IAAU,EAAa,GAAM,OAAO,QAInD,KAAK,QAAQ,IAAI,iBACpB,EAAM,WAAiB,EACvB,EAAM,cAAoB,KAAK,IAAI,EAAe,IAClD,EAAM,YAAkB,KAAK,QAAQ,GACrC,EAAM,WAAiB,EAAY,QAGhC,KAAK,QAAQ,IAAI,iBACpB,EAAM,YAAkB,EAAO,OAG5B,KAAK,QAAQ,IAAI,kBACpB,EAAM,aAAmB,EAAO,QAYpC,OAPA,KAAK,YAAW,6BACd,YAAa,EAAO,aACjB,GACA,IAIE,KAAK,qBAAqB,EAAQ,EAAa,KAAK,aA9kB/D,CAAI,IAAJ,kBAAI,MAAJ,WA6lBI,MAAO,OA7lBX,KCJa,GAAb,YAAE,qBAAF,iBAOI,SAAF,EACE,EACA,EACA,GAA8B,kCAE9B,cAAM,EAAmB,EAAO,IAPxB,cAAgB,IASxB,EAAK,QAAU,EAJe,EAVhC,uBAAE,IAAJ,oBAAI,MAAJ,SAkBoB,GAChB,8CAC0B,KAAK,QAD/B,+BAEY,KAAK,QAFjB,qBAGI,KAAK,UAAU,OAHnB,uDAMY,KAAK,QANjB,qBAOI,KAAK,UAAU,OAPnB,yHAcwB,KAAK,cAd7B,iPA6BE,KAAK,iBA7BP,YAnBJ,CAAI,IAAJ,kBAAI,MAAJ,WAqDI,MAAO,CAAC,IAAK,OArDjB,CAAI,IAAJ,OAAI,MAAJ,SAwDO,GACH,IAAM,EAAc,KAAK,eAAe,GAExC,OAAO,KAAK,QAAQ,EAAa,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,MA3D3D,CAAI,IAAJ,iBAAI,MAAJ,SA8DiB,GACb,MAAO,CAAC,EAAM,EAAE,MAAM,GAAI,EAAM,EAAE,MAAM,MA/D5C,CAAI,IAAJ,UAAI,MAAJ,SAkEU,QACc,IAAhB,EAAK,OACP,KAAK,cAAgB,EAAK,OAAO,QACR,IAAhB,EAAK,SACd,KAAK,cAAgB,EAAK,OAAO,IAGnC,+DAAc,KAzElB,CAAI,IAAJ,qBAAI,MAAJ,SA4EqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,UA9F/B,CAAI,IAAJ,qBAAI,MAAJ,SAkGqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,EAAE,WAnGvC,GAAmE,ICJ7C,GAAtB,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,oBAAI,MAAJ,SAgBoB,GACV,MAAN,yEAImB,KAAK,UAAU,oBAJlC,oBAjBJ,CAAI,IAAJ,kBAAI,MAAJ,WA2BI,MAAO,CAAC,OA3BZ,CAAI,IAAJ,OAAI,MAAJ,SA8BO,GACH,OAAO,KAAK,QAAQ,EAAM,MAAM,MAAO,CAAC,EAAG,EAAM,UA/BrD,CAAI,IAAJ,iBAAI,MAAJ,SAkCiB,GACb,OAAO,EAAM,MAAM,QAnCvB,CAAI,IAAJ,UAAI,MAAJ,SAsCU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KA3ClB,CAAI,IAAJ,qBAAI,MAAJ,SA8CqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,MAAM,MACpB,OAAQ,EAAM,MAAM,OAAO,MAC3B,QAAS,EAAM,MAAM,OAAO,OAC5B,YAAa,KAAK,eAAe,GACjC,YAAa,EAAW,MACxB,aAAc,EAAW,UA3D/B,CAAI,IAAJ,qBAAI,MAAJ,SA+DqB,GACjB,gBAAU,EAAM,MAAM,WAhE1B,GAIU,ICnBG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,oBAAc,EAAd,SAZJ,GAEU,ICkCG,GAAb,kDAOI,SAAF,EACE,EACA,EACA,GAA8B,kCAE1B,EAAJ,YAAM,EAAmB,EAAO,IAPxB,cAAgB,IAKM,EAVlC,4DAgBI,0CACsB,KAAK,QAAU,EADrC,oWAhBJ,oCAmCI,mFAIe,KAAK,QAJpB,mBAKE,KAAK,UAAU,YALjB,qCAMsB,KAAK,QAN3B,iJAac,KAAK,QAbnB,mBAcE,KAAK,UAAU,WAdjB,8DAiBwB,KAAK,cAjB7B,wNA0B4B,KAAK,cA1BjC,8HAiCM,KAAK,gBAjCX,wGAuCM,KAAK,eAAe,WAAY,UAvCtC,4BAnCJ,qCAiFI,sBACE,KAAK,eAAe,MADtB,yBAEE,KAAK,eAAe,cAFtB,iCAGE,KAAK,eAAe,YAHtB,+BAIE,KAAK,eAAe,KAJtB,wBAKE,KAAK,eAAe,aALtB,0BAKoD,KAAK,QALzD,mBAME,KAAK,eAAe,QANtB,qBAM0C,KAAK,QAN/C,mBAOE,KAAK,eAAe,WAPtB,wBAOgD,KAAK,QAPrD,mBAQE,KAAK,eAAe,cARtB,4BAjFJ,wCA8FoB,GAChB,8CAC0B,KAAK,QAD/B,iDAII,KAAK,cAJT,kMAeE,KAAK,iBAfP,YA/FJ,wCAmHI,MAAO,CAAC,IAAK,OAnHjB,wCAuHI,MAAO,CACL,CAAC,KAAM,MACP,CAAC,KAAM,cACP,CAAC,KAAM,KACP,CAAC,KAAM,YACP,CAAC,KAAM,OAAQ,OAAuB,EAAf,KAAK,SAC5B,CAAC,KAAM,UAAW,OAAQ,KAAK,SAC/B,CAAC,KAAM,YAAa,OAAQ,KAAK,SACjC,CAAC,KAAM,iBA/Hb,wCAmIoB,GAChB,MAAmB,OAAf,EACK,EACiB,SAAf,EACF,EAEA,IAzIb,2BA6IO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,IAGxD,MAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,GAClB,EAAK,EAAM,EAAE,MAAM,GAEnB,EAAa,EAAQ,GAErB,EAAI,EACR,EACA,EACA,EAAM,KAAK,MAAM,EAAG,EAAM,KAAK,OAAS,GACxC,EAAM,KAAK,MAAM,EAAM,KAAK,OAAS,GACrC,EAAM,UACN,EAAM,SAEJ,EAAc,CAAC,EAAG,GAGtB,OAFA,EAAc,EAAY,OAAO,GAE1B,KAAK,QACV,EACA,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,GACtB,CACE,KACA,aACA,IACA,SAAU,EAAE,OACZ,KAAM,KAAK,QAAQ,EAAM,KAAqB,EAAf,KAAK,SACpC,QAAS,KAAK,QAAQ,EAAM,SAC5B,UAAW,KAAK,QAAQ,EAAM,WAC9B,WAAY,KAAK,kBAAkB,EAAM,gBAjLjD,qCAsLiB,GACb,IAAM,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,GAElB,EAAI,EACR,EACA,EACA,EAAM,KAAK,MAAM,EAAG,EAAM,KAAK,OAAS,GACxC,EAAM,KAAK,MAAM,EAAM,KAAK,OAAS,GACrC,EAAM,UACN,EAAM,SAEJ,EAAc,CAAC,EAAG,GAGtB,OAFA,EAAc,EAAY,OAAO,KArMrC,8BA0MU,QACc,IAAhB,EAAK,SACP,EAAK,GAAK,EAAK,OAAO,GACtB,EAAK,WAAa,EAAQ,EAAK,OAAO,MAAM,IAC5C,EAAK,SAAW,EAAK,OAAO,OAAS,EACrC,KAAK,QAAU,EAAK,OAAO,aAET,IAAhB,EAAK,SACP,EAAK,EAAI,EAAK,OAAO,GACrB,EAAK,SAAW,EAAK,OAAO,OAAS,EAErC,KAAK,QAAU,EAAK,OAAO,aAEL,IAApB,EAAK,YAAuD,kBAApB,EAAK,aAC/C,EAAK,WAAa,KAAK,kBAAkB,EAAK,aAGhD,+DAAc,KA3NlB,yCA8NqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGD,EAAa,EAAQ,EAAM,EAAE,MAAM,MAAM,IAEzC,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GAE9B,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,KAAM,EAAM,KACZ,UAAW,EAAM,UACjB,QAAS,EAAM,QAEf,GAAI,EAAM,EAAE,MAAM,GAClB,WAAY,EACZ,SAAU,EAAE,OACZ,EAAG,EACH,WAAY,KAAK,kBAAkB,EAAM,eA/P/C,yCAmQqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,EAAE,MAAnC,YAA4C,EAAM,UAAlD,YAA+D,EAAM,KAArE,YAA6E,EAAM,UAAnF,YAAgG,EAAM,QAAtG,YAAiH,EAAM,gBApQ3H,GAIU,IA8QG,GAAb,kDAKE,WACE,EACA,EACA,GAA8B,kCAE9B,cAAM,EAAmB,EAAO,IAPxB,cAAgB,IAKM,EARlC,8DAcoB,GAChB,8CAC0B,KAAK,QAD/B,qCAEkB,KAAK,QAFvB,qBAGI,KAAK,UAAU,aAHnB,uFAOI,KAAK,cAPT,kMAkBE,KAAK,iBAlBP,YAfJ,wCAsCI,MAAO,CAAC,IAAK,IAAK,OAtCtB,2BAyCO,GACH,IAAM,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,GAClB,EAAK,EAAM,EAAE,MAAM,GAEnB,EAAa,EAAQ,GAErB,EAAI,EACR,EACA,EACA,EAAM,KAAK,MAAM,EAAG,EAAM,KAAK,OAAS,GACxC,EAAM,KAAK,MAAM,EAAM,KAAK,OAAS,GACrC,EAAM,UACN,EAAM,SAEJ,EAAc,CAAC,EAAG,GAGtB,OAFA,EAAc,EAAY,OAAO,GAE1B,KAAK,QACV,EACA,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,EAAG,EAAG,EAAM,GAClC,CACE,KACA,aACA,IACA,SAAU,EAAE,OACZ,KAAM,KAAK,QAAQ,EAAM,KAAqB,EAAf,KAAK,SACpC,QAAS,KAAK,QAAQ,EAAM,SAC5B,UAAW,KAAK,QAAQ,EAAM,WAC9B,WAAY,KAAK,kBAAkB,EAAM,gBAzEjD,yCA8EqB,GACjB,IAAM,EAAI,0EAA4B,GAEtC,sCACK,GAAI,CAEP,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,WAtF9B,yCA0FqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,EAAE,WA3FzD,GAEU,ICxTG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,oBAAc,EAAd,SAZJ,GAEU,ICkBY,GAAtB,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,oBAAI,MAAJ,SAgBoB,GACV,MAAN,kCACoB,KAAK,QADzB,oCAEW,KAAK,MAAM,YAAa,aAFnC,2BAKE,KAAK,iBALP,YAjBJ,CAAI,IAAJ,iBAAI,MAAJ,SA0BiB,GACb,OAAO,EAAM,cA3BjB,CAAI,IAAJ,kBAAI,MAAJ,WA+BI,MAAO,CAAC,IAAK,OA/BjB,CAAI,IAAJ,OAAI,MAAJ,SAkCO,GACH,OAAO,KAAK,QAAQ,EAAM,YAAa,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,MAnCjE,CAAI,IAAJ,UAAI,MAAJ,SAsCU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,aAET,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KA9ClB,CAAI,IAAJ,qBAAI,MAAJ,SAiDqB,GACjB,IAAM,EAAa,GAAiB,wBAClC,EAAQ,EAAM,aACd,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OACxB,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OACxB,YAAa,EAAM,YACnB,YAAa,EAAW,MACxB,aAAc,EAAW,UAhE/B,CAAI,IAAJ,qBAAI,MAAJ,SAoEqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,EAAE,WArEvC,GAIU,ICbG,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,QAAI,MAAJ,SAaQ,EAAW,GACf,sBAAgB,EAAhB,mBAA4B,KAdhC,CAAI,IAAJ,OAAI,MAAJ,SAiBO,GACH,OAAO,KAAK,QACV,EAAM,YACN,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,GACtB,CAAC,MAAO,EAAM,MAAO,KAAM,EAAM,SArBvC,CAAI,IAAJ,eAAI,MAAJ,WA0BI,sBACE,KAAK,eAAe,SADtB,8BAEE,KAAK,eAAe,QAFtB,wBA1BJ,CAAI,IAAJ,kBAAI,MAAJ,WAiCI,MAAO,CACL,CAAC,KAAM,QAAS,KAAM,SACtB,CAAC,KAAM,OAAQ,KAAM,YAnC3B,CAAI,IAAJ,qBAAI,MAAJ,SAuCqB,GACjB,IAAM,EAAI,0EAA4B,GACtC,sCACK,GAAI,CACP,MAAO,EAAM,MACb,KAAM,EAAM,SA5ClB,CAAI,IAAJ,qBAAI,MAAJ,SAgDqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,MAAnD,YAA4D,EAAM,UAjDtE,GAAgE,ICFnD,GAAb,YAAE,qBAAF,iBAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARlC,uBAAE,IAAJ,QAAI,MAAJ,SAWQ,EAAW,GACf,sBAAgB,EAAhB,cAAuB,KAZ3B,CAAI,IAAJ,OAAI,MAAJ,SAeO,GACH,OAAO,KAAK,QACV,EAAM,YACN,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,GACtB,CAAC,MAAO,EAAM,UAnBpB,CAAI,IAAJ,eAAI,MAAJ,WAwBI,sBACE,KAAK,eAAe,SADtB,yBAxBJ,CAAI,IAAJ,kBAAI,MAAJ,WA8BI,MAAO,CAAC,CAAC,KAAM,QAAS,KAAM,YA9BlC,CAAI,IAAJ,qBAAI,MAAJ,SAiCqB,GACjB,IAAM,EAAI,0EAA4B,GACtC,sCACK,GAAI,CACP,MAAO,EAAM,UArCnB,CAAI,IAAJ,qBAAI,MAAJ,SAyCqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,WA1CvD,GAEU,ICAG,GAAb,YAAE,qBAAF,iBAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARlC,uBAAE,IAAJ,QAAI,MAAJ,SAWQ,EAAW,GACf,sBAAgB,EAAhB,mBAA4B,KAZhC,CAAI,IAAJ,OAAI,MAAJ,SAeO,GACH,OAAO,KAAK,QACV,EAAM,YACN,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,GACtB,CAAC,MAAO,EAAM,MAAO,KAAM,EAAM,SAnBvC,CAAI,IAAJ,eAAI,MAAJ,WAwBI,sBACE,KAAK,eAAe,SADtB,8BAEE,KAAK,eAAe,QAFtB,wBAxBJ,CAAI,IAAJ,kBAAI,MAAJ,WA+BI,MAAO,CACL,CAAC,KAAM,QAAS,KAAM,SACtB,CAAC,KAAM,OAAQ,KAAM,YAjC3B,CAAI,IAAJ,qBAAI,MAAJ,SAqCqB,GACjB,IAAM,EAAI,0EAA4B,GACtC,sCACK,GAAI,CACP,MAAO,EAAM,MACb,KAAM,EAAM,SA1ClB,CAAI,IAAJ,qBAAI,MAAJ,SA8CqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,MAAnD,YAA4D,EAAM,UA/CtE,GAEU,ICJG,GAAb,YAAE,qBAAF,iBAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARlC,uBAAE,IAAJ,QAAI,MAAJ,SAWQ,EAAW,GACf,sBAAgB,EAAhB,cAAuB,KAZ3B,CAAI,IAAJ,OAAI,MAAJ,SAeO,GACH,OAAO,KAAK,QACV,EAAM,YACN,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,GACtB,CAAC,MAAO,EAAM,UAnBpB,CAAI,IAAJ,eAAI,MAAJ,WAwBI,sBACE,KAAK,eAAe,SADtB,yBAxBJ,CAAI,IAAJ,kBAAI,MAAJ,WA8BI,MAAO,CAAC,CAAC,KAAM,QAAS,KAAM,YA9BlC,CAAI,IAAJ,qBAAI,MAAJ,SAiCqB,GACjB,IAAM,EAAI,0EAA4B,GACtC,sCACK,GAAI,CACP,MAAO,EAAM,UArCnB,CAAI,IAAJ,qBAAI,MAAJ,SAyCqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,WA1CvD,GAEU,ICoBG,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,kCAE1B,EAAJ,YAAM,EAAmB,EAAO,IAPxB,cAAgB,IAKM,EARhC,uBAAE,IAAJ,gBAAI,MAAJ,WAcI,0CACsB,KAAK,QAAU,EADrC,qTAdJ,CAAI,IAAJ,eAAI,MAAJ,WAgCI,sBACE,KAAK,eAAe,cADtB,iCAEE,KAAK,eAAe,YAFtB,+BAGE,KAAK,eAAe,cAHtB,iCAIE,KAAK,eAAe,QAJtB,qBAI0C,KAAK,QAJ/C,mBAKE,KAAK,eAAe,WALtB,wBAKgD,KAAK,QALrD,mBAME,KAAK,eAAe,eANtB,4BAMwD,KAAK,QAN7D,cAhCJ,CAAI,IAAJ,oBAAI,MAAJ,SA2CoB,GAChB,8CAC0B,KAAK,QAD/B,wIAQiB,KAAK,QARtB,qBASI,KAAK,UAAU,YATnB,uCAUwB,KAAK,QAV7B,qIAgBgB,KAAK,QAhBrB,qBAiBI,KAAK,UAAU,WAjBnB,2FAqB4B,KAAK,cArBjC,8HA4BM,KAAK,gBA5BX,sKAsCM,KAAK,eAAe,WAAY,eAtCtC,2FA8CE,KAAK,iBA9CP,YA5CJ,CAAI,IAAJ,kBAAI,MAAJ,WA+FI,MAAO,CAAC,OA/FZ,CAAI,IAAJ,kBAAI,MAAJ,WAmGU,MAAC,CACL,CAAC,KAAM,cACP,CAAC,KAAM,YACP,CAAC,KAAM,cACP,CAAC,KAAM,OAAQ,OAAuB,EAAf,KAAK,SAC5B,CAAC,KAAM,UAAW,OAAQ,KAAK,SAC/B,CAAC,KAAM,cAAe,OAAQ,KAAK,YAzGzC,CAAI,IAAJ,OAAI,MAAJ,SA6GO,GACG,GAAF,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAE5C,MAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GAExB,EAAa,EAAQ,EAAM,aAE3B,EAAI,EACR,EACA,EAAM,YACN,EAAM,KAAK,MAAM,EAAG,EAAM,KAAK,OAAS,GACxC,EAAM,KAAK,MAAM,EAAM,KAAK,OAAS,GACrC,IAAI,MAAM,EAAE,QAAQ,KAAK,GACzB,EAAM,SAEJ,EAAc,CAAC,EAAG,GAGtB,OAFA,EAAc,EAAY,OAAO,GAE1B,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CACE,aACA,WAAY,EAAM,WAAa,EAAI,EACnC,SAAU,EAAE,OACZ,KAAM,KAAK,QAAQ,EAAM,KAAqB,EAAf,KAAK,SACpC,QAAS,KAAK,QAAQ,EAAM,SAC5B,YAAa,KAAK,QAAQ,EAAM,iBA3IxC,CAAI,IAAJ,iBAAI,MAAJ,SAgJiB,GACb,IAAM,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GAExB,EAAI,EACR,EACA,EAAM,YACN,EAAM,KAAK,MAAM,EAAG,EAAM,KAAK,OAAS,GACxC,EAAM,KAAK,MAAM,EAAM,KAAK,OAAS,GACrC,IAAI,MAAM,EAAE,QAAQ,KAAK,GACzB,EAAM,SAEJ,EAAc,CAAC,EAAG,GAGtB,OAFA,EAAc,EAAY,OAAO,KA9JrC,CAAI,IAAJ,UAAI,MAAJ,SAmKU,QACc,IAAhB,EAAK,SACP,EAAK,SAAW,EAAK,OAAO,OAAS,EAErC,KAAK,QAAU,EAAK,OAAO,SAEL,IAApB,EAAK,WACP,EAAK,WAAa,GACW,IAApB,EAAK,aACd,EAAK,WAAa,GAGpB,+DAAc,KA/KlB,CAAI,IAAJ,qBAAI,MAAJ,SAkLqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGD,EAAa,EAAQ,EAAM,aAEjC,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EAAM,YAEnB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,KAAM,EAAM,KACZ,QAAS,EAAM,QAEf,WAAY,EACZ,SAAU,EAAM,EAAE,MAAM,OAAS,EACjC,WAAY,EAAM,WAAa,EAAI,KA3MzC,CAAI,IAAJ,qBAAI,MAAJ,SA+MqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,YAAjC,YAAgD,EAAM,KAAtD,YAA8D,EAAM,QAApE,YAA+E,EAAM,gBAhNzF,GAEU,ICRY,GAAtB,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,kCAE1B,EAAJ,YAAM,EAAmB,EAAO,IAPxB,cAAgB,IAKM,EARhC,uBAAE,IAAJ,eAAI,MAAJ,WAcI,sBACE,KAAK,eAAe,sBADtB,mCAEE,KAAK,QAFP,mBAIE,KAAK,eAAe,sBAJtB,wBAI2D,KAAK,QAJhE,mBAKE,KAAK,eAAe,sBALtB,yBAdJ,CAAI,IAAJ,oBAAI,MAAJ,SAwBoB,GAChB,8CAC0B,KAAK,QAD/B,mCAEgB,KAAK,QAFrB,qBAGI,KAAK,UAAU,WAHnB,kEAMwB,KAAK,QAN7B,sLAaI,KAAK,WAAW,WAAY,UAAW,YAb3C,mEAiBwB,KAAK,cAjB7B,yJAuBc,KAAK,KAAK,QAvBxB,gDAyBc,KAAK,OAAO,OAAQ,OAzBlC,mCA4BM,KAAK,qBAAqB,UAAW,SAAU,WA5BrD,8BA+BI,KAAK,KAAK,OA/Bd,+CAoCE,KAAK,iBApCP,YAzBJ,CAAI,IAAJ,OAAI,MAAJ,SAoEO,GACH,MAAO,KArEX,CAAI,IAAJ,OAAI,MAAJ,SAuEO,GACG,OAAC,IAxEX,CAAI,IAAJ,kBAAI,MAAJ,WA4EU,MAAC,CAAC,OA5EZ,CAAI,IAAJ,kBAAI,MAAJ,WAgFU,MAAC,CACL,CAAS,KAAF,qBAAsB,OAAQ,KAAK,SAC1C,CAAS,KAAF,UAAW,OAAQ,KAAK,SAC/B,CAAC,KAAM,cAnFb,CAAI,IAAJ,OAAI,MAAJ,SAuFO,GACG,GAAF,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAF/B,QAKU,EAC3B,EAAM,EAAE,MACR,EAAM,KACN,EAAM,UARW,mBAKZ,EALY,KAKC,EALD,KAWb,EAAe,EAAe,EAAM,EAAE,OACtC,EAAqB,GAZR,cAaH,GAbG,IAanB,2BAAuB,KAAZ,EAAY,QACrB,EAAmB,KAAK,EAAa,KAdpB,WAAAC,EAAA,iBAmBnB,IAFM,IAAF,EAAU,EACR,EAAoB,IAAI,MAAM,EAAM,EAAE,MAAM,QAAQ,KAAK,GACtD,EAAI,EAAG,EAAI,EAAM,KAAK,OAAQ,IACrC,EAAQ,EAAM,KAAK,IAAM,EACzB,GAAW,EAAM,EAAE,MAAM,EAAM,KAAK,IAGhC,OAAC,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CACE,mBAAoB,KAAK,IAAI,GAC7B,QAAS,KAAK,IAAI,GAClB,cArHR,CAAI,IAAJ,iBAAI,MAAJ,SA0HiB,GAAgB,MAEA,EAC3B,EAAM,EAAE,MACR,EAAM,KACN,EAAM,UALqB,mBAEtB,EAFsB,UAO7B,OAAO,IAjIX,CAAI,IAAJ,UAAI,MAAJ,SAoIU,GACN,QACkB,IAAhB,EAAK,aACS,IAAd,EAAK,WACa,IAAlB,EAAK,SACL,SAC6B,EAC3B,EAAK,OACL,EAAK,KACL,EAAK,UAJP,mBACO,EADP,KACoB,EADpB,KAOM,EAAe,EAAe,EAAK,QACnC,EAAqB,GAR3B,cASgB,GAThB,IASA,2BAAuB,KAAZ,EAAY,QACrB,EAAmB,KAAK,EAAa,KAVvC,8BAeA,IAFA,IAAI,EAAU,EACR,EAAoB,IAAI,MAAM,EAAK,OAAO,QAAQ,KAAK,GACpD,EAAI,EAAG,EAAI,EAAK,KAAK,OAAQ,IACpC,EAAQ,EAAK,KAAK,IAAM,EACxB,GAAW,EAAK,OAAO,EAAK,KAAK,IAGnC,EAAK,QAAU,EACf,EAAK,YAAc,EACnB,EAAK,mBAAqB,EAC1B,EAAK,QAAU,SAER,EAAI,gBACJ,EAAI,KAEX,KAAK,QAAU,EAAK,OAAO,OAG7B,+DAAc,KAxKlB,CAAI,IAAJ,qBAAI,MAAJ,SA2KqB,GAAgB,QACJ,EAC3B,EAAM,EAAE,MACR,EAAM,KACN,EAAM,UAJyB,mBAC1B,EAD0B,KACb,EADa,KAO3B,EAAe,EAAe,EAAM,EAAE,OACtC,EAAqB,GARM,cASjB,GATiB,IASjC,2BAAuB,KAAZ,EAAY,QACrB,EAAmB,KAAK,EAAa,KAVN,8BAejC,IAFA,IAAI,EAAU,EACR,EAAoB,IAAI,MAAM,EAAM,EAAE,MAAM,QAAQ,KAAK,GACtD,EAAI,EAAG,EAAI,EAAM,KAAK,OAAQ,IACrC,EAAQ,EAAM,KAAK,IAAM,EACzB,GAAW,EAAM,EAAE,MAAM,EAAM,KAAK,IAGtC,IAAM,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,qBACA,UACA,aA/MN,CAAI,IAAJ,qBAAI,MAAJ,SAmNqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,KAAjC,YAAyC,EAAM,cApNnD,GAEU,IChCG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,gBAAU,EAAV,cAAiB,KAJrB,CAAI,IAAJ,OAAI,MAAJ,SAMO,GACH,gBAAU,EAAV,cAAmB,EAAnB,wBAPJ,GAEU,ICFG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,iBAAW,EAAX,YAAgB,EAAhB,eAAwB,KAJ5B,CAAI,IAAJ,OAAI,MAAJ,SAMO,GACH,gBAAU,EAAV,cAAmB,EAAnB,sBAPJ,CAAI,IAAJ,OAAI,MAAJ,SASO,GACH,gBAAU,EAAV,YAAiB,OAVrB,GAEU,ICFG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,iBAAW,EAAX,YAAgB,EAAhB,eAAwB,KAJ5B,CAAI,IAAJ,OAAI,MAAJ,SAMO,GACH,gBAAU,EAAV,YAAiB,OAPrB,GAEU,ICFG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,gBAAU,EAAV,cAAiB,OAJrB,GAEU,ICFG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,gBAAU,EAAV,cAAiB,OAJrB,GAEU,ICFG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,oBAAc,EAAd,aAAoB,EAApB,SAJJ,GAEU,ICFG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,oBAAc,EAAd,aAAoB,EAApB,SAJJ,GAEU,ICGG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,qBAAe,EAAf,SAZJ,GAEU,ICmBG,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,oBAAI,MAAJ,SAcoB,GAChB,yXAfJ,CAAI,IAAJ,kBAAI,MAAJ,WAoCI,MAAO,CAAC,OApCZ,CAAI,IAAJ,eAAI,MAAJ,WAwCU,MAAN,gBACE,KAAK,eAAe,UADtB,+BAEE,KAAK,eAAe,UAFtB,+BAGE,KAAK,eAAe,SAHtB,4BAIE,KAAK,eAAe,SAJtB,uBAxCJ,CAAI,IAAJ,kBAAI,MAAJ,WAiDU,MAAC,CACL,CAAC,KAAM,SAAU,KAAM,SACvB,CAAC,KAAM,SAAU,KAAM,SACvB,CAAC,KAAM,SACP,CAAC,KAAM,YArDb,CAAI,IAAJ,OAAI,MAAJ,SAyDO,GACH,OAAI,KAAK,kBAAoC,IAArB,KAAK,YACpB,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,QAG3C,KAAK,QACV,EAAM,MAAM,MACZ,CAAC,EAAG,EAAM,OACV,CACE,YAAyB,IAAjB,EAAM,OAAuB,EAAM,OAAS,EACpD,YAAyB,IAAjB,EAAM,OAAuB,EAAM,OAAS,EACpD,WAAwB,IAAjB,EAAM,OAAuB,EAAI,EACxC,WAAwB,IAAjB,EAAM,OAAuB,EAAI,MArEhD,CAAI,IAAJ,iBAAI,MAAJ,SA0EiB,GACb,OAAO,EAAM,MAAM,QA3EvB,CAAI,IAAJ,UAAI,MAAJ,SA8EU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KAnFlB,CAAI,IAAJ,qBAAI,MAAJ,SAsFqB,GACjB,IAAM,EAAa,GAAiB,wBAClC,EAAM,MAAM,KACZ,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,MAAM,MACpB,OAAQ,EAAM,MAAM,OAAO,MAC3B,QAAS,EAAM,MAAM,OAAO,OAC5B,YAAa,EAAM,MAAM,MACzB,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,YAAyB,IAAjB,EAAM,OAAuB,EAAM,OAAS,EACpD,YAAyB,IAAjB,EAAM,OAAuB,EAAM,OAAS,EACpD,WAAwB,IAAjB,EAAM,OAAuB,EAAI,EACxC,WAAwB,IAAjB,EAAM,OAAuB,EAAI,KAvG9C,CAAI,IAAJ,qBAAI,MAAJ,SA2GqB,GACjB,gBAAU,EAAM,MAAM,MAAtB,YAA+B,EAAM,OAArC,YAA+C,EAAM,YA5GzD,GAAiE,ICrBpD,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,sBAAgB,EAAhB,SAZJ,GAEU,ICsBG,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,eAAI,MAAJ,WAcI,sBACE,KAAK,eAAe,QADtB,sBAdJ,CAAI,IAAJ,kBAAI,MAAJ,WAoBI,MAAO,CAAC,CAAC,KAAM,WApBnB,CAAI,IAAJ,oBAAI,MAAJ,SAwBoB,GACV,MAAN,wCAC0B,KAAK,QAD/B,mEAGwB,KAAK,QAH7B,wSAiBE,KAAK,iBAjBP,YAzBJ,CAAI,IAAJ,kBAAI,MAAJ,WA+CI,MAAO,CAAC,IAAK,OA/CjB,CAAI,IAAJ,OAAI,MAAJ,SAkDO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,IAG9D,IAAM,EAAc,KAAK,eAAe,GAExC,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,GACtB,CAAC,KAAM,EAAM,SA5DnB,CAAI,IAAJ,iBAAI,MAAJ,SAgEiB,GACb,IAAM,EAAW,YAAO,EAAM,EAAE,OAEhC,OADA,EAAY,EAAM,OAAS,EAAM,EAAE,MAAM,EAAM,MACxC,IAnEX,CAAI,IAAJ,UAAI,MAAJ,SAsEU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,aAET,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KA9ElB,CAAI,IAAJ,qBAAI,MAAJ,SAiFqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,KAAM,EAAM,QArGlB,CAAI,IAAJ,qBAAI,MAAJ,SAyGqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,EAAE,MAAnC,YAA4C,EAAM,UA1GtD,GAAmE,ICRtD,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,oBAAI,MAAJ,SAcoB,GAChB,wGAfJ,CAAI,IAAJ,kBAAI,MAAJ,WAyBI,MAAO,CAAC,OAzBZ,CAAI,IAAJ,OAAI,MAAJ,SA4BO,GACH,IAAM,EAAQ,KAAK,eAAe,GAElC,OAAO,KAAK,QAAQ,EAAO,CAAC,EAAG,EAAM,UA/BzC,CAAI,IAAJ,iBAAI,MAAJ,SAkCiB,GACb,IAAI,EAAQ,EAAM,YAIlB,YAHc,IAAV,IACF,EAAQ,EAAM,MAAM,OAEf,IAvCX,CAAI,IAAJ,UAAI,MAAJ,SA0CU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KA/ClB,CAAI,IAAJ,qBAAI,MAAJ,SAkDqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,MAAM,MACpB,OAAQ,EAAM,MAAM,OAAO,MAC3B,QAAS,EAAM,MAAM,OAAO,OAE5B,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,UAhE/B,CAAI,IAAJ,qBAAI,MAAJ,SAoEqB,GACjB,gBAAU,EAAM,MAAhB,YAAyB,KAAK,eAAe,QArEjD,GAAiE,ICApD,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,oBAAI,MAAJ,SAcoB,GACV,MAAN,wCAC0B,KAAK,QAD/B,uDAKE,KAAK,iBALP,YAfJ,CAAI,IAAJ,kBAAI,MAAJ,WAyBI,MAAO,CAAC,OAzBZ,CAAI,IAAJ,OAAI,MAAJ,SA4BO,GACH,OAAO,KAAK,QAAQ,EAAM,YAAa,CAAC,EAAG,EAAM,UA7BrD,CAAI,IAAJ,iBAAI,MAAJ,SAgCiB,GACb,OAAO,EAAM,cAjCjB,CAAI,IAAJ,UAAI,MAAJ,SAoCU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KAzClB,CAAI,IAAJ,qBAAI,MAAJ,SA4CqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,MAAM,MACpB,OAAQ,EAAM,MAAM,OAAO,MAC3B,QAAS,EAAM,MAAM,OAAO,OAE5B,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,UA1D/B,CAAI,IAAJ,qBAAI,MAAJ,SA8DqB,GACjB,gBAAU,EAAM,MAAM,MAAtB,YAA+B,EAAM,iBA/DzC,GAAmE,ICUtD,GAAb,YAAE,qBAAF,iBAOI,SAAF,EACE,EACA,EACA,GAA8B,kCAE1B,EAAJ,YAAM,EAAmB,EAAO,IAPxB,gBAAkB,GAKI,EAVhC,uBAAE,IAAJ,eAAI,MAAJ,WAgBI,sBACE,KAAK,eAAe,QADtB,2BAEE,KAAK,eAAe,eAFtB,4BAGED,KAAK,gBAHP,mBAKE,KAAK,eAAe,sBALtB,mCAME,KAAK,QANP,mBAQE,KAAK,eAAe,sBARtB,mCASE,KAAK,QATP,cAhBJ,CAAI,IAAJ,kBAAI,MAAJ,WA+BI,MAAO,CACL,CAAC,KAAM,QACP,CAAC,KAAM,cAAe,OAAQ,KAAK,iBACnC,CAAC,KAAM,qBAAsB,OAAQ,KAAK,SAC1C,CAAC,KAAM,qBAAsB,OAAQ,KAAK,YAnChD,CAAI,IAAJ,oBAAI,MAAJ,SAwCoB,GACV,MAAN,wCAC0B,KAAK,QAD/B,0HAMwB,KAAK,QAN7B,0SAiBwB,KAAK,gBAjB7B,qNA2BE,KAAK,iBA3BP,YAzCJ,CAAI,IAAJ,kBAAI,MAAJ,WAyEI,MAAO,CAAC,OAzEZ,CAAI,IAAJ,OAAI,MAAJ,SA4EO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAG5C,GAAF,EAAM,QAAQ,KAAO,KAAK,gBACpB,MAAF,IAAI,MAAJ,8CACmC,KAAK,gBADxC,+BAC8E,EAAM,QAAQ,OAgB9F,IAZA,MAAI,EAAM,EAAE,MAAM,OAClB,EAAI,EAAM,QAAQ,MAAM,OAExB,EAAe,EAAe,EAAM,EAAE,OACtC,EAAe,EAAe,EAAM,QAAQ,OAE5C,EAAa,EAAI,EAAI,EACrB,EAAc,IAAI,MAAM,GAExB,EAAqB,IAAI,MAAM,GAAY,KAAK,GAChD,EAAqB,IAAI,MAAM,GAAY,KAAK,GAE7C,EAAI,EAAG,EAAI,EAAM,KAAM,IACtB,EAAI,GAAK,EAAM,EAAE,MAAM,GACvB,EAAW,GAAK,EAAa,GAE7B,EAAW,GAAK,EAEpB,IAAD,IAAI,EAAI,EAAG,EAAI,EAAG,IACb,EAAI,EAAI,EAAM,MAAQ,EAAM,QAAQ,MAAM,GAC1C,EAAW,EAAI,EAAM,MAAQ,EAAa,GAElD,EAAmB,EAAI,EAAM,MAAQ,EAEvC,IAAK,IAAI,EAAI,EAAM,KAAO,EAAG,EAAI,EAAG,IAClC,EAAY,EAAI,EAAI,GAAK,EAAM,EAAE,MAAM,GACvC,EAAmB,EAAI,EAAI,GAAK,EAAa,GAE7C,EAAmB,EAAI,EAAI,GAAK,EAGlC,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CACE,KAAM,EAAM,KACZ,YAAa,KAAK,IAChB,MAAM,KAAK,EAAM,QAAQ,QACzB,KAAK,iBAEP,mBAAoB,KAAK,IAAI,GAC7B,mBAAoB,KAAK,IAAI,OAhIrC,CAAI,IAAJ,iBAAI,MAAJ,SAqIiB,GAOP,IANN,IAAME,EAAI,EAAM,EAAE,MAAM,OAClB,EAAI,EAAM,QAAQ,MAAM,OAGxB,EAAc,IAAI,MADL,EAAI,EAAI,GAGlB,EAAI,EAAGV,EAAI,EAAM,KAAM,IAC9B,EAAY,GAAK,EAAM,EAAE,MAAM,GAE3B,IAAD,IAAI,EAAI,EAAG,EAAI,EAAG,IACrB,EAAY,EAAI,EAAM,MAAQ,EAAM,QAAQ,MAAM,GAE9C,IAAD,IAAI,EAAI,EAAM,KAAO,EAAG,EAAI,EAAG,IAClC,EAAY,EAAI,EAAI,GAAK,EAAM,EAAE,MAAM,GAGzC,OAAO,IAtJX,CAAI,IAAJ,UAAI,MAAJ,SAyJU,GACN,QAAoB,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,YAEN,IAAjB,EAAK,cAAuC,IAAd,EAAK,MAAoB,CAazD,IAZA,IAAM,EAAI,EAAK,OAAO,OAChB,EAAI,EAAK,QAAQ,MAAM,OAEvB,EAAe,EAAe,EAAK,QACnC,EAAe,EAAe,EAAK,QAAQ,OAE3C,EAAa,EAAI,EAAI,EACrB,EAAc,IAAI,MAAM,GAExB,EAAqB,IAAI,MAAM,GAAY,KAAK,GAChD,EAAqB,IAAI,MAAM,GAAY,KAAK,GAE7C,EAAI,EAAG,EAAI,EAAK,KAAM,IAC7B,EAAY,GAAK,EAAK,OAAO,GAC7B,EAAmB,GAAK,EAAa,GAErC,EAAmB,GAAK,EAE1B,IAAK,IAAI,EAAI,EAAG,EAAI,EAAG,IACrB,EAAY,EAAI,EAAK,MAAQ,EAAK,QAAQ,MAAM,GAChD,EAAmB,EAAI,EAAK,MAAQ,EAAa,GAEjD,EAAmB,EAAI,EAAK,MAAQ,EAEtC,IAAK,IAAI,EAAI,EAAK,KAAO,EAAG,EAAI,EAAG,IACjC,EAAY,EAAI,EAAI,GAAK,EAAK,OAAO,GACrC,EAAmB,EAAI,EAAI,GAAK,EAAa,GAE7C,EAAmB,EAAI,EAAI,GAAK,EAGlC,EAAK,mBAAqB,EAC1B,EAAK,mBAAqB,EAC1B,EAAK,YAAc,MAAM,KAAK,EAAK,QAAQ,eAEpC,EAAI,QAIf,+DAAc,KArMlB,CAAI,IAAJ,qBAAI,MAAJ,SAwMqB,GAmBjB,IAlBA,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGD,EAAI,EAAM,EAAE,MAAM,OAClB,EAAI,EAAM,QAAQ,MAAM,OAExB,EAAe,EAAe,EAAM,EAAE,OACtC,EAAe,EAAe,EAAM,QAAQ,OAE5C,EAAa,EAAI,EAAI,EACrB,EAAc,IAAI,MAAM,GAExB,EAAqB,IAAI,MAAM,GAAY,KAAK,GAChD,EAAqB,IAAI,MAAM,GAAY,KAAK,GAE7C,EAAI,EAAG,EAAI,EAAM,KAAM,IAC9B,EAAY,GAAK,EAAM,EAAE,MAAM,GAC/B,EAAmB,GAAK,EAAa,GAErC,EAAmB,GAAK,EAE1B,IAAK,IAAI,EAAI,EAAG,EAAI,EAAG,IACrB,EAAY,EAAI,EAAM,MAAQ,EAAM,QAAQ,MAAM,GAClD,EAAmB,EAAI,EAAM,MAAQ,EAAa,GAElD,EAAmB,EAAI,EAAM,MAAQ,EAEvC,IAAK,IAAI,EAAI,EAAM,KAAO,EAAG,EAAI,EAAG,IAClC,EAAY,EAAI,EAAI,GAAK,EAAM,EAAE,MAAM,GACvC,EAAmB,EAAI,EAAI,GAAK,EAAa,GAE7C,EAAmB,EAAI,EAAI,GAAK,EAGlC,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,KAAM,EAAM,KACZ,YAAa,MAAM,KAAK,EAAM,QAAQ,QACtC,qBACA,wBA1PN,CAAI,IAAJ,qBAAI,MAAJ,SA8PqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,KAAjC,YAAyC,MAAM,KAC7C,EAAM,QAAQ,QADhB,YAEK,EAAM,QAAQ,WAjQvB,GAAmE,ICStD,GAAb,YAAE,qBAAF,iBAOI,SAAF,EACE,EACA,EACA,GAA8B,kCAE1B,EAAJ,YAAM,EAAmB,EAAO,IAPxB,cAAgB,IAKM,EAVhC,uBAAE,IAAJ,cAAI,MAAJ,WAgBU,MAAN,0BACY,KAAK,QADjB,qBAEI,KAAK,UAAU,OAFnB,2BAGY,KAAK,QAHjB,qBAII,KAAK,UAAU,OAJnB,uCAKwB,KAAK,QAL7B,iNAewB,KAAK,QAf7B,qbAsCwB,KAAK,cAtC7B,qGA0C0B,KAAK,QA1C/B,0aAhBJ,CAAI,IAAJ,eAAI,MAAJ,WAmFU,MAAN,gBACE,KAAK,eAAe,KADtB,wBAEE,KAAK,eAAe,KAFtB,wBAGE,KAAK,eAAe,KAHtB,wBAIE,KAAK,eAAe,QAJtB,2BAKE,KAAK,eAAe,cALtB,iCAME,KAAK,eAAe,cANtB,iCAOE,KAAK,eAAe,SAPtB,8BAQE,KAAK,eAAe,QARtB,wBAnFJ,CAAI,IAAJ,oBAAI,MAAJ,SAgGoB,GAChB,8CAC0B,KAAK,QAD/B,uBAEI,KAAK,cAFT,+CAOE,KAAK,iBAPP,YAjGJ,CAAI,IAAJ,kBAAI,MAAJ,WA6GI,MAAO,CAAC,IAAK,OA7GjB,CAAI,IAAJ,kBAAI,MAAJ,WAiHU,MAAC,CACL,CAAC,KAAM,KACP,CAAC,KAAM,KACP,CAAS,KAAF,KACP,CAAS,KAAF,QACP,CAAS,KAAF,cACP,CAAS,KAAF,cACP,CAAS,KAAF,QAAS,KAAM,SACtB,CAAS,KAAF,OAAQ,KAAM,YAzH3B,CAAI,IAAJ,OAAI,MAAJ,SA6HO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,IAGxD,MAAO,EAAM,EAAE,MAAM,OAErB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GACnB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GACnB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GAEnB,EAAa,EAAM,EAAE,MAAM,MAAM,EAAG,EAAO,GAC3C,EAAW,sBAAO,GAAP,CAAmB,EAAG,IAEjC,EAAW,CACf,IACA,IACA,IACA,OACA,WAAY,EAAM,WAAa,EAAI,EACnC,WAAY,EAAM,WAAa,EAAI,EACnC,MAAO,EAAM,MACb,KAAM,EAAM,MAGR,OAAC,KAAK,QAAQ,EAAa,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,GAAI,KA5J/D,CAAI,IAAJ,iBAAI,MAAJ,SA+JiB,GACP,MAAO,EAAM,EAAE,MAAM,OAErB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GACnB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GAEnB,EAAaW,EAAM,EAAE,MAAM,MAAM,EAAG,EAAO,GAGjD,MAFiB,sBAAO,GAAP,CAAmB,EAAG,MA1K3C,CAAI,IAAJ,UAAI,MAAJ,SA+KU,GACN,QAAoB,IAAhB,EAAK,OAAsB,CAC7B,IAAM,EAAO,EAAK,OAAO,OAIzB,GAHA,EAAK,KAAO,EACZ,KAAK,QAAU,OAES,IAApB,EAAK,WAA0B,CACjC,IAAM,EAAI,EAAK,WACX,EAAK,OAAO,EAAO,GACnB,EAAK,OAAO,EAAO,GACjB,EAAI,EAAK,WACX,EAAK,OAAO,EAAO,GACnB,EAAK,OAAO,EAAO,GAEvB,EAAK,EAAI,EACT,EAAK,EAAI,EAET,EAAK,WAAa,EAAK,WAAa,EAAI,GAItC,QAAc,IAAhB,EAAK,aAA4C,IAApB,EAAK,WAA0B,CACtD,IAAF,EAAO,EAAK,OAAO,OACnB,EAAI,EAAK,WAAa,EAAK,OAAO,EAAO,GAAK,EAAK,OAAO,EAAO,GAEvE,EAAK,EAAI,EACT,EAAK,WAAa,EAAK,WAAa,EAAI,EAG1C,+DAAc,KA5MlB,CAAI,IAAJ,qBAAI,MAAJ,SA+MqB,GACX,MAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGD,EAAO,EAAM,EAAE,MAAM,OAErB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GACnB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GACnB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GA2BzB,MAzBuB,CACrB,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,IACA,IACA,IAEA,WAAY,EAAM,WAAa,EAAI,EACnC,WAAY,EAAM,WAAa,EAAI,EACnC,MAAO,EAAM,MACb,KAAM,EAAM,KAEZ,UAxPN,CAAI,IAAJ,qBAAI,MAAJ,SA8PqB,GAEjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,EAAE,MAAnC,YAA4C,EAAM,WAAlD,YAAgE,EAAM,WAAtE,YAAoF,EAAM,MAA1F,YAAmG,EAAM,UAhQ7G,GAIU,IA0QG,GAAb,gLAMI,MAAO,CAAC,IAAK,IAAK,OANtB,wCAUoB,GAChB,8CAC0B,KAAK,QAD/B,uBAEI,KAAK,cAFT,+EASE,KAAK,iBATP,YAXJ,2BAwBO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CACpC,EAAG,EAAM,EACT,EAAG,EAAM,EACT,EAAG,EAAM,IAIb,IAAM,EAAO,EAAM,EAAE,MAAM,OAErB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GACnB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GACnB,EAAI,EAAM,WACZ,EAAM,EAAE,MAAM,EAAO,GACrB,EAAM,EAAE,MAAM,EAAO,GAEnB,EAAa,EAAM,EAAE,MAAM,MAAM,EAAG,EAAO,GAC3C,EAAW,sBAAO,GAAP,CAAmB,EAAG,IAEjC,EAAW,CACf,IACA,IACA,IACA,OACA,WAAY,EAAM,WAAa,EAAI,EACnC,WAAY,EAAM,WAAa,EAAI,EACnC,MAAO,EAAM,MACb,KAAM,EAAM,MAGd,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,EAAG,EAAG,EAAM,GAClC,KA9DN,yCAkEqB,GACjB,IAAM,EAAG,0EAA4B,GAUrC,OARU,+BACL,GAAG,CAEN,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,WA1E9B,yCAgFqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,EAAE,WAjFzD,GAAkE,ICjTrD,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,kDAWQ,EAAW,GACf,oBAAc,EAAd,aAAoB,EAApB,SAZJ,GAEU,ICFG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,qBAAe,EAAf,SAZJ,GAEU,ICFG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,oBAAc,EAAd,SAZJ,GAEU,ICkBG,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,eAAI,MAAJ,WAcI,sBACE,KAAK,eAAe,iBADtB,8BAC4D,KAAK,QADjE,cAdJ,CAAI,IAAJ,kBAAI,MAAJ,WAoBI,MAAO,CAAC,CAAC,KAAM,gBAAiB,OAAQ,KAAK,YApBjD,CAAI,IAAJ,oBAAI,MAAJ,SAwBoB,GACV,MAAN,wCAC0B,KAAK,QAD/B,kGAKE,KAAK,iBALP,YAzBJ,CAAI,IAAJ,kBAAI,MAAJ,WAmCI,MAAO,CAAC,OAnCZ,CAAI,IAAJ,OAAI,MAAJ,SAsCO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAS5C,IANN,IAAM,EAAO,EAAM,EAAE,MAAM,OAErB,EAAc,KAAK,eAAe,GAElC,EAAe,EAAe,EAAM,EAAE,OACtC,EAAgB,IAAI,MAAM,GACvB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAc,GAAK,EAAa,EAAM,YAAY,IAG9C,OAAC,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CAAC,cAAe,KAAK,IAAI,OAxD/B,CAAI,IAAJ,iBAAI,MAAJ,SA4DiB,GAIb,IAHA,IAAM,EAAO,EAAM,EAAE,MAAM,OAErB,EAAc,IAAI,MAAM,GACrB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAY,GAAK,EAAM,EAAE,MAAM,EAAM,YAAY,IAGnD,OAAO,IApEX,CAAI,IAAJ,UAAI,MAAJ,SAuEU,GACN,QAAoB,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,YAEF,IAArB,EAAK,aAA2B,CAKlC,IAJA,IAAM,EAAO,EAAK,OAAO,OAEnB,EAAe,EAAe,EAAK,QACnC,EAAgB,IAAI,MAAM,GACvB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAc,GAAK,EAAa,EAAK,YAAY,IAGnD,EAAK,cAAgB,SAEd,EAAI,YAIf,+DAAc,KA1FlB,CAAI,IAAJ,qBAAI,MAAJ,SA6FqB,GAWjB,IAVA,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGD,EAAO,EAAM,EAAE,MAAM,OAErB,EAAe,EAAe,EAAM,EAAE,OACtC,EAAgB,IAAI,MAAM,GACvB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAc,GAAK,EAAa,EAAM,YAAY,IAGpD,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,mBArHN,CAAI,IAAJ,qBAAI,MAAJ,SAyHqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,iBA1HrC,GAAsE,ICDzD,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,eAAI,MAAJ,WAcI,sBACE,KAAK,eAAe,WADtB,wBACgD,KAAK,QADrD,cAdJ,CAAI,IAAJ,kBAAI,MAAJ,WAoBI,MAAO,CAAC,CAAC,KAAM,UAAW,OAAQ,KAAK,YApB3C,CAAI,IAAJ,oBAAI,MAAJ,SAwBoB,GAChB,8CAC0B,KAAK,QAD/B,mCAEgB,KAAK,QAFrB,qBAGI,KAAK,UAAU,WAHnB,uCAIwB,KAAK,QAJ7B,2NAeE,KAAK,iBAfP,YAzBJ,CAAI,IAAJ,iBAAI,MAAJ,SA4CiB,GAIb,IAHA,IAAM,EAAO,EAAM,EAAE,MAAM,OAErB,EAAc,IAAI,MAAM,GACrB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAY,GAAK,EAAM,EAAE,MAAM,GAAK,EAAM,QAAQ,GAGpD,OAAO,IApDX,CAAI,IAAJ,kBAAI,MAAJ,WAwDI,MAAO,CAAC,OAxDZ,CAAI,IAAJ,OAAI,MAAJ,SA2DO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAGlD,IAAM,EAAc,KAAK,eAAe,GAExC,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CAAC,QAAS,KAAK,QAAQ,EAAM,aArEnC,CAAI,IAAJ,UAAI,MAAJ,SAyEU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KA9ElB,CAAI,IAAJ,qBAAI,MAAJ,SAiFqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,QAAS,EAAM,WAjGrB,CAAI,IAAJ,qBAAI,MAAJ,SAqGqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,aAtGrC,GAAmE,ICItD,GAAb,kDAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVpC,8DAcoB,GAChB,8CAC0B,KAAK,QAD/B,mCAEgB,KAAK,QAFrB,qBAGI,KAAK,UAAU,WAHnB,6HAQ0B,KAAK,QAR/B,uaAyB0B,KAAK,QAzB/B,uZAuC0B,KAAK,QAvC/B,wWAuDE,KAAK,iBAvDP,YAfJ,wCA2EI,MAAO,CAAC,OA3EZ,qCA+EI,sBACE,KAAK,eAAe,QADtB,qBACyD,EAAf,KAAK,QAD/C,mBAEE,KAAK,eAAe,SAFtB,8BAGE,KAAK,eAAe,QAHtB,sBA/EJ,wCAuFI,MAAO,CACL,CAAC,KAAM,QAAS,KAAM,SACtB,CAAC,KAAM,OAAQ,OAAuB,EAAf,KAAK,SAC5B,CAAC,KAAM,WA1Fb,kCA8Fc,GACJ,MAAU,aAAT,EAAsB,EAAa,YAAT,EAAqB,EAAI,IA/F9D,2BAkGO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,QAElD,IAAM,EAAc,KAAK,eAAe,GAExC,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,OACV,CACE,KAAM,KAAK,QAAQ,EAAM,KAAqB,EAAf,KAAK,SACpC,MAAO,EAAM,MACb,KAAM,KAAK,YAAY,EAAM,UA9GrC,qCAmHiB,GAIb,IAHA,IAAM,EAAO,EAAM,MAAM,MAAM,OAEzB,EAAW,YAAO,EAAM,MAAM,OAC3B,EAAI,EAAG,EAAI,EAAM,IACxB,EAAY,IAAM,EAAM,KAAK,GAAK,EAAM,KAAK,EAAI,GAGnD,OAAO,IA3HX,8BA8HU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,aAGX,IAAd,EAAK,MAA2C,kBAAd,EAAK,OACzC,EAAK,KAAO,KAAK,YAAY,EAAK,OAGpC,+DAAc,KAvIlB,yCA0IqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,MAAM,MACpB,OAAQ,EAAM,MAAM,OAAO,MAC3B,QAAS,EAAM,MAAM,OAAO,OAE5B,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,KAAM,EAAM,KACZ,KAAM,KAAK,YAAY,EAAM,MAC7B,MAAO,EAAM,SA5JnB,yCAgKqB,GAEjB,gBAAU,EAAM,MAAM,MAAtB,YAA+B,EAAM,KAArC,YAA6C,EAAM,KAAnD,YAA2D,EAAM,WAlKrE,GAAgE,ICInD,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,oBAAI,MAAJ,SAcoB,GAChB,8CAC0B,KAAK,QAD/B,gCAEa,KAAK,QAFlB,qBAGI,KAAK,UAAU,QAHnB,uCAIwB,KAAK,QAJ7B,sLAeE,KAAK,iBAfP,YAfJ,CAAI,IAAJ,kBAAI,MAAJ,WAmCU,MAAC,CAAC,OAnCZ,CAAI,IAAJ,eAAI,MAAJ,WAuCI,sBACE,KAAK,eAAe,WADtB,wBACgD,KAAK,QADrD,mBAEE,KAAK,eAAe,SAFtB,sBAE4C,KAAK,QAFjD,cAvCJ,CAAI,IAAJ,kBAAI,MAAJ,WA8CU,MAAC,CACL,CAAS,KAAF,UAAW,OAAQ,KAAK,SAC/B,CAAS,KAAF,QAAS,OAAQ,KAAK,YAhDnC,CAAI,IAAJ,OAAI,MAAJ,SAoDO,GACG,GAAF,KAAK,kBAAoC,IAArB,KAAK,YACnB,OAAD,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAS5C,IANA,MAAO,EAAM,EAAE,MAAM,OAErB,EAAW,YAAO,EAAM,EAAE,OAC1B,EAAoB,IAAI,MAAM,GAAM,KAAK,GACzC,EAAkB,IAAI,MAAM,GAAM,KAAK,GACzC,EAAO,EACF,EAAI,EAAG,EAAI,GAAQ,EAAO,EAAM,KAAK,OAAQ,IAChD,IAAM,EAAM,KAAK,KACnB,EAAY,GAAK,KAAK,MACnB,EAAM,KAAK,GAAQ,EAAM,OAAO,IAAS,EAAM,MAAM,IAExD,EAAQ,GAAK,EAAM,OAAO,GAC1B,EAAM,GAAK,EAAM,MAAM,GACvB,KAIJ,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CACU,QAAC,KAAK,IAAI,GACV,MAAD,KAAK,IAAI,OA/ExB,CAAI,IAAJ,iBAAI,MAAJ,SAoFiB,GAKP,IAJA,MAAO,EAAM,EAAE,MAAM,OAErB,EAAW,YAAO,EAAM,EAAE,OAC5B,EAAO,EACFX,EAAI,EAAG,EAAI,GAAQ,EAAO,EAAM,KAAK,OAAQ,IAChD,IAAM,EAAM,KAAK,KACnB,EAAY,GAAK,KAAK,MACnB,EAAM,KAAK,GAAQ,EAAM,OAAO,IAAS,EAAM,MAAM,IAExD,KAIJ,OAAO,IAlGX,CAAI,IAAJ,UAAI,MAAJ,SAqGU,GACN,QAAoB,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,YAGX,IAAd,EAAK,WACW,IAAhB,EAAK,aACS,IAAd,EAAK,WACU,IAAf,EAAK,OACL,CAMA,IALA,IAAM,EAAO,EAAK,OAAO,OAEnB,EAAoB,IAAI,MAAM,GAAM,KAAK,GACzC,EAAkB,IAAI,MAAM,GAAM,KAAK,GACzC,EAAO,EACF,EAAI,EAAG,EAAI,GAAQ,EAAO,EAAK,KAAK,OAAQ,IAC/C,IAAM,EAAK,KAAK,KAClB,EAAQ,GAAK,EAAK,OAAO,GACzB,EAAM,GAAK,EAAK,MAAM,GACtB,KAIJ,EAAK,QAAU,EACf,EAAK,MAAQ,SAEN,EAAI,cACJ,EAAI,YACJ,EAAI,KAIf,+DAAc,KArIlB,CAAI,IAAJ,qBAAI,MAAJ,SAwIqB,GAajB,IAZA,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGD,EAAO,EAAM,EAAE,MAAM,OAErB,EAAW,YAAO,EAAM,EAAE,OAC1B,EAAoB,IAAI,MAAM,GAAM,KAAK,GACzC,EAAkB,IAAI,MAAM,GAAM,KAAK,GACzC,EAAO,EACF,EAAI,EAAG,EAAI,GAAQ,EAAO,EAAM,KAAK,OAAQ,IAChD,IAAM,EAAM,KAAK,KACnB,EAAY,GAAK,KAAK,MACnB,EAAM,KAAK,GAAQ,EAAM,OAAO,IAAS,EAAM,MAAM,IAExD,EAAQ,GAAK,EAAM,OAAO,GAC1B,EAAM,GAAK,EAAM,MAAM,GACvB,KAIJ,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,UACA,WA1KN,CAAI,IAAJ,qBAAI,MAAJ,SA8KqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,KAAjC,YAAyC,EAAM,OAA/C,YAAyD,EAAM,KAA/D,YAAuE,EAAM,WA/KjF,GAAkE,ICTrD,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,oBAAI,MAAJ,SAcoB,GACV,MAAN,wCAC0B,KAAK,QAD/B,gCAEa,KAAK,QAFlB,qBAGI,KAAK,UAAU,QAHnB,yCAKwB,KAAK,QAL7B,+LAgBE,KAAK,iBAhBP,YAfJ,CAAI,IAAJ,kBAAI,MAAJ,WAoCI,MAAO,CAAC,OApCZ,CAAI,IAAJ,eAAI,MAAJ,WAwCU,MAAN,gBACE,KAAK,eAAe,UADtB,yBACgD,KAAK,QADrD,cAxCJ,CAAI,IAAJ,kBAAI,MAAJ,WA8CU,MAAC,CAAC,CAAC,KAAM,SAAU,OAAQ,KAAK,QAAS,KAAM,YA9CzD,CAAI,IAAJ,OAAI,MAAJ,SAiDO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAGlD,IAAM,EAAc,KAAK,eAAe,GAExC,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CACE,OAAQ,KAAK,QAAQ,EAAM,YA5DnC,CAAI,IAAJ,iBAAI,MAAJ,SAiEiB,GAIb,IAHA,IAAM,EAAO,EAAM,EAAE,MAAM,OAErB,EAAW,YAAO,EAAM,EAAE,OACvB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAY,GAAK,KAAK,MAAM,EAAY,GAAK,EAAM,OAAO,IAG5D,OAAO,IAzEX,CAAI,IAAJ,UAAI,MAAJ,SA4EU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KAjFlB,CAAI,IAAJ,qBAAI,MAAJ,SAoFqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,OAAQ,EAAM,UApGpB,CAAI,IAAJ,qBAAI,MAAJ,SAwGqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,YAzGrC,GAAqE,ICqBxD,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,eAAI,MAAJ,WAcU,MAAN,gBACE,KAAK,eAAe,WADtB,2BAdJ,CAAI,IAAJ,kBAAI,MAAJ,WAoBU,MAAC,CAAC,CAAC,KAAM,UAAW,KAAM,YApBpC,CAAI,IAAJ,oBAAI,MAAJ,SAwBoB,GAChB,wCACoB,KAAK,QADzB,4NAQE,KAAK,iBARP,YAzBJ,CAAI,IAAJ,iBAAI,MAAJ,SAqCiB,GACb,OAAO,EAAM,EAAE,QAtCnB,CAAI,IAAJ,kBAAI,MAAJ,WA0CI,MAAO,CAAC,IAAK,OAAQ,WAAY,QAAS,UA1C9C,CAAI,IAAJ,OAAI,MAAJ,SA6CO,GACH,OAAO,KAAK,QACV,EAAM,EAAE,MACR,CACE,EAAG,EAAM,EACT,KAAM,EAAM,KACZ,SAAU,EAAM,SAChB,MAAO,EAAM,MACb,KAAM,EAAM,MAEd,CAAC,QAAS,EAAM,YAvDtB,CAAI,IAAJ,UAAI,MAAJ,SA2DU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KAhElB,CAAI,IAAJ,qBAAI,MAAJ,SAmEqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,UAAW,EAAM,KAAK,MACtB,UAAW,EAAM,KAAK,OAAO,MAC7B,WAAY,EAAM,KAAK,OAAO,OAE9B,UAAW,EAAM,KAAK,MACtB,UAAW,EAAM,KAAK,OAAO,MAC7B,WAAY,EAAM,KAAK,OAAO,OAE9B,WAAY,EAAM,MAAM,MACxB,WAAY,EAAM,MAAM,OAAO,MAC/B,YAAa,EAAM,MAAM,OAAO,OAEhC,cAAe,EAAM,SAAS,MAC9B,cAAe,EAAM,SAAS,OAAO,MACrC,eAAgB,EAAM,SAAS,OAAO,OAEtC,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,QAAS,EAAM,WAnGrB,CAAI,IAAJ,qBAAI,MAAJ,SAuGqB,GAEjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,KAAK,MAAtC,YAA+C,EAAM,SAAS,MAA9D,YAAuE,EAAM,MAAM,MAAnF,YAA4F,EAAM,KAAK,MAAvG,YAAgH,EAAM,aAzG1H,GAAsE,ICtCzD,GAAb,WAQI,SAAF,EACU,GACqB,IAArB,EAAqB,uDAAD,EAAC,oBADrB,aACA,yBAJF,YAA6C,GANrD,uBAAE,IAAJ,aAAI,MAAJ,SAaa,GACT,IAAM,EAAG,kBAAc,GACjB,QAAmB,IAArB,KAAK,OAAO,GAAoB,CAC1B,IAAF,EAAK,KAAK,MAAM,GACd,EAAL,QAAQ,IAEX,KAAK,OAAO,GAAO,CACjB,WAAY,EACZ,SAAU,EACV,UAAW,GAIT,OAAC,KAAK,OAAO,KA1BvB,CAAI,IAAJ,OAAI,MAAJ,SA6BO,EAAc,GACjB,IAAM,EAAY,KAAK,WAAW,GAG5B,EAAoB,EAAU,UAAU,mBAAmB,QAE1B,IAAnC,KAAK,OAAO,KACd,KAAK,OAAO,GAAqB,CAC/B,WAAY,EACZ,SAAU,IAGd,IAAM,EAAS,KAAK,OAAO,GAG3B,GAFA,EAAO,WAEH,EAAO,UAAY,KAAK,kBAAmB,CAC7C,QAAyB,IAArB,EAAO,UAAyB,CAClC,EAAO,UAAY,KAAK,MAAM,GAE9B,IAAM,EAAc,EAAU,UAAU,mBAAmB,GAE3D,EAAO,UAAU,QAAQ,GAE3B,OAAO,EAAO,UAAU,KAAK,GAI7B,OAFA,EAAU,WAEH,EAAU,UAAU,KAAK,OAxDtC,KCDa,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,qBAAe,EAAf,SAZJ,GAEU,ICFG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,iBAAW,OAZf,GAEU,ICyBG,GAAb,YAAE,qBAAF,iBAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARlC,uBAAE,IAAJ,oBAAI,MAAJ,SAYoB,GACV,MAAN,wCAC0B,KAAK,QAD/B,sOAYE,KAAK,iBAZP,YAbJ,CAAI,IAAJ,kBAAI,MAAJ,WA8BU,MAAC,CAAC,IAAK,UA9BjB,CAAI,IAAJ,eAAI,MAAJ,WAkCU,MAAN,gBACE,KAAK,eAAe,UADtB,+BAEE,KAAK,eAAe,UAFtB,+BAGE,KAAK,eAAe,SAHtB,4BAIE,KAAK,eAAe,SAJtB,uBAlCJ,CAAI,IAAJ,kBAAI,MAAJ,WA2CI,MAAO,CACL,CAAC,KAAM,SAAU,KAAM,SACvB,CAAC,KAAM,SAAU,KAAM,SACvB,CAAC,KAAM,SACP,CAAC,KAAM,YA/Cb,CAAI,IAAJ,OAAI,MAAJ,SAmDO,GACH,OAAI,KAAK,kBAAoC,IAArB,KAAK,YACpB,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,MAAO,KAAM,EAAM,OAG9D,KAAK,QACV,EAAM,MAAM,MACZ,CAAC,EAAG,EAAM,MAAO,KAAM,EAAM,MAC7B,CACE,YAAyB,IAAjB,EAAM,OAAuB,EAAM,OAAS,EACpD,YAAyB,IAAjB,EAAM,OAAuB,EAAM,OAAS,EACpD,WAAwB,IAAjB,EAAM,OAAuB,EAAI,EACxC,WAAwB,IAAjB,EAAM,OAAuB,EAAI,MA/DhD,CAAI,IAAJ,iBAAI,MAAJ,SAoEiB,GACb,OAAO,EAAM,MAAM,QArEvB,CAAI,IAAJ,UAAI,MAAJ,SAwEU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KA7ElB,CAAI,IAAJ,qBAAI,MAAJ,SAgFqB,GACjB,IAAM,EAAa,GAAiB,wBAClC,EAAM,MAAM,KACZ,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,MAAM,MACpB,OAAQ,EAAM,MAAM,OAAO,MAC3B,QAAS,EAAM,MAAM,OAAO,OAE5B,UAAW,EAAM,KAAK,MACtB,UAAW,EAAM,KAAK,OAAO,MAC7B,WAAY,EAAM,KAAK,OAAO,OAE9B,YAAa,EAAM,MAAM,MACzB,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,YAAyB,IAAjB,EAAM,OAAuB,EAAM,OAAS,EACpD,YAAyB,IAAjB,EAAM,OAAuB,EAAM,OAAS,EACpD,WAAwB,IAAjB,EAAM,OAAuB,EAAI,EACxC,WAAwB,IAAjB,EAAM,OAAuB,EAAI,KAtG9C,CAAI,IAAJ,qBAAI,MAAJ,SA0GqB,GACjB,gBAAU,EAAM,MAAM,MAAtB,YAA+B,EAAM,OAArC,YAA+C,EAAM,YA3GzD,GAEU,ICGG,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,kCAE9B,cAAM,EAAmB,EAAO,IAPxB,cAAgB,IAKM,EARhC,uBAAE,IAAJ,gBAAI,MAAJ,WAcI,0CACsB,KAAK,QAAU,EADrC,umBAdJ,CAAI,IAAJ,cAAI,MAAJ,WA6CI,mFAIe,KAAK,QAJpB,mBAKEQ,KAAK,UAAU,YALjB,qCAMsB,KAAK,QAN3B,iJAac,KAAK,QAbnB,mBAcE,KAAK,UAAU,WAdjB,8DAiBwB,KAAK,cAjB7B,wNA0B4B,KAAK,cA1BjC,8HAiCM,KAAK,gBAjCX,wGAuCM,KAAK,eAAe,WAAY,UAvCtC,4BA7CJ,CAAI,IAAJ,eAAI,MAAJ,WA2FI,sBACE,KAAK,eAAe,MADtB,yBAEE,KAAK,eAAe,cAFtB,iCAGE,KAAK,eAAe,YAHtB,+BAIE,KAAK,eAAe,KAJtB,wBAKE,KAAK,eAAe,aALtB,0BAKoD,KAAK,QALzD,mBAME,KAAK,eAAe,QANtB,qBAM0C,KAAK,QAN/C,mBAOE,KAAK,eAAe,WAPtB,wBAOgD,KAAK,QAPrD,cA3FJ,CAAI,IAAJ,oBAAI,MAAJ,SAuGoB,GACV,MAAN,wCAC0B,KAAK,QAD/B,iDAII,KAAK,cAJT,+CASE,KAAK,iBATP,YAxGJ,CAAI,IAAJ,kBAAI,MAAJ,WAsHI,MAAO,CAAC,IAAK,OAtHjB,CAAI,IAAJ,kBAAI,MAAJ,WA0HU,MAAC,CACL,CAAC,KAAM,MACP,CAAC,KAAM,cACP,CAAC,KAAM,KACP,CAAC,KAAM,YACP,CAAC,KAAM,OAAQ,OAAuB,EAAf,KAAK,SAC5B,CAAC,KAAM,UAAW,OAAQ,KAAK,SAC/B,CAAC,KAAM,YAAa,OAAQ,KAAK,YAjIvC,CAAI,IAAJ,OAAI,MAAJ,SAqIO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,IAGxD,MAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,GAClB,EAAK,EAAM,EAAE,MAAM,GAEnB,EAAa,EAAQ,GAErB,EAAI,EACR,EACA,EACA,EAAM,KAAK,MAAM,EAAG,EAAM,KAAK,OAAS,GACxC,EAAM,KAAK,MAAM,EAAM,KAAK,OAAS,GACrC,EAAM,UACN,EAAM,SAEJ,EAAc,CAAC,EAAG,GAGtB,OAFA,EAAc,EAAY,OAAO,GAE1B,KAAK,QACV,EACA,CAAC,EAAG,EAAM,EAAG,EAAG,EAAM,GACtB,CACE,KACA,aACA,IACA,SAAU,EAAE,OACZ,KAAM,KAAK,QAAQ,EAAM,KAAqB,EAAf,KAAK,SACpC,QAAS,KAAK,QAAQ,EAAM,SAC5B,UAAW,KAAK,QAAQ,EAAM,eAxKtC,CAAI,IAAJ,iBAAI,MAAJ,SA6KiB,GACb,IAAM,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,MAAM,GACxB,EAAI,EAAM,EAAE,MAAM,GAElB,EAAI,EACR,EACA,EACA,EAAM,KAAK,MAAM,EAAG,EAAM,KAAK,OAAS,GACxC,EAAM,KAAK,MAAM,EAAM,KAAK,OAAS,GACrC,EAAM,UACN,EAAM,SAEJ,EAAc,CAAC,EAAG,GAGtB,OAFA,EAAc,EAAY,OAAO,KA5LrC,CAAI,IAAJ,UAAI,MAAJ,SAiMU,QACc,IAAhB,EAAK,SACP,EAAK,GAAK,EAAK,OAAO,GACtB,EAAK,WAAa,EAAQ,EAAK,OAAO,MAAM,IAC5C,EAAK,SAAW,EAAK,OAAO,OAAS,EACrC,KAAK,QAAU,EAAK,OAAO,aAET,IAAhB,EAAK,SACP,EAAK,EAAI,EAAK,OAAO,GACrB,EAAK,SAAW,EAAK,OAAO,OAAS,EAErC,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KA/MlB,CAAI,IAAJ,qBAAI,MAAJ,SAkNqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGD,EAAa,EAAQ,EAAM,EAAE,MAAM,MAAM,IAEzC,EAAI,EAAM,EAAE,MAAM,GAClB,EAAI,EAAM,EAAE,MAAM,MAAM,GAE9B,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,KAAM,EAAM,KACZ,UAAW,EAAM,UACjB,QAAS,EAAM,QAEf,GAAI,EAAM,EAAE,MAAM,GAClB,WAAY,EACZ,SAAU,EAAE,OACZ,EAAG,KAlPT,CAAI,IAAJ,qBAAI,MAAJ,SAsPqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,EAAE,MAAnC,YAA4C,EAAM,UAAlD,YAA+D,EAAM,KAArE,YAA6E,EAAM,UAAnF,YAAgG,EAAM,aAvP1G,GAEU,IClCG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,gCAA0B,EAA1B,UAZJ,GAEU,ICSG,GAAb,YAAE,qBAAF,iBAOI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAZlC,uBAAE,IAAJ,YAAI,MAAJ,SAeY,GACR,uBAAiB,EAAjB,YAhBJ,CAAI,IAAJ,OAAI,MAAJ,SAmBO,GACH,OAAO,KAAK,QACV,EAAM,MAAM,MACZ,CAAC,EAAG,EAAM,OACV,CAAC,OAAQ,EAAM,OAAQ,IAAK,EAAM,QAvBxC,CAAI,IAAJ,qBAAI,MAAJ,SA2BqB,GACjB,IAAM,EAAI,0EAA4B,GAEtC,sCACK,GAAI,CACP,OAAQ,EAAM,OACd,IAAK,EAAM,QAjCjB,CAAI,IAAJ,qBAAI,MAAJ,SAqCqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,OAAnD,YAA6D,EAAM,OAtCvE,CAAI,IAAJ,eAAI,MAAJ,WA0CI,sBACE,KAAK,eAAe,UADtB,+BAEE,KAAK,eAAe,OAFtB,uBA1CJ,CAAI,IAAJ,kBAAI,MAAJ,WAiDI,MAAO,CACL,CAAC,KAAM,SAAU,KAAM,SACvB,CAAC,KAAM,MAAO,KAAM,cAnD1B,GAEU,ICWG,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,eAAI,MAAJ,WAcI,sBACE,KAAK,eAAe,UADtB,uBAC8C,KAAK,QADnD,cAdJ,CAAI,IAAJ,kBAAI,MAAJ,WAoBU,MAAC,CAAC,CAAC,KAAM,SAAU,OAAQ,KAAK,YApB1C,CAAI,IAAJ,oBAAI,MAAJ,SAwBoB,GAChB,8CAC0B,KAAK,QAD/B,mCAEgB,KAAK,QAFrB,qBAGI,KAAK,UAAU,WAHnB,oEAOwB,KAAK,QAP7B,oaA2BE,KAAK,iBA3BP,YAzBJ,CAAI,IAAJ,kBAAI,MAAJ,WAyDI,MAAO,CAAC,IAAK,YAzDjB,CAAI,IAAJ,OAAI,MAAJ,SA4DO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,EAAG,OAAQ,EAAM,SAGnE,IAAM,EAAc,KAAK,eAAe,GAExC,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,EAAG,OAAQ,EAAM,QAC3B,CAAC,OAAQ,KAAK,IAAI,EAAM,YAtE9B,CAAI,IAAJ,iBAAI,MAAJ,SA0EiB,GACb,OAAO,EAAM,EAAE,QA3EnB,CAAI,IAAJ,UAAI,MAAJ,SA8EU,QACc,IAAhB,EAAK,SACP,KAAK,QAAU,EAAK,OAAO,QAG7B,+DAAc,KAnFlB,CAAI,IAAJ,qBAAI,MAAJ,SAsFqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EAAM,OAAO,MAC1B,YAAa,EAAM,OAAO,OAAO,MACjC,aAAc,EAAM,OAAO,OAAO,OAElC,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,OAAQ,EAAM,UA1GpB,CAAI,IAAJ,qBAAI,MAAJ,SA8GqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,OAAO,MAAxC,YAAiD,EAAM,YA/G3D,GAAsE,ICxBzD,GAAb,kDAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,oBAAc,EAAd,SAZJ,GAEU,IAcG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,qBAAe,EAAf,SAZJ,GAEU,IAcG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,qBAAe,EAAf,oBAAgC,EAAhC,cAZJ,GAEU,IClCG,GAAb,kDAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,oBAAc,EAAd,SAZJ,GAEU,IAcG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,qBAAe,EAAf,SAZJ,GAEU,IAcG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,gCAA0B,EAA1B,wBAA+C,EAA/C,UAZJ,GAEU,IClCG,GAAb,kDAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,oBAAc,EAAd,SAZJ,GAEU,IAcG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,qBAAe,EAAf,SAZJ,GAEU,IAcG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,yBAAmB,EAAnB,6BAA6C,EAA7C,gBAZJ,GAEU,ICvCG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,gBAAU,EAAV,cAAiB,KAJrB,CAAI,IAAJ,OAAI,MAAJ,SAMO,GACH,gBAAU,EAAV,kBAAuB,EAAvB,QAPJ,CAAI,IAAJ,OAAI,MAAJ,SAUO,GACH,gBAAU,OAXd,GAEU,ICFG,GAAb,YAAE,qBAAF,iBAAE,SAAF,IAAI,2BAAJ,wBAAE,uBAAE,IAAJ,SAAI,MAAJ,SAGS,EAAW,GAChB,oBAAc,EAAd,eAAsB,KAJ1B,CAAI,IAAJ,OAAI,MAAJ,SAMO,GACH,gBAAU,EAAV,kBAAuB,EAAvB,QAPJ,CAAI,IAAJ,OAAI,MAAJ,SAUO,GACH,oBAAc,EAAd,SAXJ,GAEU,ICcG,GAAb,YAAE,qBAAF,iBAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARlC,uBAAE,IAAJ,YAAI,MAAJ,SAWY,GACR,4EAAsE,EAAtE,eAZJ,CAAI,IAAJ,OAAI,MAAJ,SAeO,GACH,OAAO,KAAK,QACV,EAAM,MAAM,MACZ,CAAC,EAAG,EAAM,OACV,CAAC,MAAO,EAAM,MAAO,KAAM,EAAM,SAnBvC,CAAI,IAAJ,qBAAI,MAAJ,SAuBqB,GACjB,IAAM,EAAI,0EAA4B,GAEtC,sCACK,GAAI,CACP,MAAO,EAAM,MACb,KAAM,EAAM,SA7BlB,CAAI,IAAJ,qBAAI,MAAJ,SAiCqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,MAAnD,YAA4D,EAAM,QAlCtE,CAAI,IAAJ,eAAI,MAAJ,WAsCI,sBACE,KAAK,eAAe,SADtB,8BAEE,KAAK,eAAe,QAFtB,wBAtCJ,CAAI,IAAJ,kBAAI,MAAJ,WA6CI,MAAO,CACL,CAAC,KAAM,QAAS,KAAM,SACtB,CAAC,KAAM,OAAQ,KAAM,cA/C3B,GAEU,ICFG,GAAb,YAAE,qBAAF,iBAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARlC,uBAAE,IAAJ,YAAI,MAAJ,SAWY,GACR,MAAM,IAAI,MAAM,6BAZpB,CAAI,IAAJ,oBAAI,MAAJ,SAeoB,GAChB,gUAhBJ,CAAI,IAAJ,OAAI,MAAJ,SA6BO,GACH,OAAO,KAAK,QACV,EAAM,MAAM,MACZ,CAAC,EAAG,EAAM,OACV,CAAC,OAAQ,EAAM,OAAQ,MAAO,EAAM,UAjC1C,CAAI,IAAJ,qBAAI,MAAJ,SAqCqB,GACjB,IAAM,EAAI,0EAA4B,GAEtC,sCACK,GAAI,CACP,OAAQ,EAAM,OACd,MAAO,EAAM,UA3CnB,CAAI,IAAJ,qBAAI,MAAJ,SA+CqB,GACjB,0FAAmC,GAAnC,YAA6C,EAAM,OAAnD,YAA6D,EAAM,SAhDvE,CAAI,IAAJ,eAAI,MAAJ,WAoDI,sBACE,KAAK,eAAe,UADtB,+BAEE,KAAK,eAAe,SAFtB,yBApDJ,CAAI,IAAJ,kBAAI,MAAJ,WA2DI,MAAO,CACL,CAAC,KAAM,SAAU,KAAM,SACvB,CAAC,KAAM,QAAS,KAAM,cA7D5B,GAEU,ICbG,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,sDAWY,GACR,sBAAgB,EAAhB,mBAZJ,GAEU,ICmEG,GAAb,YAAE,qBAAF,iBAkBI,SAAF,EACE,EACO,EACP,GAAgB,IAAJ,EAAI,4BAEZ,EAAJ,YAAM,GAAU,YAHT,QAJE,EAAJ,SAAU,EASX,EAAC,KAAO,EAAQ,GAGZ,EAAD,OADH,aAAkB,MACN,GAAiB,gBAAgB,EAAQ,EAAK,OAE9C,EATJ,EArBd,uBAAE,IAAJ,OAAI,MAAJ,SAiD4B,GACxB,GAAc,YAAV,EACF,MAAM,IAAI,MAAM,sDAElB,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KArD5C,CAAI,IAAJ,YAAI,MAAJ,WAwDW,WACP,GAAmB,YAAf,KAAK,MACP,OAAO,IAAI,SAAQ,YACjB,GAAG,CAAC,YAAa,EAAK,OAAO,aAA7B,EAA2C,WACzC,IAAI,EAAS,IAAI,aAAa,EAAK,OAAO,MAG1C,GAFA,EAAS,GAAG,KAAK,GAEE,YAAf,EAAK,MACP,EAAQ,EAAO,SAAS,EAAG,EAAK,WAC3B,CAIL,IAHA,IAAM,EAAM,IAAI,EAAwB,EAAK,OAC3C,EAAK,MAEE,EAAI,EAAG,EAAI,EAAK,KAAM,IAC7B,EAAI,GAAK,EAAO,GAElB,EAAQ,UAKhB,MAAM,IAAI,MAAM,wDA7EpB,CAAI,IAAJ,WAAI,MAAJ,WAiFI,OAAO,KAAK,QAjFhB,CAAI,IAAJ,eAAI,MAAJ,SAoFe,GACX,OAAO,IAAI,EACT,IAAI,MAAM,KAAK,MAAM,KAAK,GAC1B,KAAK,MACL,KAAK,SAxFX,CAAI,IAAJ,iBAAI,MAAJ,SA4FiB,GACb,OAAO,IAAI,EAAU,CAAC,GAAQ,CAAC,GAAI,KAAK,SA7F5C,CAAI,IAAJ,SAAI,MAAJ,WAiGS,KAAK,UACR,KAAK,SAAU,EACf,GAAiB,WAAW,KAAK,QAEjC,KAAK,YAAS,KArGpB,CAAI,IAAJ,OAAI,MAAJ,WA0GI,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KAAK,SA1GjD,CAAI,IAAJ,MAAI,MAAJ,WA8GI,OAAO,GAAY,KAAK,CAAC,MAAO,MAAO,KAAK,SA9GhD,CAAI,IAAJ,MAAI,MAAJ,WAkHI,OAAO,GAAY,KAAK,CAAC,MAAO,MAAO,KAAK,SAlHhD,CAAI,IAAJ,OAAI,MAAJ,WAsHI,OAAO,GAAa,KAAK,CAAC,MAAO,MAAOA,KAAK,SAtHjD,CAAI,IAAJ,MAAI,MAAJ,WA0HI,OAAO,GAAY,KAAK,CAAC,MAAO,MAAO,KAAK,SA1HhD,CAAI,IAAJ,MAAI,MAAJ,WA8HI,OAAO,GAAY,KAAK,CAAC,MAAO,MAAO,KAAK,SA9HhD,CAAI,IAAJ,MAAI,MAAJ,WAkII,OAAO,GAAY,KAAK,CAAC,MAAO,MAAO,KAAK,SAlIhD,CAAI,IAAJ,MAAI,MAAJ,WAsII,OAAO,GAAY,KAAK,CAAC,MAAO,MAAO,KAAK,SAtIhD,CAAI,IAAJ,OAAI,MAAJ,WA0II,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KAAK,SA1IjD,CAAI,IAAJ,OAAI,MAAJ,WA8II,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KAAK,SA9IjD,CAAI,IAAJ,OAAI,MAAJ,WAkJI,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KAAK,SAlJjD,CAAI,IAAJ,OAAI,MAAJ,WAsJI,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KAAK,SAtJjD,CAAI,IAAJ,OAAI,MAAJ,WA0JI,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KAAK,SA1JjD,CAAI,IAAJ,OAAI,MAAJ,WA8JI,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KAAK,SA9JjD,CAAI,IAAJ,QAAI,MAAJ,WAkKI,MAAM,IAAI,MAAM,4BAlKpB,CAAI,IAAJ,QAAI,MAAJ,WAsKI,MAAM,IAAI,MAAM,4BAtKpB,CAAI,IAAJ,QAAI,MAAJ,WA0KI,MAAM,IAAI,MAAM,4BA1KpB,CAAI,IAAJ,UAAI,MAAJ,WA8KI,OAAO,GAAgB,KAAK,CAAC,MAAO,MAAO,KAAK,SA9KpD,CAAI,IAAJ,cAAI,MAAJ,SAiLc,EAAe,GACzB,OAAO,GAAoB,KACzB,CAAC,MAAO,KAAM,QAAO,QACrB,KAAK,SApLX,CAAI,IAAJ,QAAI,MAAJ,WAyLU,OAAC,GAAc,KAAK,CAAC,MAAO,MAAO,KAAK,SAzLlD,CAAI,IAAJ,OAAI,MAAJ,WA6LI,OAAO,GAAa,KAAK,CAAC,MAAO,MAAO,KAAK,SA7LjD,CAAI,IAAJ,QAAI,MAAJ,WAiMI,OAAO,GAAc,KAAK,CAAC,MAAO,MAAO,KAAK,SAjMlD,CAAI,IAAJ,SAAI,MAAJ,WAqMI,OAAO,GAAe,KAAK,CAAC,MAAO,MAAO,KAAK,SArMnD,CAAI,IAAJ,oBAAI,MAAJ,SAwMoB,EAAgB,GAChC,OAAO,GAA0B,KAC/B,CAAC,MAAO,KAAM,SAAQ,OACtB,KAAK,SA3MX,CAAI,IAAJ,cAAI,MAAJ,SA+Mc,EAAe,GACzB,OAAO,GAAoB,KACzB,CAAC,MAAO,KAAM,SAAQ,SACtB,KAAK,SAlNX,CAAI,IAAJ,OAAI,MAAJ,WAuNI,OAAO,GAAa,KAAK,CAAC,MAAOA,MAAO,KAAK,SAvNjD,CAAI,IAAJ,YAAI,MAAJ,SA0NY,EAAsB,GACxB,kBAAkB,GACtB,MAAM,IAAI,MAAM,yCAElB,OAAO,GAAkB,KACvB,CAAC,EAAG,KAAM,OAAQ,EAAQ,UAC1B,KAAK,SAhOX,CAAI,IAAJ,WAAI,MAAJ,SAqOI,EACA,EACA,EACA,EACA,GAEM,kBAAkB,MAAgB,aAAc,GAC5C,MAAF,IAAI,MAAM,yCAEZ,OAAC,GAAY,KACjB,CAAS,EAAL,EAAI,EAAG,EAAQ,YAAa,EAAa,QAAO,QACpD,KAAK,SAhPX,CAAI,IAAJ,gBAAI,MAAJ,SAqPI,EACA,EACA,EACA,EACA,GAEM,kBAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,gDAElB,OAAO,GAAiB,KACtB,CAAC,EAAG,EAAI,EAAG,EAAQ,YAAa,EAAa,QAAO,QACpD,KAAK,SAhQX,CAAI,IAAJ,gBAAI,MAAJ,SAqQI,EACA,EACA,EACA,GAEA,KAAM,aAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,gDAElB,OAAO,GAAiB,KACtB,CAAC,EAAG,EAAI,EAAG,EAAQ,YAAa,EAAa,SAC7C,KAAK,SA/QX,CAAI,IAAJ,cAAI,MAAJ,SAoRI,EACA,EACA,EACA,GAEA,KAAM,aAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,4CAElB,OAAO,GAAe,KACpB,CAAC,EAAG,EAAI,EAAG,EAAQ,YAAa,EAAa,SAC7C,KAAK,SA9RX,CAAI,IAAJ,aAAI,MAAJ,SAmSI,EACA,EACA,GAEA,KAAM,aAAkB,MAAgB,aAAc,GACpD,MAAM,IAAI,MAAM,mDAEZ,OAAC,GAAc,KACnB,CAAC,EAAG,EAAI,EAAG,EAAQ,YAAa,GAChC,KAAK,SA5SX,CAAI,IAAJ,SAAI,MAAJ,SAgTS,GACL,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,qDAElB,OAAO,GAAe,KACpB,CAAC,EAAG,KAAM,EAAG,GACb,KAAK,SAtTX,CAAI,IAAJ,YAAI,MAAJ,SA2TI,EACA,EACA,EACA,EACA,EACA,GAEM,KACF,aAAa,SAAoB,IAAN,GAAmB,aAAa,IAE7D,MAAM,IAAI,MAAM,qCAEZ,YAAI,IAAN,EACK,GAAa,KAClB,CAAC,EAAG,KAAMI,IAAG,aAAY,aAAY,QAAO,QAC5C,KAAK,OAGA,GAAc,KACnB,CACE,EAAG,KACH,IACA,EAAG,EACH,aACA,aACA,QACA,QAEF,KAAK,SAvVb,CAAI,IAAJ,WAAI,MAAJ,SA4VW,EAAgB,GACvB,OAAO,GAAY,KACjB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SA/VX,CAAI,IAAJ,iBAAI,MAAJ,SAmWiB,EAAgB,GAC7B,OAAO,GAAkB,KACvB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SAtWX,CAAI,IAAJ,kBAAI,MAAJ,SA0WkB,EAAgB,GAC9B,OAAO,GAAa,KAClB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SA7WX,CAAI,IAAJ,wBAAI,MAAJ,SAiXwB,EAAgB,GACpC,OAAO,GAAmB,KACxB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SApXX,CAAI,IAAJ,oBAAI,MAAJ,SAwX8B,EAAgB,GAC1C,OAAO,GAAe,KACpB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SA3XX,CAAI,IAAJ,uBAAI,MAAJ,SAgYI,EACA,GAEA,OAAO,GAAkB,KACvB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SArYX,CAAI,IAAJ,eAAI,MAAJ,SAyYe,EAAgB,GAC3B,OAAO,GAAgB,KACrB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SA5YX,CAAI,IAAJ,WAAI,MAAJ,SAgZW,EAAgB,GACvB,OAAO,GAAY,KACjB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SAnZX,CAAI,IAAJ,WAAI,MAAJ,SAuZW,EAAgB,GACvB,OAAO,GAAY,KACjB,CAAC,EAAG,KAAM,OAAM,YAChB,KAAK,SA1ZX,CAAI,IAAJ,YAAI,MAAJ,SA+ZI,EACA,EACA,EACA,EACA,EACA,EACA,GAEA,KACI,aAAkB,SACV,IAAT,KAAwB,aAAgB,GAEzC,MAAM,IAAI,MAAM,yDAGlB,YAAa,IAAT,EACK,GAAa,KAClB,CACE,EAAG,KACH,EAAG,EACH,OACA,YACA,UACA,cAEF,KAAK,OAGA,GAAiB,KACtB,CACE,EAAG,KACH,EAAG,EACH,EAAG,EACH,OACA,YACA,UACA,cAEF,KAAK,SArcb,CAAI,IAAJ,qBAAI,MAAJ,SA2cI,EACA,EACA,EACA,EACA,GAEA,KAAM,aAAkB,GACtB,MAAM,IAAI,MACR,mEAIJ,OAAO,GAAsB,KAC3B,CACE,EAAG,KACH,EAAG,EACH,OACA,YACA,WAEF,KAAK,SA/dX,CAAI,IAAJ,mBAAI,MAAJ,SAoeI,EACA,EACA,EACA,GAEA,OAAO,GAAoB,KACzB,CACE,EAAG,KACH,aACA,cACA,OACA,WAEF,KAAK,SAjfX,CAAI,IAAJ,eAAI,MAAJ,SAqfe,EAAiB,GAC5B,OAAI,EACK,GAAa,KAClB,CAAC,MAAO,KAAM,YAAa,GAC3B,KAAK,OAGA,IAAI,EAAU,KAAK,OAAQ,EAAO,KAAK,SA5fpD,CAAI,IAAJ,SAAI,MAAJ,SAggBS,EAAsB,GAC3B,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,4CAKlB,OAHI,EAAO,IACT,GAAQ,KAAK,MAAM,QAEd,GAAe,KACpB,CAAC,EAAG,KAAM,EAAG,EAAQ,QACrB,KAAK,SAzgBX,CAAI,IAAJ,iBAAI,MAAJ,SA6gBiB,GACb,OAAO,GAAkB,KACvB,CAAC,EAAG,KAAM,eACV,KAAK,SAhhBX,CAAI,IAAJ,OAAI,MAAJ,SAohBO,EAAc,GACjB,OAAO,GAAa,KAClB,CAAC,MAAO,KAAM,OAAQ,EAAK,OAAQ,GACnC,KAAK,SAvhBX,CAAI,IAAJ,eAAI,MAAJ,SA2hBe,EAAoB,EAAc,GAC7C,OAAO,GAAqB,KAC1B,CAAC,MAAO,KAAM,OAAQ,EAAK,OAAQ,EAAK,QACxC,KAAK,SA9hBX,CAAI,IAAJ,SAAI,MAAJ,SAkiBS,GACL,OAAO,GAAe,KACpB,CAAC,EAAG,KAAM,WACV,KAAK,SAriBX,CAAI,IAAJ,SAAI,MAAJ,SAyiBS,GAAwB,MAEK,KAAK,YAAY,KAAK,MAAO,GAFlC,mBAEtB,EAFsB,KAEV,GAFU,WAG7B,OAAI,EAAc,KAAK,MAAO,GACrB,KAAK,OAEP,GAAe,KACpB,CACE,MAAO,KAAK,QAAQ,GAAQ,GAC5B,YAAa,GAEf,KAAK,SApjBX,CAAI,IAAJ,WAAI,MAAJ,SAwjBW,EAAgB,EAAe,GACtC,OAAO,GAAY,KACjB,CAAC,MAAO,KAAM,OAAM,OAAM,SAC1B,KAAK,SA3jBX,CAAI,IAAJ,SAAI,MAAJ,SA+jBS,EAAc,GACnB,OAAO,GAAe,KACpB,CAAC,EAAG,KAAM,OAAM,WAChB,KAAK,SAlkBX,CAAI,IAAJ,aAAI,MAAJ,SAukBI,EACA,EACA,EACA,GAEA,OAAO,GAAc,KACnB,CAAC,EAAG,KAAM,SAAQ,OAAM,OAAM,SAC9B,KAAK,SA9kBX,CAAI,IAAJ,WAAI,MAAJ,SAklBW,GACP,OAAO,GAAiB,KACtB,CAAC,EAAG,KAAM,UACV,KAAK,SArlBX,CAAI,IAAJ,YAAI,MAAJ,SA0lBI,EACA,EACA,EACA,EACA,GAEA,KACI,aAAgB,MAChB,aAAoB,MACpB,aAAiB,MACjB,aAAgB,GAElB,MAAM,IAAI,MAAM,uCAGlB,OAAO,GAAkB,KACvB,CACE,EAAG,KACH,KAAM,EACN,SAAU,EACV,MAAO,EACP,KAAM,EACN,WAEF,KAAK,UAlnBX,EAAI,IAAJ,QAAI,MAAJ,SAGe,EAAe,EAAe,EAAe,GAGxD,IAFA,IAAM,EAAO,KAAK,IAAI,KAAK,MAAM,EAAQ,GAAS,GAAQ,GACpD,EAAS,IAAI,MAAM,GAChB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAO,GAAK,EAAQ,EAAI,EAEpB,OAAC,IAAI,EAAU,EAAQ,CAAC,GAAO,KATzC,CAAI,IAAJ,WAAI,MAAJ,SAkCkB,GACd,IAAM,EAAU,GAAG,QAAQ,CACzB,KAAM,EACN,OAAQ,OACR,KAAM,GAAiB,aAAa,aAGhC,EAAS,GAAiB,oBAAoB,EAAS,WAEvD,EAAQ,EAAQ,MAGhB,OAAC,IAAI,EAAU,EAAQ,CAFd,EAAQ,OAEe,EAAO,GAAI,eA9CrD,GACU,GAsnBJ,SAAU,GACd,EACA,EACA,GAEA,OAAO,IAAI,GAAU,EAAG,EAAG,GAG7B,ICvrBI,GACA,GACA,GACA,GACA,GACA,GACA,GACA,GAEO,GD8qBL,GAAiB,IAAI,IACzB,SAAC,GAAD,OAAqB,IAAI,GAAgB,GAAgB,MAGrD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAgB,IAAI,IACxB,SAAC,GAAD,OAAqB,IAAI,GAAe,GAAgB,MAIpD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAkB,IAAI,IAC1B,SAAC,GAAD,OAAqB,IAAI,GAAiB,GAAgB,MAEtD,GAAsB,IAAI,IAC9B,SAAC,GAAD,OAAqB,IAAI,GAAqB,GAAgB,MAE1D,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAgB,IAAI,IACxB,SAAC,GAAD,OAAqB,IAAI,GAAe,GAAgB,MAEpD,GAAgB,IAAI,IACxB,SAAC,GAAD,OAAqB,IAAI,GAAe,GAAgB,MAEpD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAuB,IAAI,IAC/B,SAAC,GAAD,OAAqB,IAAI,GAAsB,GAAgB,MAE3D,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAiB,IAAI,IACzB,SAAC,GAAD,OAAqB,IAAI,GAAgB,GAAgB,MAErD,GAA4B,IAAI,IACpC,SAAC,GAAD,OAAqB,IAAI,GAA2B,GAAgB,MAEhE,GAAsB,IAAI,IAC9B,SAAC,GAAD,OAAqB,IAAI,GAAqB,GAAgB,MAE1D,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAInD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAsB,IAAI,IAC9B,SAAC,GAAD,OAAqB,IAAI,GAAqB,GAAgB,MAE1D,GAAmB,IAAI,IAC3B,SAAC,GAAD,OAAqB,IAAI,GAAkB,GAAgB,MAEvD,GAAwB,IAAI,IAChC,SAAC,GAAD,OAAqB,IAAI,GAAuB,GAAgB,MAE5D,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAmB,IAAI,IAC3B,SAAC,GAAD,OAAqB,IAAI,GAAkB,GAAgB,MAIvD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAmB,IAAI,IAC3B,SAAC,GAAD,OAAqB,IAAI,GAAkB,GAAgB,MAEvD,GAAmB,IAAI,IAC3B,SAAC,GAAD,OAAqB,IAAI,GAAkB,GAAgB,MAEvD,GAAiB,IAAI,IACzB,SAAC,GAAD,OAAqB,IAAI,GAAgB,GAAgB,MAErD,GAAgB,IAAI,IACxB,SAAC,GAAD,OAAqB,IAAI,GAAe,GAAgB,MAIpD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAoB,GAAgB,MAEzD,GAAqB,IAAI,IAC7B,SAAC,GAAD,OAAqB,IAAI,GAA0B,GAAgB,MAE/D,GAAoB,IAAI,IAC5B,SAAC,GAAD,OAAqB,IAAI,GAAmB,GAAgB,MAExD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAkB,IAAI,IAC1B,SAAC,GAAD,OAAqB,IAAI,GAAiB,GAAgB,MAEtD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAc,IAAI,IACtB,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MAElD,GAAiB,IAAI,IACzB,SAAC,GAAD,OAAqB,IAAI,GAAsB,GAAgB,MAE3D,GAAoB,IAAI,IAC5B,SAAC,GAAD,OAAqB,IAAI,GAAyB,GAAgB,MAI9D,GAAiB,IAAI,IACzB,SAAC,GAAD,OAAqB,IAAI,GAAgB,GAAgB,MAErD,GAAoB,IAAI,IAC5B,SAAC,GAAD,OAAqB,IAAI,GAAmB,GAAgB,MAExD,GAAe,IAAI,IACvB,SAAC,GAAD,OAAqB,IAAI,GAAc,GAAgB,MAEnD,GAAiB,IAAI,IACzB,SAAC,GAAD,OAAqB,IAAI,GAAgB,GAAgB,MAErD,GAAiB,IAAI,IACzB,SAAC,GAAD,OAAqB,IAAI,GAAgB,GAAgB,MAErD,GAAoB,IAAI,IAC5B,SAAC,GAAD,OAAqB,IAAI,GAAmB,GAAgB,MAExD,GAAiB,IAAI,IACzB,SAAC,GAAD,OAAqB,IAAI,GAAgB,GAAgB,MAErD,GAAgB,IAAI,IACxB,SAAC,GAAD,OAAqB,IAAI,GAAe,GAAgB,MAEpD,GAAoB,IAAI,IAC5B,SAAC,GAAD,OAAqB,IAAI,GAAmB,GAAgB,MC71BjD,GAA4B,IAAI,SAAQ,YACnD,8BAAsC,MAAK,YACzC,GAAW,EAAE,UACb,GAAW,EAAE,UACT,GAAO,EAAE,UACT,GAAO,EAAE,UACT,GAAM,EAAE,SACR,GAAO,EAAE,UACT,GAAO,EAAE,UACT,GAAM,EAAE,SAER,GAAgB,CAClB,QAAS,GACH,QAAG,GACT,MAAO,GACP,MAAO,GACP,KAAM,GACN,OAAQ,GACR,OAAQ,GACR,MAAO,IAGL,UAyBK,GAAb,YAAE,qBAAF,iBAcI,SAAF,EAAY,EAA6B,EAAqB,GAAY,MAGxE,GAHwE,oBACpE,EAAJ,YAAM,GAAU,WAEZ,aAAkB,MAAO,CAC3B,QAAc,IAAV,EACF,MAAM,IAAI,MACR,0DAIJ,IAAM,EAAQ,IAAI,EAAwB,EAAK,OAC7C,GAGI,EAAD,WAAa,GAAkB,EAAK,OAAO,OAC9C,EACA,QAGF,EAAK,WAAa,EAnBoD,SAd1E,uBAAE,IAAJ,OAAI,MAAJ,SAqC4B,GACxB,MAAM,IAAI,MAAM,6BAtCpB,CAAI,IAAJ,YAAI,MAAJ,WA0CI,OAAO,QAAQ,QAAQ,KAAK,WAAW,cA1C3C,CAAI,IAAJ,WAAI,MAAJ,WA8CI,OAAO,MAAM,KAAK,KAAK,WAAW,eA9CtC,CAAI,IAAJ,eAAI,MAAJ,SAiDe,GAEL,OAAC,IAAI,EAAW,CAAC,GAAQ,KAAK,WAAW,YAAa,KAAK,SAnDrE,CAAI,IAAJ,iBAAI,MAAJ,SAsDiB,GACb,OAAO,IAAI,EAAW,CAAC,GAAQ,IAAI,YAAY,CAAC,IAAK,KAAK,SAvD9D,CAAI,IAAJ,SAAI,MAAJ,gBA2D4B,IAApB,KAAK,aACP,KAAK,WAAW,OAEhB,KAAK,gBAAa,KA9DxB,CAAI,IAAJ,OAAI,MAAJ,WAmEI,OAAO,IAAI,EACT,KAAK,WAAW,YAChB,EACA,KAAK,SAtEX,CAAI,IAAJ,MAAI,MAAJ,WA2EI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,2CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,SAjF1C,CAAI,IAAJ,MAAI,MAAJ,WAqFU,KACF,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,2CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,SA3F1C,CAAI,IAAJ,OAAI,MAAJ,WA+FI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,UArG1C,CAAI,IAAJ,MAAI,MAAJ,WAyGI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAEZ,OAAC,IAAI,EAAW,KAAK,WAAW,SAlH1C,CAAI,IAAJ,MAAI,MAAJ,WAsHI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,2CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,SA5H1C,CAAI,IAAJ,MAAI,MAAJ,WAgIU,KACF,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,2CAEZ,OAAC,IAAI,EAAW,KAAK,WAAW,SAtI1C,CAAI,IAAJ,MAAI,MAAJ,WA0IU,KACF,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,2CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,SAhJ1C,CAAI,IAAJ,OAAI,MAAJ,WAoJI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAEZ,OAAC,IAAI,EAAW,KAAK,WAAW,UA1J1C,CAAI,IAAJ,OAAI,MAAJ,WA8JI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,UApK1C,CAAI,IAAJ,OAAI,MAAJ,WAwKU,KACF,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,UA9K1C,CAAI,IAAJ,OAAI,MAAJ,WAkLI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAEZ,OAAC,IAAI,EAAW,KAAK,WAAW,UAxL1C,CAAI,IAAJ,OAAI,MAAJ,WA4LI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,UAlM1C,CAAI,IAAJ,OAAI,MAAJ,WAsMI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,UA5M1C,CAAI,IAAJ,QAAI,MAAJ,WAgNI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,6CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,WAtN1C,CAAI,IAAJ,QAAI,MAAJ,WA0NU,KACF,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,6CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,WAhO1C,CAAI,IAAJ,QAAI,MAAJ,WAoOI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,6CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,WA1O1C,CAAI,IAAJ,UAAI,MAAJ,WA8OI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,+CAEZ,OAAC,IAAI,EAAW,KAAK,WAAW,aApP1C,CAAI,IAAJ,cAAI,MAAJ,SAuPc,EAAe,GACzB,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,mDAElB,OAAO,IAAI,EACT,KAAK,WAAW,aAAa,EAAO,MA/P1C,CAAI,IAAJ,SAAI,MAAJ,WAoQI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,+CAEZ,OAAC,IAAI,EAAW,KAAK,WAAW,YA7Q1C,CAAI,IAAJ,cAAI,MAAJ,SAgRc,EAAe,GACzB,OAAO,IAAI,EACT,KAAK,WAAW,aAAa,EAAO,MAlR1C,CAAI,IAAJ,oBAAI,MAAJ,SAsRoB,EAAgB,GAChC,OAAO,IAAI,EACT,KAAK,WAAW,oBAAoB,EAAQ,MAxRlD,CAAI,IAAJ,OAAI,MAAJ,WA6RI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,6CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,UAtS1C,CAAI,IAAJ,YAAI,MAAJ,SAySY,EAAsB,GACxB,kBAAkB,GACtB,MAAM,IAAI,MAAM,2CAElB,OAAO,IAAI,EACT,KAAK,WAAW,WACd,EAAO,WACP,IAAI,YAAY,OAhTxB,CAAI,IAAJ,WAAI,MAAJ,SAsTI,EACA,EAEI,EACJ,EACA,GAEA,KAAM,aAAkB,MAAiB,aAAc,GACrD,MAAM,IAAI,MAAM,2CAGlB,OAAO,IAAI,EACT,EAAG,WAAW,SAAS,EAAO,WAAY,EAAO,MAlUvD,CAAI,IAAJ,gBAAI,MAAJ,SAuUI,EACA,EAEI,EACJ,EACA,GAEA,KAAM,aAAkB,MAAiB,aAAc,GACrD,MAAM,IAAI,MAAM,kDAElB,OAAO,IAAI,EACT,EAAG,WAAW,YAAY,EAAO,WAAY,EAAO,MAlV1D,CAAI,IAAJ,gBAAI,MAAJ,SAuVI,EACA,EAEI,EACJ,GAEA,KAAM,aAAkB,MAAiB,aAAc,GACrD,MAAM,IAAI,MAAM,kDAElB,OAAO,IAAI,EAAW,EAAG,WAAW,SAAS,EAAO,WAAY,MAhWpE,CAAI,IAAJ,cAAI,MAAJ,SAoWI,EACA,EAEI,EACJ,GAEA,KAAM,aAAkB,MAAiB,aAAc,GACrD,MAAM,IAAI,MAAM,8CAElB,OAAO,IAAI,EAAW,EAAG,WAAW,OAAO,EAAO,WAAY,MA7WlE,CAAI,IAAJ,aAAI,MAAJ,SAiXI,EACA,EAEA,GAEA,KAAM,aAAkB,MAAiB,aAAc,GACrD,MAAM,IAAI,MAAM,qDAElB,OAAO,IAAI,EAAW,EAAG,WAAW,MAAM,EAAO,eAzXrD,CAAI,IAAJ,SAAI,MAAJ,SA4XS,GACC,kBAAkB,GACtB,MAAM,IAAI,MAAM,2CAElB,OAAO,IAAI,EACT,KAAK,WAAW,OAAO,EAAO,eAjYpC,CAAI,IAAJ,YAAI,MAAJ,SAsYI,EACA,EACA,EACA,EACA,EACA,GAEA,KACI,aAAa,SAAqB,IAAN,GAAmB,aAAa,IAE9D,MAAM,IAAI,MAAM,qCAElB,OACS,IAAI,OADH,IAAN,EAEA,KAAK,WAAW,YACd,EAAE,WACF,EACA,EACA,EACC,EAAuB,WACxB,GAKF,KAAK,WAAW,KACd,EAAE,WACF,EACA,EACA,MAnaV,CAAI,IAAJ,WAAI,MAAJ,SAyaW,EAAgB,GACvB,OAAO,IAAI,EACT,KAAK,WAAW,IAAI,IAAI,YAAY,GAAO,MA3ajD,CAAI,IAAJ,iBAAI,MAAJ,SA+aiB,EAAgB,GAC7B,OAAO,IAAI,EACT,KAAK,WAAW,WAAW,IAAI,YAAY,GAAO,MAjbxD,CAAI,IAAJ,eAAI,MAAJ,SAqbe,EAAgB,GAC3B,OAAO,IAAI,EACT,KAAK,WAAW,QAAQ,IAAI,YAAY,GAAO,MAvbrD,CAAI,IAAJ,WAAI,MAAJ,SA2bW,EAAgB,GACvB,OAAO,IAAI,EACT,KAAK,WAAW,IAAI,IAAI,YAAY,GAAO,MA7bjD,CAAI,IAAJ,WAAI,MAAJ,SAicW,EAAgB,GACvB,OAAO,IAAI,EACT,KAAK,WAAW,IAAI,IAAI,YAAY,GAAO,MAncjD,CAAI,IAAJ,kBAAI,MAAJ,SAuckB,EAAgB,GAC9B,OAAO,IAAI,EACT,KAAK,WAAW,YAAY,IAAI,YAAY,GAAO,MAzczD,CAAI,IAAJ,wBAAI,MAAJ,SA6cwB,EAAgB,GACpC,OAAO,IAAI,EACT,KAAK,WAAW,mBACd,IAAI,YAAY,GAChB,MAjdR,CAAI,IAAJ,oBAAI,MAAJ,SAsd8B,EAAgB,GAC1C,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,oDAElB,OAAO,IAAI,EACT,KAAK,WAAW,eACd,IAAI,YAAY,GAChB,MAheR,CAAI,IAAJ,uBAAI,MAAJ,SAseI,EACA,GAEA,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,uDAElB,OAAO,IAAI,EACT,KAAK,WAAW,mBACd,IAAI,YAAY,GAChB,MAlfR,CAAI,IAAJ,oBAAI,MAAJ,SAufoB,GAChB,MAAmB,OAAf,EACK,EACiB,SAAf,EACF,EAEA,IA7fb,CAAI,IAAJ,YAAI,MAAJ,SAkgBI,EACA,EACA,EACA,EACA,EACA,EACA,GAEA,KACI,aAAkB,SACV,IAAT,KAAwB,aAAgB,GAEzC,MAAM,IAAI,MACR,2DAIJ,IAAM,EAAiB,KAAK,kBAAkB,GAE9C,OACS,IAAI,OADA,IAAT,EAEA,KAAK,WAAW,eACd,EAAO,WACN,EAA0B,WAC3B,IAAI,YAAY,GAChB,EACA,IAAI,YAAY,GAChB,IAAI,YAAY,GAChB,GAKF,KAAK,WAAW,KACd,EAAO,WACP,IAAI,YAAY,GAChB,EACA,IAAI,YAAY,GAChB,IAAI,YAAY,GAChB,MAziBV,CAAI,IAAJ,qBAAI,MAAJ,SAgjBI,EACA,EACA,EACA,EACA,GAEA,KAAM,aAAkB,GACtB,MAAM,IAAI,MACR,qEAIJ,OAAO,IAAI,EACT,KAAK,WAAW,eACd,EAAO,WACP,IAAI,YAAY,GAChB,EACA,IAAI,YAAY,GAChB,IAAI,YAAY,OAlkBxB,CAAI,IAAJ,mBAAI,MAAJ,SAwkBI,EACA,EACA,EACA,GAEA,OAAO,IAAI,EACT,KAAK,WAAW,aACd,IAAI,YAAY,GAChB,IAAI,YAAY,GAChB,IAAI,YAAY,GAChB,MAllBR,CAAI,IAAJ,eAAI,MAAJ,SAulBe,GACX,IAAM,EAAK,IAAI,YAAY,GAC3B,OAAO,IAAI,EAAW,KAAK,WAAW,QAAQ,GAAiB,KAzlBnE,CAAI,IAAJ,SAAI,MAAJ,SA4lBS,EAAsB,GAC3B,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,8CAKlB,OAHI,EAAO,IACT,GAAQ,KAAK,WAAW,QAEnB,IAAI,EACT,KAAK,WAAW,OAAO,EAAO,WAAY,MApmBhD,CAAI,IAAJ,iBAAI,MAAJ,SAwmBiB,GACb,OAAO,IAAI,EACT,KAAK,WAAW,UAAU,IAAI,YAAY,OA1mBhD,CAAI,IAAJ,OAAI,MAAJ,SA8mBO,EAAc,GACjB,YAAY,IAAR,QAA6B,IAAR,EAChB,IAAI,EAAW,KAAK,WAAW,KAAK,EAAK,SAC/B,IAAR,EACF,IAAI,EAAW,KAAK,WAAW,SAAS,SAC9B,IAAR,EACF,IAAI,EAAW,KAAK,WAAW,SAAS,IAE1C,KAAK,SAtnBhB,CAAI,IAAJ,eAAI,MAAJ,SAynBe,EAAoB,EAAc,GAC7C,KAAM,aAAgB,GACpB,MAAM,IAAI,MAAM,8CAElB,YAAY,IAAR,QAA6B,IAAR,EAChB,IAAI,EACT,KAAK,WAAW,cAAc,EAAK,EAAK,EAAK,kBAE9B,IAAR,EACF,IAAI,EACT,KAAK,WAAW,kBAAkB,EAAK,EAAK,kBAE7B,IAAR,EACF,IAAI,EACT,KAAK,WAAW,kBAAkB,EAAK,EAAK,aAGzC,KAAK,SA1oBhB,CAAI,IAAJ,SAAI,MAAJ,SA6oBS,GACL,OAAO,IAAI,EACT,KAAK,WAAW,OAAO,IAAI,YAAY,OA/oB7C,CAAI,IAAJ,SAAI,MAAJ,SAmpBS,GACL,IAAM,EAAY,KAAK,WADM,EAIO,KAAK,YAAY,EAAW,GAJnC,mBAItB,EAJsB,KAIR,GAJQ,WAK7B,OAAI,EAAc,EAAW,GACpB,KAAK,OAKP,IAAI,EAFM,KAAK,QAAQ,GAAQ,GAG3B,WAAW,OAAO,IAAI,YAAY,OA/pBjD,CAAI,IAAJ,WAAI,MAAJ,SAyqBW,EAAgB,EAAe,GACtC,OAAO,IAAI,EACT,KAAK,WAAW,IACd,IAAI,YAAY,GAChB,EAAW,aAAa,GACxB,MA9qBR,CAAI,IAAJ,SAAI,MAAJ,SAmrBS,EAAc,GACnB,OAAO,IAAI,EACT,KAAK,WAAW,OACd,EACA,EAAQ,OACR,IAAI,YAAY,EAAQ,WAxrBhC,CAAI,IAAJ,QAAI,MAAJ,WA8rBI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,6CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,WApsB1C,CAAI,IAAJ,OAAI,MAAJ,WAwsBI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,4CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,UA9sB1C,CAAI,IAAJ,QAAI,MAAJ,WAktBI,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,6CAElB,OAAO,IAAI,EAAW,KAAK,WAAW,WAxtB1C,CAAI,IAAJ,aAAI,MAAJ,SA4tBI,EACA,EACA,EACA,GAEA,OAAO,IAAI,EACT,KAAK,WAAW,MACd,IAAI,YAAY,GAChB,IAAI,YAAY,GAChB,IAAI,YAAY,GAChB,IAAI,WAAW,OAtuBvB,CAAI,IAAJ,WAAI,MAAJ,SA2uBW,GACP,OAAO,IAAI,EACT,KAAK,WAAW,SAAS,IAAI,aAAa,OA7uBhD,CAAI,IAAJ,YAAI,MAAJ,SAkvBI,EACA,EACA,EACA,EACA,GAEA,KACI,aAAgB,MAChB,aAAoB,MACpB,aAAiB,MACjB,aAAgB,GAElB,MAAM,IAAI,MAAM,wCAElB,KACI,KAAK,sBAAsB,OAC3B,KAAK,sBAAsB,IAE7B,MAAM,IAAI,MAAM,iDAElB,OAAO,IAAI,EACR,KAAK,WAAmB,UACvB,EAAK,WACL,EAAS,WACT,EACA,EAAM,WACN,EAAK,gBA5wBb,EAAI,IAAJ,QAAI,MAAJ,SAGe,EAAe,EAAe,GAGnC,IAFN,IAAM,EAAO,KAAK,IAAI,KAAK,MAAM,EAAQ,GAAS,GAAQ,GACpD,EAAS,IAAI,MAAM,GAChB,EAAI,EAAG,EAAI,EAAM,IACxB,EAAO,GAAK,EAAQ,EAAI,EAE1B,OAAO,IAAI,EAAW,EAAQ,IAAI,YAAY,CAAC,SATnD,GAEU,GC3EJ,SAAU,GACd,EACA,EACA,EACA,EACA,EACA,GAA8C,IAQ1C,EAR0C,EAEjB,EAAgB,EAAO,MAAO,EAAM,GAFnB,mBAEvC,EAFuC,KAE1B,EAF0B,KAGxC,EAAS,IAAI,EAAU,OAAa,EAAW,EAAO,OAAO,OAE7D,EAAa,EAAO,gBACpB,EAAY,EAAQ,EAAY,QAGzB,IAAT,QAAsC,IAAhB,IACpB,EAAI,IAAI,MAAM,EAAO,MAAM,KAAK,IAKpC,IAFA,IAAI,EAAW,EAAO,QAClB,EAAU,EAAO,OACd,EAAI,EAAGZ,EAAI,EAAO,IAAK,IAAK,CAE/B,IADA,IAAE,EAAqB,IAAI,MAAM,EAAO,YACnC,EAAI,EAAG,EAAI,EAAO,WAAY,IAC/B,EAAG,GAAK,EAAS,IAAI,EAAI,EAAO,WAAa,GAIjD,IADA,IAAE,EAAoB,IAAI,MAAM,EAAO,WAAW,KAAK,GAClD,EAAI,EAAG,EAAI,EAAW,IAAK,CAG5B,IAFA,MAAQ,UAAO,EAAP,YAAoB,IAC5B,EAAiB,IAAI,MAAM,EAAM,QAC9B,EAAI,EAAG,EAAI,EAAM,OAAQ,IAChC,EAAe,GAAK,EAAS,EAAM,IAG/B,MAAM,EAAW,EAAgB,EAAO,cAEjC,IAAT,QAAgC,IAAV,GACL,IAAf,EAAM,GACR,EAAO,IAAI,EAAK,EAAK,EAAQ,IAAI,EAAI,EAAY,KAKnD,EAAO,IAAI,EAAK,EAAG,EAAO,IAAI,GAAM,EAAQ,IAAI,EAAI,EAAY,UAGpD,IAAV,GACF,EAAM,KAGR,EAAe,EAAS,IAI5B,QAAoB,IAAhB,QAAuC,IAAV,EAC/B,IAAK,IAAI,EAAI,EAAG,EAAI,EAAO,KAAM,IAC/B,EAAO,IAAI,EAAG,EAAY,EAAO,IAAI,GAAI,EAAM,KAInD,OAAO,EC1DH,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GACL,EACA,EACA,GACA,SAAC,EAAG,GAAJ,OAAU,KAAK,IAAI,EAAG,MACtB,YAAC,OAAI,KDqBE,CAAa,EAAQ,EAAM,GAC7B,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,OAAO,IAAI,GACR,EAAO,OAA4B,WAAW,WAC7C,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFsBA,CAAc,EAAe,EAAM,GAE5C,MAAM,IAAI,MACR,+DA1BO,CAAU,EAAQ,EAAM,GAC1B,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,IACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGhBtD,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GACL,EACA,EACA,GACA,SAAC,EAAG,GAAJ,OAAU,KAAK,IAAI,EAAG,MACtB,YAAC,OAAI,KDqBE,CAAa,EAAQ,EAAM,GAC7B,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,OAAO,IAAI,GACR,EAAO,OAA4B,WAAW,WAC7C,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFsBA,CAAc,EAAe,EAAM,GAE5C,MAAM,IAAI,MACR,+DA1BO,CAAU,EAAQ,EAAM,GAC1B,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,IACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGhBtD,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GACL,EACA,EACA,GACA,SAAC,EAAG,GAAJ,OAAU,EAAI,KACd,YAAC,OAAI,KDqBE,CAAiB,EAAQ,EAAM,GACjC,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,OAAO,IAAI,GACR,EAAO,OAA4B,WAAW,eAC7C,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFsBA,CAAkB,EAAe,EAAM,GAEhD,MAAM,IAAI,MACR,+DA1BO,CAAc,EAAQ,EAAM,GAC9B,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,QACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGhBtD,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GACL,EACA,EACA,GACA,SAAC,EAAG,GAAJ,OAAU,EAAI,SACd,GACA,SAAC,EAAG,GAAJ,OAAU,KAAK,IAAI,MDoBZ,CAAsB,EAAQ,EAAM,GACtC,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,GAAqB,YAAjB,EAAO,OAAwC,YAAjB,EAAO,MACvC,MAAM,IAAI,MACR,iEACE,EAAO,OAGb,OAAO,IAAI,GACP,EAAO,OACN,WAAmB,sBACpB,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFeA,CAAuB,EAAe,EAAM,GAErD,MAAM,IAAI,MACR,sEA1BO,CAAmB,EAAQ,EAAM,GACnC,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,aACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGhBtD,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GACL,EACA,EACA,GACA,SAAC,EAAG,GAAJ,OAAU,EAAI,KAAK,IAAI,MACvB,YAAC,OAAI,KAAK,IAAI,MACd,SAAC,EAAG,GAAJ,OAAU,KAAK,IAAI,MDoBZ,CAAyB,EAAQ,EAAM,GACzC,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,GAAqB,YAAjB,EAAO,OAAwC,YAAjB,EAAO,MACvC,MAAM,IAAI,MACR,qEACE,EAAO,OAGb,OAAO,IAAI,GACP,EAAO,OACN,WAAmB,0BACpB,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFeA,CAA0B,EAAe,EAAM,GAExD,MAAM,IAAI,MACR,0EA1BO,CAAsB,EAAQ,EAAM,GACtC,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,gBACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGhBtD,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GACL,EACA,EACA,GACA,SAAC,EAAG,GAAJ,OAAU,EAAI,SACd,GACA,SAAC,EAAG,GAAJ,OAAU,EAAI,KDoBP,CAAoB,EAAQ,EAAM,GACpC,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,OAAO,IAAI,GACR,EAAO,OAA4B,WAAW,mBAC7C,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFsBA,CAAqB,EAAe,EAAM,GAEnD,MAAM,IAAI,MACR,mEA1BO,CAAiB,EAAQ,EAAM,GACjC,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,WACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGhBtD,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GACL,EACA,EACA,GACA,SAAC,EAAG,GAAJ,OAAU,EAAI,EAAI,KAClB,YAAC,OAAI,EAAI,KACT,SAAC,EAAG,GAAJ,OAAU,EAAI,KDoBP,CAA0B,EAAQ,EAAM,GAC1C,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,OAAO,IAAI,GACR,EAAO,OAA4B,WAAW,2BAC7C,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFsBA,CAA2B,EAAe,EAAM,GAEzD,MAAM,IAAI,MACR,2EA1BO,CAAuB,EAAQ,EAAM,GACvC,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,iBACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGhBtD,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GAAmB,EAAQ,EAAM,GAAU,SAAC,EAAG,GAAJ,OAAU,EAAI,KD0BvD,CAAa,EAAQ,EAAM,GAC7B,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,OAAO,IAAI,GACR,EAAO,OAA4B,WAAW,WAC7C,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFsBA,CAAc,EAAe,EAAM,GAE5C,MAAM,IAAI,MAAM,2DAzBP,CAAU,EAAQ,EAAM,GAC1B,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,IACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGhBtD,SAAU,GACd,EACA,EACA,GAEA,QAAgD,IAA5C,EAAK,MAAK,YAAE,OAAI,EAAK,EAAO,cAC9B,OAeJ,SACE,EACA,EACA,GAEA,GAAI,EAAO,kBAAkB,EAC3B,OC/BE,SACJ,EACA,EACA,GAEA,OAAO,GACL,EACA,EACA,GACA,SAAC,EAAG,GAAJ,OAAU,EAAI,EAAI,KAClB,YAAC,OAAI,EAAI,KDqBF,CAAmB,EAAQ,EAAM,GACnC,GAAI,EAAO,kBAAkB,GAClC,OEnCE,SACJ,EACA,EACA,GAEA,OAAO,IAAI,GACR,EAAO,OAA4B,WAAW,kBAC7C,IAAI,YAAY,EAAO,YACtB,EAAO,QAAiC,WACzC,IAAI,YAAY,GAChB,QAEF,EACA,EAAO,OFsBA,CAAoB,EAAe,EAAM,GAElD,MAAM,IAAI,MACR,mEA1BO,CAAgB,EAAQ,EAAM,GAChC,MACyB,EAAgB,EAAO,MAAO,EAAM,GAD7D,mBACE,EADF,UAEL,OAAO,IAAI,GACT,EAAO,OAAO,UACZ,EAAK,KAAI,YAAE,OAAI,EAAK,EAAO,WAAa,KACxC,GAEF,EAAO,QAAQ,OACf,EACA,EAAW,EAAO,UAAY,EAAO,UAAY,EAAK,QGpBtD,SAAU,GACd,EACA,EACA,EACA,GAkBA,IAhBE,IAAI,EAAI,EAAE,WAEN,EAAoB,EAAY,MAAM,EAAG,GACzC,EAAmB,EAAY,MAAM,GACrC,EAAY,EAAQ,EAAkB,GAEtC,EAAQ,EAAE,OACV,EAAW,EAAE,QAEb,EAAS,IAAI,EAAJ,CACZ,EAAE,KADU,mBACF,SACX,EACA,EAAE,OAEE,EAAU,EAAE,QAAQ,OAEjB,EAAI,EAAG,EAAI,EAAE,IAAK,IAAK,CAE9B,IADA,IAAM,EAAqB,IAAI,MAAM,EAAkB,QAC9C,EAAI,EAAG,EAAI,EAAkB,OAAQ,IAC5C,EAAS,GAAK,EAAS,IAAI,EAAI,EAAI,GAIrC,IADA,IAAM,EAAoB,IAAI,MAAM,EAAiB,QAAQ,KAAK,GACzD,EAAI,EAAG,EAAI,EAAW,IAAK,CAClC,IAAM,EAAK,EAAM,IAAN,CAAW,GAAX,mBAAiB,KACtB,EAAK,EAAE,IAAF,UAAU,EAAV,YAAuB,KAElC,EAAO,IAAI,EAAI,EAAY,EAAG,EAAG,EAAI,IAErC,EAAe,EAAS,IAI1B,OAAK,IAAI,GAAa,EAAQ,EAAS,EAAa,EAAE,WAGpD,SAAU,GACd,EACA,EACA,EACA,GAiBA,IAfE,IAAI,EAAI,EAAE,WAEN,EAAoB,EAAY,MAAM,EAAG,GACzC,EAAmB,EAAY,MAAM,GACrC,EAAY,EAAQ,EAAkB,GAEtC,EAAQ,EAAE,OACV,EAAW,EAAE,QAEb,EAAQ,EAAE,OACV,EAAW,EAAE,QAEb,EAAgB,EAAe,GAE/B,EAA+C,GAC5C,EAAI,EAAG,EAAI,EAAE,IAAK,IAAK,CAE9B,IADA,IAAI,EAAM,EACD,EAAI,EAAG,EAAI,EAAG,IACrB,GAAO,EAAS,IAAI,EAAI,EAAI,GAAK,EAAc,GAEjD,EAAe,GAAO,EAUxB,IAPA,IAAM,EAAS,IAAI,EAAJ,CACZ,EAAE,KADU,mBACF,SACX,EACA,EAAE,OAEE,EAAU,EAAE,QAAQ,OAEjB,EAAI,EAAG,EAAI,EAAE,IAAK,IAAK,CAE9B,IADA,IAAI,EAAM,EACD,EAAI,EAAG,EAAI,EAAG,IACrB,GAAO,EAAS,IAAI,EAAI,EAAI,GAAK,EAAc,GAKjD,IAHA,IAAM,EAAK,EAAe,GAEpB,EAAoB,IAAI,MAAM,EAAiB,QAAQ,KAAK,GACzD,EAAI,EAAG,EAAI,EAAW,IAAK,CAClC,IAAM,EAAK,EAAM,IAAN,CAAW,GAAX,mBAAiB,KACtB,EAAK,EAAM,IAAN,CAAW,GAAX,mBAAkB,KAE7B,EAAO,IAAI,EAAI,EAAY,EAAG,EAAG,EAAI,IAErC,EAAe,EAAS,IAI5B,OAAO,IAAI,GAAa,EAAQ,EAAS,EAAa,EAAE,WC9FpD,SAAU,GACd,EACA,EACA,EACA,EACA,GAEE,OAAE,aAAa,GAOnB,SACE,EACA,EACA,EACA,EACA,GAEA,GAAI,EAAE,MAAQ,EAAE,IACd,MAAM,IAAI,MACR,mIAEG,GAAI,EAAE,YAAc,EAAE,UACvB,MAAE,IAAI,MACR,2GAGJ,GAAI,EAAE,kBAAkB,EACtB,OCvBE,SACJ,EACA,EACA,EACA,EACA,GAEA,OAAO,GAAgB,EAAG,EAAG,GAAa,SAAC,EAAG,GAAJ,OAAU,EAAQ,EAAI,EAAO,KDgB9D,CAAa,EAAG,EAAG,EAAa,EAAO,GACzC,GAAI,EAAE,kBAAkB,GAC7B,OEfE,SACJ,EACA,EACA,EACA,EACA,GAEA,IAAM,EAAO,IAAI,GACd,EAAE,OAA4B,WAAW,kBACvC,EAAE,QAAiC,WACnC,EAAE,QAAiC,WACnC,EAAE,OAA4B,WAC/B,IAAI,YAAY,GAChB,EACA,QAEF,EACA,EAAE,OAGJ,OAAO,IAAI,GAAa,EAAM,EAAE,QAAQ,OAAQ,EAAa,EAAE,WFLtD,CAAc,EAAU,EAAU,EAAa,EAAO,GAE/D,MAAM,IAAI,MACR,gEA5BO,CAAU,EAAG,EAAG,EAAa,EAAO,GAgC/C,SACE,EACA,EACA,EACA,EACA,GAEA,GAAI,aAAa,EACf,OClDE,SACJ,EACA,EACA,EACA,EACA,GAEA,OAAO,GAAe,EAAG,EAAG,GAAa,SAAC,EAAG,GAAJ,OAAU,EAAQ,EAAI,EAAO,KD2C7D,CAAY,EAAG,EAAG,EAAa,EAAO,GACxC,GAAI,aAAa,GACtB,OEtDE,SACJ,EACA,EACA,EACA,EACA,GAEA,IAAM,EAAO,IAAI,GACd,EAAE,OAA4B,WAAW,iBACvC,EAAE,QAAiC,WACpC,EAAE,WACF,IAAI,YAAY,GAChB,EACA,QAEF,EACA,EAAE,OAGJ,OAAO,IAAI,GAAa,EAAM,EAAE,QAAQ,OAAQ,EAAa,EAAE,WFmCtD,CAAa,EAAG,EAAG,EAAa,EAAO,GAEhD,MAAM,IAAI,MACR,+DA3CO,CAAS,EAAG,EAAG,EAAa,EAAO,GGVxC,SAAU,GACd,EACA,EACA,EACA,GAEA,OAAI,aAAa,GAOnB,SACE,EACA,EACA,EACA,GAEE,GAAE,EAAE,MAAQ,EAAE,IACd,MAAM,IAAI,MACR,gJAEG,GAAI,EAAE,YAAc,EAAE,UAC3B,MAAM,IAAI,MACR,wHAGF,GAAE,EAAE,kBAAkB,EACtB,OCtBE,SACJ,EACA,EACA,EACA,GAEA,OAAO,GAAgB,EAAG,EAAG,GAAa,SAAC,EAAG,GAAJ,OAAW,EAAQ,EAAK,KDgBzD,CAAgB,EAAG,EAAG,EAAa,GACrC,GAAI,EAAE,kBAAkB,GAC7B,OEfE,SACJ,EACA,EACA,EACA,GAEA,IAAM,EAAO,IAAI,GACd,EAAE,OAA4B,WAAW,qBACvC,EAAE,QAAiC,WACnC,EAAE,QAAiC,WACnC,EAAE,OAA4B,WAC/B,IAAI,YAAY,GAChB,QAEF,EACA,EAAE,OAGJ,OAAO,IAAI,GAAa,EAAM,EAAE,QAAQ,OAAQ,EAAa,EAAE,WFHtD,CAAiB,EAAU,EAAU,EAAa,GAE3D,MAAM,IAAI,MACR,gEA3BO,CAAa,EAAG,EAAG,EAAa,GA+B3C,SACE,EACA,EACA,EACA,GAEA,GAAI,aAAa,EACf,OC/CE,SACJ,EACA,EACA,EACA,GAEA,OAAO,GAAe,EAAG,EAAG,GAAa,SAAC,EAAG,GAAJ,OAAW,EAAQ,EAAK,KDyCxD,CAAe,EAAG,EAAG,EAAa,GACpC,GAAI,aAAa,GACtB,OEnDE,SACJ,EACA,EACA,EACA,GAEA,IAAM,EAAO,IAAI,GACd,EAAE,OAA4B,WAAW,oBACvC,EAAE,QAAiC,WACpC,EAAE,WACF,IAAI,YAAY,GAChB,QAEF,EACA,EAAE,OAGJ,OAAO,IAAI,GAAa,EAAM,EAAE,QAAQ,OAAQ,EAAa,EAAE,WFkCtD,CAAgB,EAAG,EAAG,EAAa,GAE5C,MAAM,IAAI,MACR,4EAzCO,CAAY,EAAG,EAAG,EAAa,GGTpC,SAAU,GACd,EACA,EACA,EACA,GAEA,OAAI,aAAa,GAOnB,SACE,EACA,EACA,EACA,GAEE,GAAE,EAAE,MAAQ,EAAE,IACd,MAAM,IAAI,MACR,sJAEG,GAAI,EAAE,YAAc,EAAE,UAC3B,MAAM,IAAI,MACR,8HAGF,GAAE,EAAE,kBAAkB,EACtB,OCtBE,SACJ,EACA,EACA,EACA,GAEA,OAAO,GAAgB,EAAG,EAAG,GAAa,SAAC,EAAG,GAAJ,OAAU,EAAQ,EAAI,KDgBvD,CAAkB,EAAG,EAAG,EAAa,GACvC,GAAI,EAAE,kBAAkB,GAC7B,OEfE,SACJ,EACA,EACA,EACA,GAEA,IAAM,EAAO,IAAI,GACd,EAAE,OAA4B,WAAW,uBACvC,EAAE,QAAiC,WACnC,EAAE,QAAiC,WACnC,EAAE,OAA4B,WAC/B,IAAI,YAAY,GAChB,QAEF,EACA,EAAE,OAGJ,OAAO,IAAI,GAAa,EAAM,EAAE,QAAQ,OAAQ,EAAa,EAAE,WFHtD,CAAmB,EAAU,EAAU,EAAa,GAE7D,MAAM,IAAI,MACR,gEA3BO,CAAe,EAAG,EAAG,EAAa,GA+B7C,SACE,EACA,EACA,EACA,GAEA,GAAI,aAAa,EACf,OC/CE,SACJ,EACA,EACA,EACA,GAEA,OAAO,GAAe,EAAG,EAAG,GAAa,SAAC,EAAG,GAAJ,OAAU,EAAQ,EAAI,KDyCtD,CAAiB,EAAG,EAAG,EAAa,GACtC,GAAI,aAAa,GACtB,OEnDE,SACJ,EACA,EACA,EACA,GAEA,IAAM,EAAO,IAAI,GACd,EAAE,OAA4B,WAAW,sBACvC,EAAE,QAAiC,WACpC,EAAE,WACF,IAAI,YAAY,GAChB,QAEF,EACA,EAAE,OAGJ,OAAO,IAAI,GAAa,EAAM,EAAE,QAAQ,OAAQ,EAAa,EAAE,WFkCtD,CAAkB,EAAG,EAAG,EAAa,GAE9C,MAAM,IAAI,MACR,uFAzCO,CAAc,EAAG,EAAG,EAAa,GGTtC,SAAU,GACd,EACA,EACA,EACA,EACA,GAEE,OAAE,aAAa,GAOnB,SACE,EACA,EACA,EACA,EACA,GAEA,GAAI,EAAE,MAAQ,EAAE,IACd,MAAM,IAAI,MACR,sIAEG,GAAI,EAAE,YAAc,EAAE,UACvB,MAAE,IAAI,MACR,8GAGJ,GAAI,EAAE,kBAAkB,EACtB,OCvBE,SACJ,EACA,EACA,EACA,EACA,GAEA,OAAO,GAAgB,EAAG,EAAG,GAAa,SAAC,EAAG,GAAJ,OAAU,EAAQ,EAAI,EAAO,KDgB9D,CAAkB,EAAG,EAAG,EAAa,EAAO,GAC9C,GAAI,EAAE,kBAAkB,GAC7B,OEfE,SACJ,EACA,EACA,EACA,EACA,GAEA,IAAM,EAAO,IAAI,GACd,EAAE,OAA4B,WAAW,uBACvC,EAAE,QAAiC,WACnC,EAAE,QAAiC,WACnC,EAAE,OAA4B,WAC/B,IAAI,YAAY,GAChB,EACA,QAEF,EACA,EAAE,OAGJ,OAAO,IAAI,GAAa,EAAM,EAAE,QAAQ,OAAQ,EAAa,EAAE,WFLtD,CACL,EACA,EACA,EACA,EACA,GAGJ,MAAM,IAAI,MACR,mEAlCO,CAAe,EAAG,EAAG,EAAa,EAAO,GAsCpD,SACE,EACA,EACA,EACA,EACA,GAEA,GAAI,aAAa,EACf,OCxDE,SACJ,EACA,EACA,EACA,EACA,GAEA,OAAO,GAAe,EAAG,EAAG,GAAa,SAAC,EAAG,GAAJ,OAAU,EAAQ,EAAI,EAAO,KDiD7D,CAAiB,EAAG,EAAG,EAAa,EAAO,GAC7C,GAAI,aAAa,GACtB,OE5DE,SACJ,EACA,EACA,EACA,EACA,GAEA,IAAM,EAAO,IAAI,GACd,EAAE,OAA4B,WAAW,sBACvC,EAAE,QAAiC,WACpC,EAAE,WACF,IAAI,YAAY,GAChB,EACA,QAEF,EACA,EAAE,OAGJ,OAAO,IAAI,GAAa,EAAM,EAAE,QAAQ,OAAQ,EAAa,EAAE,WFyCtD,CAAkB,EAAG,EAAG,EAAa,EAAO,GAErD,MAAM,IAAI,MACR,oEAjDO,CAAc,EAAG,EAAG,EAAa,EAAO,GvCiuB1C,gBAAe,CACpB,SAAU,EACV,QAAS,EACT,KAAM,G0CttBV,IAAa,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GAVlC,uBAAE,IAAJ,eAAI,MAAJ,WAcU,MAAN,gBACE,KAAK,eAAe,QADtB,2BAEE,KAAK,eAAe,SAFtB,uBAdJ,CAAI,IAAJ,kBAAI,MAAJ,WAqBI,MAAO,CAAC,CAAC,KAAM,QAAS,CAAC,KAAM,YArBnC,CAAI,IAAJ,oBAAI,MAAJ,SAyBoB,GAChB,8CAC0B,KAAK,QAD/B,yJAWE,KAAK,iBAXP,YA1BJ,CAAI,IAAJ,kBAAI,MAAJ,WA0CI,MAAO,CAAC,OA1CZ,CAAI,IAAJ,OAAI,MAAJ,SA6CO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAGlD,IAAM,EAAc,KAAK,eAAe,GAElC,EAAO,KAAK,mBAAmB,GAErC,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CACE,KAAM,EAAK,KACX,MAAO,EAAK,UA3DpB,CAAI,IAAJ,iBAAI,MAAJ,SAgEiB,GACb,mBAAW,EAAM,EAAE,SAjEvB,CAAI,IAAJ,UAAI,MAAJ,SAoEU,GACN,+DAAc,KArElB,CAAI,IAAJ,qBAAI,MAAJ,SAwEqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,KAAM,EAAM,KACZ,MAAO,EAAM,SAzFnB,CAAI,IAAJ,qBAAI,MAAJ,SA6FqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,KAAjC,YAAyC,EAAM,WA9FnD,GAAqE,IAkGxD,GAAmB,IAAI,IAClC,SAAC,GAAD,OAAqB,IAAI,GAAkB,GAAgB,MCxHvD,SAAU,GACd,EACA,EACA,GAEE,IAAG,EAAc,EAAE,MAAO,EAAE,QAAUa,EAAE,aAAe,EAAE,WACrD,MAAE,IAAI,MACR,yFAGF,GAAE,EAAO,EAAE,WACP,MAAE,IAAI,MACR,qEAGF,IAAM,EAAS,EAAE,OAAO,OAAO,EAAE,OAAQ,GACnC,EAWV,SACE,EACA,EACA,GAEA,OAAI,aAAmB,ECxCnB,SACJ,EACA,EACA,GAGA,IADA,IAAM,EAAS,EAAQ,OACd,EAAI,EAAM,EAAI,EAAO,KAAM,GAAK,EAAQ,MAAM,GACrD,EAAO,IAAI,EAAG,EAAO,IAAI,GAAK,GAEhC,OAAO,EDgCE,CAAY,EAAS,EAAM,GACzB,aAAmB,GE1C1B,SACJ,EACA,EACA,GAEA,OAAO,IAAI,GAAW,EAAQ,WAAW,UAAU,EAAM,IFsChD,CAAa,EAAS,EAAM,GDwFjC,SACJ,EACA,EACA,GAEA,OAAO,GAAiB,KACtB,CACE,EAAG,EACH,OACA,SAEF,UCjGO,CAAY,EAAgC,EAAM,GArBtC,CAAS,EAAE,QAAS,EAAM,EAAE,MAAM,IAC/C,EAAU,EAAE,QAAQ,OAAO,EAAY,GAC7C,EAAW,SAEX,IAAM,EAAW,YAAO,EAAE,OAG1B,OAFA,EAAY,IAAS,EAAE,MAAM,GAEtB,IAAI,GAAa,EAAQ,EAAS,EAAa,EAAE,WG1BtD,SAAU,GACd,EACA,GAEE,GAAE,aAAa,GACf,MAAM,IAAI,MAAM,2DAEhB,OAIJ,SACE,EACA,GAEE,GAAkB,IAAhB,EAAE,UACJ,OAAO,IAAI,GACT,EAAE,OAAO,OAAO,GAChB,EAAE,QAAQ,OACV,CAAC,EAAE,MAAM,GAAI,EAAE,WAAW,IAC1B,GAIJ,GAAI,aAAa,EACf,OCtBE,SACJ,EACA,GASE,IAPA,IAAI,EAAI,EAAE,MAAM,GACZ,EAAI,EAAE,MAAM,GACZ,EAAS,IAAI,EAAU,CAAC,EAAG,QAAI,EAAW,EAAE,OAE5C,EAAU,EAAE,QACZ,EAAS,EAAE,OAER,EAAI,EAAG,EAAI,EAAE,IAAK,IAMzB,IALA,IAAM,EAAI,EAAQ,IAAQ,EAAJ,GAChB,EAAI,EAAQ,IAAQ,EAAJ,EAAQ,GAExB,EAAI,EAAO,IAAI,GAEZ,EAAI,EAAG,EAAI,EAAG,IACrB,EAAO,IAAI,EAAI,EAAI,EAAG,EAAO,IAAI,EAAI,EAAI,GAAK,EAAI,EAAE,IAAI,EAAI,EAAI,IAIpE,OAAO,EDAE,CAAqB,EAAG,GAC1B,GAAI,aAAa,GACtB,OEzBE,SACJ,EACA,GAEA,OAAO,IAAI,GACR,EAAE,OAA4B,WAAW,oBACvC,EAAE,QAAiC,WACpC,EAAE,WACF,EAAE,MAAM,SAEV,EACA,EAAE,OFcK,CAAsB,EAAG,GAElC,MAAM,IAAI,MACR,iEAvBO,CAAkB,EAAG,GGkBhC,IAAa,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARlC,uBAAE,IAAJ,eAAI,MAAJ,WAYU,MAAN,gBACE,KAAK,eAAe,eADtB,4BACwD,KAAK,QAD7D,mBAEE,KAAK,eAAe,iBAFtB,8BAE4D,KAAK,QAFjE,cAZJ,CAAI,IAAJ,kBAAI,MAAJ,WAmBI,MAAO,CACL,CAAC,KAAM,cAAe,OAAQ,KAAK,SACnC,CAAC,KAAM,gBAAiB,OAAQ,KAAK,YArB3C,CAAI,IAAJ,oBAAI,MAAJ,SA0BoB,GACV,MAAN,wCAC0B,KAAK,QAD/B,gLAQc,KAAK,QARnB,qBASI,KAAK,UAAU,SATnB,wFAaiB,KAAK,QAbtB,qBAcI,KAAK,UAAU,YAdnB,mBAeI,KAAK,WAAW,gBAAiB,WAAY,aAfjD,uEAkBwB,KAAK,QAlB7B,gLA4BE,KAAK,iBA5BP,YA3BJ,CAAI,IAAJ,kBAAI,MAAJ,WA4DI,MAAO,CAAC,OA5DZ,CAAI,IAAJ,OAAI,MAAJ,SA+DO,GACH,GAAI,KAAK,kBAAoC,IAArB,KAAK,YAC3B,OAAO,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAGlD,IAAM,EAAc,KAAK,eAAe,GAElC,EAAO,KAAK,mBAAmB,GAErC,OAAO,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CACE,YAAa,KAAK,IAAI,EAAK,aAC3B,cAAe,KAAK,IAAI,EAAK,mBA7ErC,CAAI,IAAJ,iBAAI,MAAJ,SAkFiB,GACb,MAAO,CAAC,EAAM,EAAE,MAAM,GAAK,EAAM,YAAa,EAAM,EAAE,MAAM,MAnFhE,CAAI,IAAJ,UAAI,MAAJ,SAsFU,QACmB,IAArB,EAAK,cACP,KAAK,QAAU,EAAK,YAAY,aAEP,IAAvB,EAAK,gBACP,KAAK,QAAU,EAAK,cAAc,QAGpC,+DAAc,KA9FlB,CAAI,IAAJ,qBAAI,MAAJ,SAiGqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,YAAa,EAAM,MACnB,cAAe,EAAe,EAAM,YAlH1C,CAAI,IAAJ,qBAAI,MAAJ,SAsHqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,QAAjC,YAA4C,EAAM,WAvHtD,GAEU,IAyHG,GAAsB,IAAI,IACrC,SAAC,GAAD,OAAqB,IAAI,GAAqB,GAAgB,MCnJ1D,SAAU,GACd,EACA,GAEE,IAME,EANE,EAAgB,EAAQ,MAAM,EAAG,EAAO,YACxC,EAAe,EAAQ,MAAM,EAAO,YAEpC,EAAoB,EAAc,QAAO,SAAC,EAAG,GAAJ,OAAU,EAAI,IAAG,GAE1D,EAAS,EAAO,OAAO,OAAd,CAAsB,GAAtB,mBAA4C,KAGzD,EADE,EAAoB,EAgB1B,SACE,EACA,EACA,EACA,GAEA,OAAI,aAAmB,ECvCnB,SACJ,EACA,EACA,EACA,GASE,IAPA,IAAI,EAAM,EAAQ,MAAM,GACpB,EAAS,EAAM,EACf,EAAI,EAAQ,MAAM,GAClB,EAAS,IAAI,EAAU,CAAC,EAAM,EAAa,QAAI,EAAW,UAE1D,EAAgB,EAAe,GAE5B,EAAI,EAAG,EAAI,EAAQ,IAO1B,IANA,IAAM,EAAO,EAAI,EAIX,EAFW,EADC,KAAK,MAAM,EAAI,GACM,GAEnB,KAAI,SAAC,EAAG,GAAJ,OAAU,EAAI,EAAM,MAEnC,EAAI,EAAG,EAAI,EAAG,IACrB,EAAO,IAAI,EAAI,EAAI,EAAG,EAAG,GAAK,EAAQ,IAAI,EAAO,EAAI,IAIzD,OAAO,EDeE,CAAiB,EAAS,EAAS,EAAO,GACxC,aAAmB,GE1C1B,SACJ,EACA,EACA,EACA,GAEA,OAAO,IAAI,GACT,EAAQ,WAAW,sBACjB,IAAI,YAAY,GAChB,IAAI,YAAY,GAChB,IFiCK,CAAkB,EAAS,EAAS,EAAO,GDkHhD,SACJ,EACA,EACA,EACA,GAEA,OAAO,GAAoB,KACzB,CACE,EAAG,EACH,UACA,QACA,eAEF,UC7HO,CACL,EACA,EACA,EACA,GA9BQ,CACR,EAAO,QACP,EACA,EAAO,iBACP,GAGQ,EAAO,QAAQ,OAG3B,IAAM,EAAW,EAAO,MAAM,KAAI,SAAC,EAAG,GAAJ,OAAU,EAAI,EAAQ,MAExD,OAAO,IAAI,GAAa,EAAQ,EAAS,EAAU,EAAO,WGE5D,IAAa,GAAb,kDAGI,SAAF,EACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,2DAYI,sBACE,KAAK,eAAe,eADtB,kCAEE,KAAK,eAAe,cAFtB,iCAGE,KAAK,eAAe,oBAHtB,iCAIE,KAAK,QAJP,mBAME,KAAK,eAAe,oBANtB,iCAOE,KAAK,QAPP,cAZJ,wCAyBI,MAAO,CACL,CAAC,KAAM,eACP,CAAC,KAAM,cACP,CAAC,KAAM,mBAAoB,OAAQ,KAAK,SACxC,CAAC,KAAM,mBAAoB,OAAQ,KAAK,YA7B9C,wCAkCoB,GAChB,8CAC0B,KAAK,QAD/B,0MAQoB,KAAK,QARzB,qBASI,KAAK,UAAU,eATnB,uCAUwB,KAAK,QAV7B,yVAmBoB,KAAK,QAnBzB,qBAoBI,KAAK,UAAU,eApBnB,mBAqBI,KAAK,WAAW,mBAAoB,cAAe,gBArBvD,0EAyBwB,KAAK,QAzB7B,6JAmCE,KAAK,iBAnCP,YAnCJ,wCA2EU,MAAC,CAAC,OA3EZ,2BA8EO,GACG,GAAF,KAAK,kBAAoC,IAArB,KAAK,YACnB,OAAD,KAAK,QAAQ,KAAK,YAAa,CAAC,EAAG,EAAM,IAGlD,IAAM,EAAc,KAAK,eAAe,GAElC,EAAO,KAAK,mBAAmB,GAE/B,OAAC,KAAK,QACV,EACA,CAAC,EAAG,EAAM,GACV,CACE,YAAa,EAAK,YAClB,WAAY,EAAK,WACjB,iBAAkB,KAAK,IAAI,EAAK,kBAChC,iBAAkB,KAAK,IAAI,EAAK,sBA9FxC,qCAmGiB,GAKb,IAJA,IAAM,EAAgB,EAAQ,EAAM,aAE9B,EAAc,GAChB,EAAa,EACR,EAAI,EAAG,EAAI,EAAM,MAAM,QAC1B,EAAa,EADqB,IAEpC,GAAc,EAAM,MAAM,GAC1B,EAAY,KAAK,EAAM,MAAM,IAMjC,IAAM,EAAc,EAAa,EAGjC,MAAO,CAFK,EAAM,IAAM,EAEX,EAAY,UApH7B,8BAuHU,GACN,+DAAc,KAxHlB,yCA2HqB,GAYjB,IAXA,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGD,EAAgB,EAAQ,EAAM,aAE9B,EAAc,GACd,EAAa,GACf,EAAa,EACR,EAAI,EAAG,EAAI,EAAM,MAAM,OAAQ,IAClC,EAAa,GACf,GAAc,EAAM,MAAM,GAC1B,EAAY,KAAK,EAAM,MAAM,KAE7B,EAAW,KAAK,EAAM,MAAM,IAIhC,IAAM,EAAmB,EAAe,EAAM,aACxC,EAAmB,EAAe,GAElC,EAAc,EAAa,EAEjC,MAAO,CACL,OAAQ,EAAM,EAAE,MAChB,OAAQ,EAAM,EAAE,OAAO,MACvB,QAAS,EAAM,EAAE,OAAO,OAExB,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,WAAY,EAAM,YAAY,OAC9B,mBACA,mBACA,iBAjKN,yCAqKqB,GACjB,gBAAU,EAAM,EAAE,MAAlB,YAA2B,EAAM,IAAjC,YAAwC,EAAM,MAA9C,YAAuD,EAAM,iBAtKjE,GAEU,IAwKG,GAAyB,IAAI,IACxC,SAAC,GAAD,OAAqB,IAAI,GAAwB,GAAgB,MCrM7D,SAAU,GACd,EACA,EACA,GAEE,OAAE,EAAO,kBAAkB,ECHzB,SACJ,EACA,EACA,EACA,EACA,GAOE,IALA,IAAI,EAAgB,EAAQ,EAAO,kBAE/B,EAAc,GACd,EAAa,GACf,EAAa,EACR,EAAI,EAAG,EAAI,EAAM,OAAQ,IAC5B,EAAa,GACf,GAAc,EAAM,GACpB,EAAY,KAAK,EAAM,KAEvB,EAAW,KAAK,EAAM,IAIxB,IAQE,EARE,EAAmB,EAAe,EAAO,kBACzC,EAAmB,EAAe,GAElC,EAAc,EAAa,EAC3B,EAAM,EAAO,IAAM,EAEnB,EAAY,EAAO,QAAP,CAAgB,GAAhB,OAAwB,GAAa,GAGrD,IAAG,GAAQ,EAAc,EAAa,EAAO,kBAC7C,EAAa,MACR,CACL,EAAa,IAAI,EAAU,CAAC,EAAK,EAAY,aAAS,EAAW,UACjE,IAAK,IAAI,EAAI,EAAG,EAAI,EAAK,IAAK,CAI5B,IAHA,IAAM,EAAW,KAAK,MAAM,EAAI,GAE1B,EAAc,GACX,EAAI,EAAG,EAAI,EAAO,WAAY,IACrC,EAAY,KAAK,EAAQ,IAAI,EAAW,EAAO,WAAa,IAK9D,IAHA,IAEM,EAAc,EAFC,EAAW,EAAa,GACT,EAAe,EAAI,EACV,GACpC,EAAI,EAAG,EAAI,EAAY,OAAQ,IACtC,EAAW,IAAI,EAAI,EAAY,OAAS,EAAG,EAAY,KAK7D,OAAO,IAAI,GAAa,EAAW,EAAY,EAAO,EAAW,QD9CxD,CACL,EACA,EAAO,OACP,EAAO,QACP,EACA,GAEO,EAAO,kBAAkB,GElBhC,SACJ,EACA,EACA,EACA,EACA,GAOE,IALA,IAAI,EAAgB,EAAQ,EAAO,kBAE/B,EAAc,GACd,EAAa,GACf,EAAa,EACR,EAAI,EAAG,EAAI,EAAM,OAAQ,IAC5B,EAAa,GACf,GAAc,EAAM,GACpB,EAAY,KAAK,EAAM,KAEvB,EAAW,KAAK,EAAM,IAIxB,IAIE,EAJE,EAAc,EAAa,EAC3B,EAAM,EAAO,IAAM,EAEnB,EAAY,EAAO,QAAP,CAAgB,GAAhB,OAAwB,GAAa,GAevD,OAZE,GADG,GAAQ,EAAc,EAAa,EAAO,kBAChC,EAEA,IAAI,GACf,EAAQ,WAAW,uBACjB,IAAI,YAAY,EAAO,kBACvB,IAAI,YAAY,SAElB,EACA,UAIG,IAAI,GAAa,EAAW,EAAY,EAAO,EAAW,QFpBxD,CACL,EACA,EAAO,OACP,EAAO,QACP,EACA,GDqLA,SACJ,EACA,EACA,EACA,EACA,GAOA,IALA,IAAM,EAAgB,EAAQ,EAAO,kBAE/B,EAAc,GACd,EAAa,GACf,EAAa,EACR,EAAI,EAAG,EAAI,EAAM,OAAQ,IAC5B,EAAa,GACf,GAAc,EAAM,GACpB,EAAY,KAAK,EAAM,KAEvB,EAAW,KAAK,EAAM,IAI1B,IAII,EAJE,EAAc,EAAa,EAC3B,EAAM,EAAO,IAAM,EAEnB,EAAY,EAAO,QAAP,CAAgB,GAAhB,OAAwB,GAAa,GAgBvD,OAbE,GADG,GAAQ,EAAc,EAAa,EAAO,kBAChC,EAEA,GAAuB,KAClC,CACE,EAAG,EACH,YAAa,EAAO,iBACpB,MAAO,EACP,IAAK,EAAO,KAEd,UAIG,IAAI,GAAa,EAAW,EAAY,EAAO,EAAW,QC1NxD,CACL,EACA,EAAO,OACP,EAAO,QACP,EACA,G,2SGJO,GAAb,YAAE,qBAAF,iBAqGI,SAAF,EAIS,EASE,EAKA,GAKW,MAAb,EAAa,uDAAD,EAAC,4BAEpB,cAAM,EAAO,QArBN,SASI,EAAJ,UAKI,EAAJ,QAKA,cAIP,EAAK,KAAO,EAAQ,GAChB,EAAC,QAAU,EAAe,GAC9B,EAAK,IAAM,EAAK,QAAQ,WAAW,GACnC,EAAK,WAAa,EAAK,MAAM,OAAS,EAAK,UAPvB,EA5HtB,uBAAE,IAAJ,YAAI,MAAJ,W,6IAuIiB,O,SAAM,KAAK,OAAO,Y,OACf,OADV,E,gBACgB,KAAK,QAAQ,Y,OAOnC,IAPM,E,OAEA,EAAY,EAAQ,KAAK,gBAAiB,GAE1C,EAAgB,EAAe,KAAK,kBAEpC,EAAS,IAAI,EAAwB,KAAK,OAAO,OAAO,KAAK,MAC1D,EAAI,EAAG,EAAI,KAAK,IAAK,IAAK,CAEjC,IADM,EAAW,GACR,EAAI,EAAG,EAAI,KAAK,WAAY,IACnC,EAAS,KAAK,EAAQ,EAAI,KAAK,WAAa,IAI9C,IAFM,EAAY,EAAW,EAAU,GAE9B,EAAI,EAAG,EAAI,EAAW,IAC7B,EAAO,EAAY,EAAY,GAAK,EAAK,EAAI,EAAY,G,yBAItD,G,mDA3JX,CAAI,IAAJ,iBAAI,MAAJ,WAmKI,OAAO,KAAK,MAAM,MAAM,EAAG,KAAK,MAAM,OAAS,KAAK,aAnKxD,CAAI,IAAJ,gBAAI,MAAJ,WA2KI,OAAO,KAAK,MAAM,MAAM,KAAK,MAAM,OAAS,KAAK,aA3KrD,CAAI,IAAJ,WAAI,MAAJ,WA+KI,OAAO,KAAK,QA/KhB,CAAI,IAAJ,eAAI,MAAJ,SAwLe,GACX,OAAO,IAAI,EACT,KAAK,OAAO,aAAa,GACzB,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA7LX,CAAI,IAAJ,iBAAI,MAAJ,SAoMiB,GACb,MAAM,IAAI,MAAM,6BArMpB,CAAI,IAAJ,OAAI,MAAJ,SAwM4B,GACxB,OAAO,IAAI,EACT,KAAK,OAAO,KAAK,GACjB,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA7MX,CAAI,IAAJ,SAAI,MAAJ,WAkNI,KAAK,OAAO,SACZ,KAAK,QAAQ,WAnNjB,CAAI,IAAJ,eAAI,MAAJ,SAuNI,EACA,GAEA,OAAO,GAAQ,KAAM,EAAO,KA1NhC,CAAI,IAAJ,MAAI,MAAJ,WA8NI,OAAO,IAAI,EACT,KAAK,OAAO,MACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAlOX,CAAI,IAAJ,MAAI,MAAJ,WAuOI,OAAO,IAAI,EACT,KAAK,OAAO,MACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA3OX,CAAI,IAAJ,OAAI,MAAJ,WAgPI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aApPX,CAAI,IAAJ,MAAI,MAAJ,WAyPI,OAAO,IAAI,EACT,KAAK,OAAO,MACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA7PX,CAAI,IAAJ,MAAI,MAAJ,WAkQI,OAAO,IAAI,EACT,KAAK,OAAO,MACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAtQX,CAAI,IAAJ,MAAI,MAAJ,WA2QI,OAAO,IAAI,EACT,KAAK,OAAO,MACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA/QX,CAAI,IAAJ,MAAI,MAAJ,WAoRI,OAAO,IAAI,EACT,KAAK,OAAO,MACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAxRX,CAAI,IAAJ,OAAI,MAAJ,WA6RI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAjSX,CAAI,IAAJ,OAAI,MAAJ,WAsSI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA1SX,CAAI,IAAJ,OAAI,MAAJ,WA+SI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAnTX,CAAI,IAAJ,OAAI,MAAJ,WAwTI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA5TX,CAAI,IAAJ,OAAI,MAAJ,WAiUI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aArUX,CAAI,IAAJ,OAAI,MAAJ,WA0UI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA9UX,CAAI,IAAJ,QAAI,MAAJ,WAmVI,OAAO,IAAI,EACT,KAAK,OAAO,QACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAvVX,CAAI,IAAJ,QAAI,MAAJ,WA4VI,OAAO,IAAI,EACT,KAAK,OAAO,QACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAhWX,CAAI,IAAJ,QAAI,MAAJ,WAqWI,OAAO,IAAI,EACT,KAAK,OAAO,QACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAzWX,CAAI,IAAJ,SAAI,MAAJ,WA8WI,OAAO,IAAI,EACT,KAAK,OAAO,SACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAlXX,CAAI,IAAJ,cAAI,MAAJ,SAsXc,EAAe,GACzB,OAAO,IAAI,EACT,KAAK,OAAO,YAAY,EAAO,GAC/B,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA3XX,CAAI,IAAJ,UAAI,MAAJ,WAgYI,OAAO,IAAI,EACT,KAAK,OAAO,UACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aApYX,CAAI,IAAJ,cAAI,MAAJ,SAwYc,EAAe,GACzB,OAAO,IAAI,EACT,KAAK,OAAO,YAAY,EAAO,GAC/B,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA7YX,CAAI,IAAJ,OAAI,MAAJ,WAkZI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAtZX,CAAI,IAAJ,oBAAI,MAAJ,SA0ZoB,EAAgB,GAChC,OAAO,IAAI,EACT,KAAK,OAAO,kBAAkB,EAAQ,GACtC,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA/ZX,CAAI,IAAJ,SAAI,MAAJ,SAobS,GACL,OAAO,GAAO,KAAM,KArbxB,CAAI,IAAJ,SAAI,MAAJ,SA+bS,EAAsB,GAC3B,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,wCAElB,OAAO,GAAO,KAAM,EAAQ,KAnchC,CAAI,IAAJ,OAAI,MAAJ,SAscO,EAAc,GACjB,OAAO,IAAI,EACT,KAAK,OAAO,KAAK,EAAK,GACtB,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA3cX,CAAI,IAAJ,eAAI,MAAJ,SAkde,EAAoB,EAAc,GAC7C,MAAM,IAAI,MAAM,6BAndpB,CAAI,IAAJ,SAAI,MAAJ,SAsdS,GACL,OAAO,GAAO,KAAM,KAvdxB,CAAI,IAAJ,SAAI,MAAJ,SA6dS,GACL,MAAM,IAAI,MAAM,6BA9dpB,CAAI,IAAJ,OAAI,MAAJ,WAkeI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAteX,CAAI,IAAJ,SAAI,MAAJ,SA6eS,EAAc,GACnB,MAAM,IAAI,MAAM,6BA9epB,CAAI,IAAJ,YAAI,MAAJ,SAofY,EAAsB,GAC9B,MAAM,IAAI,MAAM,6BArfpB,CAAI,IAAJ,QAAI,MAAJ,WAyfI,OAAO,IAAI,EACT,KAAK,OAAO,QACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA7fX,CAAI,IAAJ,OAAI,MAAJ,WAkgBI,OAAO,IAAI,EACT,KAAK,OAAO,OACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aAtgBX,CAAI,IAAJ,QAAI,MAAJ,WA2gBI,OAAO,IAAI,EACT,KAAK,OAAO,QACZ,KAAK,QAAQ,OACb,KAAK,MACL,KAAK,aA/gBX,CAAI,IAAJ,WAAI,MAAJ,SAshBW,GACP,MAAM,IAAI,MAAM,6BAvhBpB,CAAI,IAAJ,YAAI,MAAJ,SA8hBI,EACA,EACA,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BApiBpB,CAAI,IAAJ,MAAI,MAAJ,SAojBM,EAAsB,EAAgB,GACxC,kEAAiB,EAAQ,EAAO,KArjBpC,CAAI,IAAJ,WAAI,MAAJ,SA4jBW,EAAsB,EAAgB,GAC7C,uEAAsB,EAAQ,EAAO,KA7jBzC,CAAI,IAAJ,WAAI,MAAJ,SAokBW,EAAsB,GAC7B,uEAAsB,EAAQ,KArkBlC,CAAI,IAAJ,SAAI,MAAJ,SA4kBS,EAAsB,GAC3B,qEAAoB,EAAQ,KA7kBhC,CAAI,IAAJ,WAAI,MAAJ,SAilBI,EACA,EACA,EACA,EACA,GAEA,OAAO,GAAI,EAA0B,EAAQ,EAAa,EAAO,KAvlBrE,CAAI,IAAJ,gBAAI,MAAJ,SA2lBI,EACA,EACA,EACA,EACA,GAEA,OAAO,GAAS,EAA0B,EAAQ,EAAa,EAAO,KAjmB1E,CAAI,IAAJ,gBAAI,MAAJ,SAqmBI,EACA,EACA,EACA,GAEA,OAAO,GAAS,EAA0B,EAAQ,EAAa,KA1mBnE,CAAI,IAAJ,cAAI,MAAJ,SA8mBI,EACA,EACA,EACA,GAEA,OAAO,GAAO,EAA0B,EAAQ,EAAa,KAnnBjE,CAAI,IAAJ,aAAI,MAAJ,SA0nBI,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BA9nBpB,CAAI,IAAJ,YAAI,MAAJ,SAqoBI,EACA,EACA,EACA,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BA5oBpB,CAAI,IAAJ,MAAI,MAAJ,SAspBM,EAA0B,GAC5B,kEAAiB,EAAM,KAvpB3B,CAAI,IAAJ,WAAI,MAAJ,SA0pBqB,EAAgB,GACjC,OAAO,GAAI,KAAM,EAAM,KA3pB3B,CAAI,IAAJ,iBAAI,MAAJ,SA8pB2B,EAAgB,GACvC,OAAO,GAAU,KAAM,EAAM,KA/pBjC,CAAI,IAAJ,eAAI,MAAJ,SAkqByB,EAAgB,GACrC,OAAO,GAAQ,KAAM,EAAM,KAnqB/B,CAAI,IAAJ,WAAI,MAAJ,SAsqBqB,EAAgB,GACjC,OAAO,GAAI,KAAM,EAAM,KAvqB3B,CAAI,IAAJ,WAAI,MAAJ,SA0qBqB,EAAgB,GACjC,OAAO,GAAI,KAAM,EAAM,KA3qB3B,CAAI,IAAJ,kBAAI,MAAJ,SA8qB4B,EAAgB,GACxC,OAAO,GAAW,KAAM,EAAM,KA/qBlC,CAAI,IAAJ,wBAAI,MAAJ,SAmrBI,EACA,GAEA,OAAO,GAAiB,KAAM,EAAM,KAtrBxC,CAAI,IAAJ,oBAAI,MAAJ,SAyrB8B,EAAgB,GAC1C,OAAO,GAAa,KAAM,EAAM,KA1rBpC,CAAI,IAAJ,uBAAI,MAAJ,SA8rBI,EACA,GAEA,OAAO,GAAgB,KAAM,EAAM,KAjsBvC,CAAI,IAAJ,YAAI,MAAJ,SAwsBI,EACA,EACA,EACA,EACA,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BAhtBpB,CAAI,IAAJ,qBAAI,MAAJ,SAutBI,EACA,EACA,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BA7tBpB,CAAI,IAAJ,WAAI,MAAJ,SAouBI,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BAxuBpB,CAAI,IAAJ,mBAAI,MAAJ,SA+uBI,EACA,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BApvBpB,CAAI,IAAJ,iBAAI,MAAJ,SA0vB2B,GACvB,MAAM,IAAI,MAAM,6BA3vBpB,CAAI,IAAJ,aAAI,MAAJ,SAkwBI,EACA,EACA,EACA,GAEA,MAAM,IAAI,MAAM,8BAvwBpB,EAAI,IAAJ,YAAI,MAAJ,SAcI,GAKA,IAHA,IAAI,EAAM,EACJ,EAAK,GACL,EAAO,GACJ,EAAI,EAAG,EAAI,EAAO,KAAM,IAC/B,GAAsB,IAAlB,EAAO,IAAI,GAAU,CACvB,IAGA,IADA,IAAM,EAAQ,EAAW,EAAG,EAAO,SAC1B,EAAI,EAAG,EAAI,EAAO,MAAM,OAAQ,IACvC,EAAG,KAAK,EAAM,IAEhB,EAAK,KAAK,EAAO,IAAI,IAIzB,IAAM,EAAU,IAAI,EAAU,CAAC,EAAK,EAAO,MAAM,QAAS,EAAI,UAE9D,OAAO,IAAI,EADI,IAAI,EAAU,CAAC,GAAM,EAAM,EAAO,OACjB,EAAS,EAAO,WAjCpD,GAAkE,GC7BrD,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAO,KAAK,MAAM,MAAM,OACxB,EAAU,EAAK,SAAS,GAC9B,EAAK,SACU,KAAK,MAAM,SAAS,IAEjC,EAAQ,WATd,CAAI,IAAJ,SAAI,MAAJ,WAcS,KAAK,MAAM,UACd,KAAK,MAAM,aAfjB,KCAa,GAAb,WACI,SAAF,EAAmB,EAA+B,GAAiB,oBAAhD,aAA+B,WADlD,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,EAAK,SAAS,KAAK,KACpB,KAAK,MAAM,SAAS,IAEjC,EAAQ,WAPd,CAAI,IAAJ,SAAI,MAAJ,WAYS,KAAK,MAAM,UACd,KAAK,MAAM,aAbjB,KCAa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,EAAK,OAAO,KAAK,MAAM,OACxB,KAAK,MAAM,SAAS,IAEjC,EAAQ,WAPd,CAAI,IAAJ,SAAI,MAAJ,WAYS,KAAK,MAAM,UACd,KAAK,MAAM,aAbjB,KCAa,GAAb,WACI,SAAF,EAAmB,EAA2B,GAAkB,oBAA7C,SAA2B,SAD9C,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACD,IAAD,KAAK,EAAE,OAAQ,CAClB,IAAM,EAAQ,KAAK,EAAE,MAAM,KAAK,GAAM,GAAM,GAC7B,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIJ,IAAD,KAAK,EAAE,OAAQ,CAClB,IAAM,EAAQ,EAAK,KAAK,KAAK,EAAE,OAAO,GAAO,GAC9B,KAAK,EAAE,SAAS,IAE7B,EAAM,YAhBd,CAAI,IAAJ,SAAI,MAAJ,WAsBS,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,aA3Bb,KCAa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAS,EAAK,SACL,KAAK,MAAM,SAAS,IAEjC,EAAO,WAPb,CAAI,IAAJ,SAAI,MAAJ,WAYS,KAAK,MAAM,UACd,KAAK,MAAM,aAbjB,KCAa,GAAb,WACI,SAAF,EAAmB,EAA+B,GAAkB,oBAAjD,aAA+B,YADlD,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAW,EAAK,OAAO,KAAK,KAAM,IACzB,KAAK,MAAM,SAAS,IAEjC,EAAS,WAPf,CAAI,IAAJ,SAAI,MAAJ,WAYS,KAAK,MAAM,UACd,KAAK,MAAM,aAbjB,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAY,oBAFZ,SACA,SACA,YAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACP,IAAI,EAAO,KAAK,KAKV,GAJF,EAAO,IACT,GAAQ,KAAK,EAAE,WAAW,SAGvB,KAAK,EAAE,OAAQ,CAClB,IAAM,EAAQ,EAAK,MAAM,CAAC,GAAI,CAAC,KAAK,EAAE,WAAW,IAAQ,CAAC,IAC3C,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIV,IAAK,KAAK,EAAE,OAAQ,CAClB,IAAM,EAAQ,EAAK,MACjB,CAAC,KAAK,EAAE,WAAW,IACnB,CAAC,EAAK,WAAW,IACjB,CAAC,IAEY,KAAK,EAAE,SAAS,IAE7B,EAAM,YA7Bd,CAAI,IAAJ,SAAI,MAAJ,WAmCS,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,aAxCb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAY,YAAAL,KAAA,GAFZ,aACA,WACA,WAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACP,IAAM,EAAS,KAAK,MAAM,MAAM,aAAa,EAAM,KAAK,IAAK,KAAK,KACnD,KAAK,MAAM,SAAS,IAEjC,EAAO,WAXb,CAAI,IAAJ,SAAI,MAAJ,WAgBS,KAAK,MAAM,UACd,KAAK,MAAM,aAjBjB,KCAa,GAAb,WACI,SAAF,EAAmB,EAA2B,GAAiB,oBAA5C,SAA2B,eAD9C,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GAKD,IAJN,IAAM,EAAS,KAAK,EAAE,WAEhB,EAAe,GACf,EAAU,GACP,EAAI,EAAG,EAAI,EAAO,OAAQ,IACjC,EAAa,KAAK,KAAK,QAAQ,GAAI,EAAO,IAC1C,EAAQ,KAAS,EAAJ,GAGT,MAAQ,EAAK,QAAQ,GAAc,GAAO,IAAI,GAAS,GAC9C,KAAK,EAAE,SAAS,IAE7B,EAAM,WAhBZ,CAAI,IAAJ,SAAI,MAAJ,WAqBS,KAAK,EAAE,UACV,KAAKK,EAAE,aAtBb,KCAa,GAAb,WACI,SAAF,EAAmB,EAA2B,GAAwB,oBAAnD,SAA2B,aAD9C,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GAQP,IARyB,MAEW,KAAK,EAAE,MAAM,YAC/C,KAAK,EAAE,WACP,KAAK,OAJkB,mBAElB,EAFkB,KAEV,EAFU,KAOnB,GAPmB,KAOT,IACP,EAAI,EAAG,EAAI,EAAO,OAAQ,IAC7B,EAAO,GAAK,EAAK,IACnB,EAAQ,KAAK,GAIjB,IAAM,EAAQ,EAAK,IAAI,GAAS,QAAQ,KAAK,EAAE,YAChC,KAAKA,EAAE,SAAS,IAE7B,EAAM,WApBZ,CAAI,IAAJ,SAAI,MAAJ,WAyBS,KAAK,EAAE,UACV,KAAK,EAAE,aA1Bb,KCAa,GAAb,WACI,SAAF,EAAmB,GAAkB,oBAAlB,SADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACD,MAAS,KAAK,EAAE,WAEhB,IAAD,KAAK,EAAE,OAAQ,CACV,IAAF,EAAQ,EAAK,QAAQ,GACZ,KAAK,EAAE,SAAS,IAE7B,EAAM,YAVd,CAAI,IAAJ,SAAI,MAAJ,WAgBS,KAAK,EAAE,UACV,KAAKA,EAAE,aAjBb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,EACA,GAAY,YAAAL,KAAA,GAJZ,SACA,SACA,aACA,aACA,YANT,uBAAE,IAAJ,WAAI,MAAJ,SASW,GAMD,IALN,IAAM,EAAS,KAAK,EAAE,WAChB,EAAS,KAAK,EAAE,WAEhB,EAAW,GACX,EAAW,GACR,EAAI,EAAG,EAAI,KAAK,MAAM,OAAQ,IACjC,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAK,GAGZ,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAKR,GAIZ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACJ,GAAwB,IAApB,EAAS,OAET,EADiB,IAAf,KAAK,MACC,EAAK,QAAQ,GAEb,EAAK,eAAe,KAAK,OAAO,QAAQ,GAAQ,QAGhD,GAAS,IAAf,KAAK,MACP,EAAQ,EAAK,IAAI,GAAU,QAAQ,GAAQ,OACtC,CACL,IAAM,EAAS,EAAK,IAAI,GAClB,EAAS,EAAO,eAAe,KAAK,OAC1C,EAAO,SACP,EAAQ,EAAO,QAAQ,GAAQ,GAGpB,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIJ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACI,GAAgB,IAApB,EAAS,OAET,EADgB,IAAd,KAAK,KACC,EAAK,QAAQ,GAEb,EAAK,eAAe,KAAK,MAAM,QAAQ,GAAQ,QAGzD,GAAkB,IAAd,KAAK,KACP,EAAQ,EAAK,IAAI,GAAU,QAAQ,GAAQ,OACtC,CACL,IAAM,EAAS,EAAK,IAAI,GAClB,EAAS,EAAO,eAAe,KAAK,MAC1C,EAAO,SACP,EAAQ,EAAO,QAAQ,GAAQ,GAGpB,KAAK,EAAE,SAAS,IAE7B,EAAM,YArEd,CAAI,IAAJ,SAAI,MAAJ,WA2ES,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,aAhFb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,EACA,GAAY,YAAAQ,KAAA,GAJZ,SACA,SACA,aACA,aACA,YANT,uBAAE,IAAJ,WAAI,MAAJ,SASW,GAMD,IALN,IAAM,EAAS,KAAK,EAAE,WAChB,EAAS,KAAK,EAAE,WAEhB,EAAW,GACX,EAAW,GACR,EAAI,EAAG,EAAI,KAAK,MAAM,OAAQ,IACjC,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAK,GAGZ,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAKR,GAIZ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACJ,GAAwB,IAApB,EAAS,OAET,EADiB,IAAf,KAAK,MACC,EAAK,QAAQ,GAEb,EAAK,eAAe,KAAK,OAAO,QAAQ,GAAQ,QAGhD,GAAS,IAAf,KAAK,MACP,EAAQ,EAAK,IAAI,GAAU,QAAQ,GAAQ,OACtC,CACL,IAAM,EAAS,EAAK,IAAI,GAClB,EAAS,EAAO,eAAe,KAAK,OAC1C,EAAO,SACP,EAAQ,EAAO,QAAQ,GAAQ,GAGpB,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIJ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACI,GAAgB,IAApB,EAAS,OACX,EAAQ,EAAK,gBAAgB,KAAK,MAAM,QAAQ,GAAQ,OACnD,CACL,IAAM,EAAS,EAAK,IAAI,GAClB,EAAS,EAAO,gBAAgB,KAAK,MAC3C,EAAO,SACP,EAAQ,EAAO,QAAQ,GAAQ,GAElB,KAAK,EAAE,SAAS,IAE7B,EAAM,YA7Dd,CAAI,IAAJ,SAAI,MAAJ,WAmES,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,aAxEb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,GAAa,oBAHb,SACA,SACA,aACA,aALT,uBAAE,IAAJ,WAAI,MAAJ,SAQW,GAMD,IALN,IAAM,EAAS,KAAK,EAAE,WAChB,EAAS,KAAK,EAAE,WAEhB,EAAW,GACX,EAAW,GACR,EAAI,EAAG,EAAI,KAAK,MAAM,OAAQ,IACjC,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAK,GAGZ,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAK,GAIZ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACI,GAAgB,IAApB,EAAS,OACX,EAAQ,EAAK,SAAS,KAAK,EAAE,MAAO,KAAK,OAAO,QAAQ,GAAQ,OAC3D,CACL,IAAM,EAAO,EAAK,SAAS,KAAK,EAAE,MAAO,KAAK,OACxC,EAAS,EAAK,IAAI,GACxB,EAAK,SACL,EAAQ,EAAO,QAAQ,GAAQ,GAElB,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIJ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACI,GAAgB,IAApB,EAAS,OACX,EAAQ,EAAK,SAAS,KAAK,EAAE,MAAO,KAAK,OAAO,QAAQ,GAAQ,OAC3D,CACL,IAAM,EAAO,EAAK,SAAS,KAAK,EAAE,MAAO,KAAK,OACxC,EAAS,EAAK,IAAI,GACxB,EAAK,SACL,EAAQ,EAAO,QAAQ,GAAQ,GAElB,KAAK,EAAE,SAAS,IAE7B,EAAM,YApDd,CAAI,IAAJ,SAAI,MAAJ,WA0DS,KAAK,EAAE,UACV,KAAKa,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,aA/Db,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,EACA,EACA,EACA,GAAmB,oBANnB,SACA,SACA,eACA,eACA,iBACA,aACA,SART,uBAAE,IAAJ,WAAI,MAAJ,SAWW,GACD,IAAD,KAAK,EAAE,OAAQ,CAClB,IAAM,EAAQ,KAAK,EAAE,MAAM,KACzB,OACA,EACA,KAAK,QACL,KAAK,MACL,KAAK,QACL,KAAK,WAEQ,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIJ,QAAS,IAAX,KAAK,IAAoB,KAAK,EAAE,OAAQ,CAE1C,IADA,IAAM,EAAU,CAAC,GACR,EAAI,EAAG,EAAI,KAAK,UAAU,OAAQ,IACzC,EAAQ,KAAK,EAAI,GAGnB,IAAM,EAAQ,EAAK,IAAI,GACR,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIV,IAAK,KAAK,EAAE,OAAQ,CAIlB,IAHA,IAAM,EAAS,KAAK,EAAE,WAElB,EAAQ,GACH,EAAI,EAAG,EAAI,KAAK,UAAU,OAAQ,IACzC,EAAM,KAAK,EAAO,EAAI,GAAK,KAAK,QAAQ,GAAK,KAAK,UAAU,GAAK,GAEnE,EAAK,sBAAO,GAAP,YAAiB,IAEtB,IAAM,EAAQ,EAAK,cACjB,KAAK,EAAE,MACP,KAAK,UACL,KAAK,MACL,EACA,KAAK,SAEQ,KAAK,EAAE,SAAS,IAE7B,EAAM,YA1Dd,CAAI,IAAJ,SAAI,MAAJ,WAgES,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,cAGM,IAAX,KAAK,GAAoB,KAAK,EAAE,UAClC,KAAK,EAAE,aAzEb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,EACA,GAAa,oBAJb,SACA,SACA,iBACA,aACA,aANT,uBAAE,IAAJ,WAAI,MAAJ,SASW,GAMD,IALN,IAAM,EAAS,KAAK,EAAE,WAChB,EAAS,KAAK,EAAE,WAEhB,EAAW,GACX,EAAW,GACR,EAAI,EAAG,EAAI,KAAK,MAAM,OAAQ,IACjC,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAK,GAGZ,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAK,GAIZ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACI,GAAgB,IAApB,EAAS,OACX,EAAQ,EAAK,OAAO,KAAK,EAAE,MAAO,KAAK,OAAO,QAAQ,GAAQ,OACzD,CACL,IAAM,EAAO,EAAK,OAAO,KAAK,EAAE,MAAO,KAAK,OACtC,EAAS,EAAK,IAAI,GACxB,EAAK,SACL,EAAQ,EAAO,QAAQ,GAAQ,GAElB,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIJ,IAAD,KAAK,EAAE,OAAQ,CAClB,IAAI,EACI,GAAgB,IAApB,EAAS,OAAc,CACzB,IAAM,EAAa,EAAK,SAAS,KAAK,WAChC,EAAU,EAAW,OAAO,KAAK,EAAE,OAAQ,KAAK,OACtD,EAAW,SAEX,EAAQ,EAAQ,QAAQ,GAAQ,OAC3B,CACL,IAAM,EAAa,EAAK,SAAS,KAAK,WAChC,EAAU,EAAW,OAAO,KAAK,EAAE,OAAQ,KAAK,OACtD,EAAW,SACX,IAAM,EAAS,EAAQ,IAAI,GAC3B,EAAQ,SACR,EAAQ,EAAO,QAAQ,GAAQ,GAElB,KAAK,EAAE,SAAS,IAE7B,EAAM,YA3Dd,CAAI,IAAJ,SAAI,MAAJ,WAiES,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,aAtEb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,GAAwB,oBAHxB,SACA,SACA,mBACA,aALT,uBAAE,IAAJ,WAAI,MAAJ,SAQW,GAMD,IALN,IAAM,EAAS,KAAK,EAAE,WAChB,EAAS,KAAK,EAAE,WAEhB,EAAW,GACX,EAAW,GACR,EAAI,EAAG,EAAI,KAAK,MAAM,OAAQ,IACjC,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAK,GAGZ,EAAO,GAAK,KAAK,MAAM,IACzB,EAAS,KAAK,GAIZ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACJ,GAAwB,IAApB,EAAS,OAAc,CACzB,IAAM,EAAa,KAAK,YAAY,SAAS,KAAK,EAAE,OAC9C,EAAU,EAAW,OAAO,KAAK,EAAE,OACzC,EAAW,SACX,IAAM,EAAW,EAAK,SAAS,GAC/B,EAAQ,SACR,EAAQ,EAAS,QAAQ,GAAQ,OAC5B,CACL,IAAM,EAAa,KAAK,YAAY,SAAS,KAAK,EAAE,OAC9C,EAAU,EAAW,OAAO,KAAK,EAAE,OACzC,EAAW,SACX,IAAM,EAAW,EAAK,SAAS,GAC/B,EAAQ,SACR,IAAM,EAAS,EAAS,IAAI,GAC5B,EAAS,SACT,EAAQ,EAAO,QAAQ,GAAQ,GAElB,KAAK,EAAE,SAAS,IAE7B,EAAM,SAIJ,IAAD,KAAK,EAAE,OAAQ,CACV,IAAJ,EACJ,GAAwB,IAApB,EAAS,OAAc,CACzB,IAAM,EAAM,KAAK,EAAE,MAAM,MACnB,EAAO,KAAK,YAAY,SAAS,GACvC,EAAI,SACJ,EAAQ,EAAK,SAAS,GACtB,EAAK,SAEL,EAAQ,EAAM,QAAQ,GAAQ,OACzB,CACL,IAAM,EAAM,KAAK,EAAE,MAAM,MACnB,EAAO,KAAK,YAAY,SAAS,GACvC,EAAI,SACJ,IAAM,EAAS,EAAK,SAAS,GAC7B,EAAK,SACL,IAAM,EAAS,EAAO,IAAI,GAC1B,EAAO,SACP,EAAQ,EAAO,QAAQ,GAAQ,GAElB,KAAK,EAAE,SAAS,IAE7B,EAAM,YAvEd,CAAI,IAAJ,SAAI,MAAJ,WA6ES,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,aAlFb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,EACA,EACA,EACA,GAAmB,oBANnB,SACA,SACA,cACA,cACA,aACA,YACA,SART,uBAAE,IAAJ,WAAI,MAAJ,SAWW,GAEL,IAAI,EAaA,EAdD,KAAK,EAAE,SAGR,EADE,KAAK,OACC,EAAK,KAAK,KAAK,EAAE,OAAO,EAAM,KAAK,OAAQ,KAAK,OAEhD,KAAK,EAAE,MAAM,KAAK,GAAO,KAAK,QAAQ,EAAO,KAAK,OAE7C,KAAK,EAAE,SAAS,IAE7B,EAAM,UAIL,KAAK,EAAE,SAGR,EADE,KAAK,OACC,KAAK,EAAE,MAAM,KAAK,EAAM,KAAK,QAAQ,EAAM,KAAK,OAEhD,EAAK,KAAK,KAAK,EAAE,OAAO,GAAQ,KAAK,OAAQ,KAAK,OAE7C,KAAK,EAAE,SAAS,IAE7B,EAAM,UAIJ,QAAS,IAAX,KAAK,IAAoB,KAAK,EAAE,OAAQ,CAK1C,IAJQ,IAAF,EAAY,EAAK,WACjB,EAAS,KAAK,EAAE,WAChB,EAAW,GAER,EAAI,EAAG,EAAI,EAAU,OAAQ,IAChC,EAAO,GAAK,EAAU,IACxB,EAAS,KAAK,GAIlB,IAAI,EAAQ,EAAK,IAAI,GAAU,QAAQ,GAAQ,GAC/C,GAAkB,IAAd,KAAK,KAAY,CACnB,IAAM,EAAW,EACjB,EAAQ,EAAM,eAAe,KAAK,MAClC,EAAS,SAGI,KAAK,EAAE,SAAS,IAE7B,EAAM,YA1Dd,CAAI,IAAJ,SAAI,MAAJ,WAgES,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,cAGM,IAAX,KAAK,GAAoB,KAAK,EAAE,UAClC,KAAK,EAAE,aAzEb,KCAa,GAAb,WACI,SAAF,EAAmB,EAA2B,GAAqB,oBAAhD,SAA2B,mBAD9C,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GAED,IADN,IAAM,EAAc,IAAI,MAAM,KAAK,YAAY,QACtC,EAAI,EAAG,EAAIL,KAAKM,YAAY,OAAQ,IAC3C,EAAY,KAAK,YAAY,IAAM,EAGrC,IAAM,EAAQ,EAAK,UAAU,GACd,KAAK,EAAE,SAAS,IAE7B,EAAM,WAZZ,CAAI,IAAJ,SAAI,MAAJ,WAiBS,KAAK,EAAE,UACV,KAAK,EAAE,aAlBb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAiB,oBAFjB,aACA,eACA,gBAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACD,MAAU,KAAK,MAAM,MAAM,WAC3B,IAAD,KAAK,SAAU,CAGV,IAFA,IAAF,EAAW,GACb,EAAO,EACF,EAAI,EAAG,EAAI,EAAQ,OAAQ,IAC9B,EAAO,KAAK,QAAQ,QAAU,KAAK,QAAQ,KAAU,GACvD,EAAS,KAAK,GACd,KAEA,EAAS,KAAK,EAAQ,IAI1B,EAAO,EAAK,QAAQ,GAAU,GAGhC,EAAO,EAAK,OAAO,GACJ,KAAK,MAAM,SAAS,IAEjC,EAAK,WA3BX,CAAI,IAAJ,SAAI,MAAJ,WAgCS,KAAK,MAAM,UACd,KAAK,MAAM,aAjCjB,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAiB,oBAFjB,aACA,eACA,gBAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACD,MAAU,KAAK,MAAM,MAAM,WAC3B,IAAD,KAAK,SAAU,CAGV,IAFA,IAAF,EAAW,GACb,EAAO,EACF,EAAI,EAAG,EAAI,EAAQ,OAAQ,IAC9B,EAAO,KAAK,QAAQ,QAAU,KAAK,QAAQ,KAAU,GACvD,EAAS,KAAK,GACd,KAEA,EAAS,KAAK,EAAQ,IAI1B,EAAO,EAAK,QAAQ,GAAU,GAG1B,MAAW,EAAK,OAAO,GACvB,EAAS,EAAS,SAAS,KAAK,MAAM,MAAO,GACnD,EAAS,SAEM,KAAK,MAAM,SAAS,IAEjC,EAAO,WA9Bb,CAAI,IAAJ,SAAI,MAAJ,WAmCS,KAAK,MAAM,UACd,KAAK,MAAM,aApCjB,KCAa,GAAb,WAEI,SAAF,EAAmB,EAA+B,GAAc,oBAA7C,aAA+B,cAFlD,uBAAE,IAAJ,WAAI,MAAJ,SAIW,GACP,IAAM,EAAS,EAAK,eAAe,KAAK,QACzB,KAAK,MAAM,SAAS,IAEjC,EAAO,WARb,CAAI,IAAJ,SAAI,MAAJ,WAaS,KAAK,MAAM,UACd,KAAK,MAAM,aAdjB,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAiB,oBAFjB,aACA,eACA,gBAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACD,MAAU,KAAK,MAAM,MAAM,WAC3B,IAAD,KAAK,SAAU,CAGV,IAFA,IAAF,EAAW,GACb,EAAO,EACF,EAAI,EAAG,EAAI,EAAQ,OAAQ,IAC9B,EAAO,KAAK,QAAQ,QAAU,KAAK,QAAQ,KAAU,GACvD,EAAS,KAAK,GACd,KAEA,EAAS,KAAK,EAAQ,IAI1B,EAAO,EAAK,QAAQ,GAAU,GAIhC,IADA,IAAI,EAAU,EACL,EAAI,EAAG,EAAI,KAAK,QAAQ,OAAQ,IACvC,GAAW,EAAQ,KAAK,QAAQ,IAGlC,IAAM,EAAa,EAAK,eAAe,EAAI,GACrC,EAAS,EAAW,OAAO,GACjC,EAAW,OACI,KAAK,MAAM,SAAS,IAEjC,EAAK,WAlCX,CAAI,IAAJ,SAAI,MAAJ,WAuCS,KAAK,MAAM,UACd,KAAK,MAAM,aAxCjB,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAiB,oBAFjB,aACA,eACA,gBAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACD,MAAU,KAAK,MAAM,MAAM,WAC3B,IAAD,KAAK,SAAU,CAGV,IAFA,IAAF,EAAW,GACb,EAAO,EACF,EAAI,EAAG,EAAI,EAAQ,OAAQ,IAC9B,EAAO,KAAK,QAAQ,QAAU,KAAK,QAAQ,KAAU,GACvD,EAAS,KAAK,GACd,KAEA,EAAS,KAAK,EAAQ,IAI1B,EAAO,EAAK,QAAQ,GAAU,GAIhC,IADA,IAAI,EAAU,EACL,EAAI,EAAG,EAAI,KAAK,QAAQ,OAAQ,IACvC,GAAW,EAAQ,KAAK,QAAQ,IAGlC,IAAM,EAAW,EAAK,OAAO,GACvB,EAAS,EAAS,SAAS,KAAK,MAAM,MAAO,EAAI,GACvD,EAAS,SAEM,KAAK,MAAM,SAAS,IAEjC,EAAO,WAnCb,CAAI,IAAJ,SAAI,MAAJ,WAwCS,KAAK,MAAM,UACd,KAAK,MAAM,aAzCjB,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,EACA,GAEP,GAFsB,oBAJf,SACA,cACA,YACA,YACA,kBAE0B,IAA7B,EAAM,MAAK,YAAC,OAAU,IAAN,KAClB,MAAM,IAAI,MAAM,oDATpB,uBAAE,IAAJ,WAAI,MAAJ,SAaW,GACD,IAAD,KAAK,EAAE,OAAQ,CAKlB,IAJQ,IAAF,EAAS,KAAK,EAAE,WAChB,EAAO,EAAO,OAEd,EAAO,IAAI,MAAa,EAAP,GAAU,KAAK,GAC7B,EAAI,EAAG,EAAI,KAAK,KAAK,OAAQ,IACpC,EAAK,KAAK,KAAK,IAAM,KAAK,OAAO,GACjC,EAAK,EAAO,KAAK,KAAK,IAAM,EAAO,KAAK,KAAK,IAAM,KAAK,KAAK,GAG/D,IAAM,EAAQ,EAAK,IAAI,EAAM,WAAY,GAC1B,KAAK,EAAE,SAAS,IAE7B,EAAM,YA3Bd,CAAI,IAAJ,SAAI,MAAJ,WAiCS,KAAK,EAAE,UACV,KAAK,EAAE,aAlCb,KCAa,GAAb,WACI,SAAF,EACS,EACP,EACA,EACA,EACA,GAAmB,oBAJZ,SAFT,uBAAE,IAAJ,WAAI,MAAJ,SASW,GACP,MAAM,IAAI,MAAM,oDAVpB,CAAI,IAAJ,SAAI,MAAJ,WAcS,KAAK,EAAE,UACV,KAAK,EAAE,aAfb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,GAAa,oBAHb,SACA,YACA,YACA,aALT,uBAAE,IAAJ,WAAI,MAAJ,SAQW,GACD,UAAI,MAAM,2CATpB,CAAI,IAAJ,SAAI,MAAJ,WAaS,KAAK,EAAE,UACV,KAAK,EAAE,aAdb,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,EACA,GAAiB,oBAHjB,aACA,eACA,eACA,gBALT,uBAAE,IAAJ,WAAI,MAAJ,SAQW,GACD,MAAU,KAAK,MAAM,MAAM,WAE7B,EAAO,EAAK,SAAS,KAAK,SACxB,IAAD,KAAK,SAAU,CAGV,IAFA,IAAF,EAAW,GACb,EAAO,EACF,EAAI,EAAG,EAAI,EAAQ,OAAQ,IAC9B,EAAO,KAAK,QAAQ,QAAU,KAAK,QAAQ,KAAU,GACvD,EAAS,KAAK,GACd,KAEA,EAAS,KAAK,EAAQ,IAI1B,EAAO,EAAK,QAAQ,GAAU,GAG1B,MAAW,EAAK,OAAO,GAC7B,EAAK,SACL,IAAM,EAAS,EAAS,OAAO,KAAK,MAAM,OAC1C,EAAS,SAEM,KAAK,MAAM,SAAS,IAEjC,EAAO,WAlCb,CAAI,IAAJ,SAAI,MAAJ,WAuCS,KAAK,MAAM,UACd,KAAK,MAAM,aAxCjB,KCAa,GAAb,WACI,SAAF,EAAmB,EAA+B,GAAqB,oBAApD,aAA+B,eADlD,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAWN,KAAKO,QAAQ,mBAAmB,EAAG,GAC9C,EAAO,KAAK,QAAQ,SAAS,GACnC,EAAS,SACT,IAAM,EAAS,EAAK,SAAS,GAC7B,EAAK,SACU,KAAK,MAAM,SAAS,IAEjC,EAAO,WAXb,CAAI,IAAJ,SAAI,MAAJ,WAgBS,KAAK,MAAM,UACd,KAAK,MAAM,aAjBjB,KCAa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAM,KAAK,MAAM,MAAM,MACvB,EAAU,EAAK,SAAS,GAC9B,EAAI,SACW,KAAK,MAAM,SAAS,IAEjC,EAAQ,WATd,CAAI,IAAJ,SAAI,MAAJ,WAcS,KAAK,MAAM,UACd,KAAK,MAAM,aAfjB,KAoBa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,KAAK,MAAM,MAAM,SAAS,KAAK,MAAM,OAC/C,EAAW,EAAQ,mBAAmB,EAAG,GAC/C,EAAQ,SACR,IAAM,EAAO,EAAS,OACtB,EAAS,SACT,IAAM,EAAW,EAAK,OAAO,GACvB,EAAD,SACU,KAAK,MAAM,SAAS,IAEjC,EAAS,WAbf,CAAI,IAAJ,SAAI,MAAJ,WAkBS,KAAK,MAAM,UACd,KAAK,MAAM,aAnBjB,KAwBa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAO,KAAK,MAAM,MAAM,OACxB,EAAW,EAAK,SAAS,GAC/B,EAAK,SACU,KAAK,MAAM,SAAS,IAEjC,EAAS,WATf,CAAI,IAAJ,SAAI,MAAJ,WAcS,KAAK,MAAM,UACd,KAAK,MAAM,aAfjB,KAoBa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,KAAK,MAAM,MAAM,SAAS,KAAK,MAAM,OAC/C,EAAU,EAAQ,kBAAkB,EAAG,GAC7C,EAAQ,SACR,IAAM,EAAO,EAAQ,OACf,EAAE,SACR,IAAM,EAAY,EAAK,OAAO,GAC9B,EAAK,SACU,KAAK,MAAM,SAAS,IAEjC,EAAU,WAbhB,CAAI,IAAJ,SAAI,MAAJ,WAkBS,KAAK,MAAM,UACd,KAAK,MAAM,aAnBjB,KChEa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAM,KAAK,MAAM,MAAM,MACvB,EAAU,EAAK,SAAS,GAAM,GACpC,EAAI,SACW,KAAK,MAAM,SAAS,IAEjC,EAAQ,WATd,CAAI,IAAJ,SAAI,MAAJ,WAcS,KAAK,MAAM,UACd,KAAK,MAAM,aAfjB,KAoBa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,KAAK,MAAM,MAAM,SAAS,KAAK,MAAM,OAC/C,EAAW,EAAQ,mBAAmB,EAAG,GAC/C,EAAQ,SACR,IAAM,EAAO,EAAS,OACtB,EAAS,SACT,IAAM,EAAW,EAAK,OAAO,GAAO,GAC9B,EAAD,SACU,KAAK,MAAM,SAAS,IAEjC,EAAS,WAbf,CAAI,IAAJ,SAAI,MAAJ,WAkBS,KAAK,MAAM,UACd,KAAK,MAAM,aAnBjB,KAwBa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAO,KAAK,MAAM,MAAM,OACxB,EAAW,EAAK,SAAS,GAC/B,EAAK,SACU,KAAK,MAAM,SAAS,IAEjC,EAAS,WATf,CAAI,IAAJ,SAAI,MAAJ,WAcS,KAAK,MAAM,UACd,KAAK,MAAM,aAfjB,KAoBa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,KAAK,MAAM,MAAM,SAAS,KAAK,MAAM,OAC/C,EAAU,EAAQ,kBAAkB,GAAI,GAC9C,EAAQ,SACR,IAAM,EAAO,EAAQ,OACf,EAAE,SACR,IAAM,EAAY,EAAK,OAAO,GAC9B,EAAK,SACU,KAAK,MAAM,SAAS,IAEjC,EAAU,WAbhB,CAAI,IAAJ,SAAI,MAAJ,WAkBS,KAAK,MAAM,UACd,KAAK,MAAM,aAnBjB,KChEa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAM,KAAK,MAAM,MAAM,MACvB,EAAO,EAAI,SAAS,GAC1B,EAAI,SACJ,IAAM,EAAU,EAAK,OAAO,GAC5B,EAAK,SACU,KAAK,MAAM,SAAS,IAEjC,EAAQ,WAXd,CAAI,IAAJ,SAAI,MAAJ,WAgBS,KAAK,MAAM,UACd,KAAK,MAAM,aAjBjB,KAsBa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,KAAK,MAAM,MAAM,SAAS,KAAK,MAAM,OAC/C,EAAU,EAAQ,kBAAkB,EAAG,GAC7C,EAAQ,SACR,IAAM,EAAW,EAAK,OAAO,GACvB,EAAE,SACO,KAAK,MAAM,SAAS,IAEjC,EAAS,WAXf,CAAI,IAAJ,SAAI,MAAJ,WAgBS,KAAK,MAAM,UACd,KAAK,MAAM,aAjBjB,KAsBa,GAAb,WACI,SAAF,EAAmB,EAA+B,GAAkB,oBAAjD,aAA+B,YADlD,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,KAAK,KAAKC,SAAS,KAAK,MAClC,EAAU,EAAQ,mBAAmB,EAAG,GAC9C,EAAQ,SACR,IAAM,EAAW,EAAK,SAAS,GACzB,EAAE,SACO,KAAK,MAAM,SAAS,IAEjC,EAAS,WAXf,CAAI,IAAJ,SAAI,MAAJ,WAgBS,KAAK,MAAM,UACd,KAAK,MAAM,aAjBjB,KAsBa,GAAb,WACI,SAAF,EAAmB,GAAsB,oBAAtB,aADnB,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACP,IAAM,EAAU,KAAK,MAAM,MAAM,SAAS,KAAK,MAAM,OAC/C,EAAU,EAAQ,mBAAmB,EAAG,GAC9C,EAAQ,SACF,MAAY,EAAK,OAAO,GAC9B,EAAQ,SACO,KAAK,MAAM,SAAS,IAEjC,EAAU,WAXhB,CAAI,IAAJ,SAAI,MAAJ,WAgBS,KAAK,MAAM,UACd,KAAK,MAAM,aAjBjB,KClEa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAiB,oBAFjB,aACA,eACA,gBAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACP,IAAM,EAAU,KAAK,MAAM,MAAM,WAE3B,EAAM,KAAK,MAAM,MAAM,IAAI,KAAK,QAAS,KAAK,UAChD,EAAa,EAAK,OAAO,GAGvB,GAFA,EAAF,UAEC,KAAK,SAAU,CAGV,IAFA,IAAF,EAAW,GACb,EAAO,EACF,EAAI,EAAG,EAAI,EAAQ,OAAQhB,IAC9B,EAAO,KAAK,QAAQ,QAAU,KAAK,QAAQ,KAAU,GACvD,EAAS,KAAK,GACd,KAEA,EAAS,KAAK,EAAQ,IAI1B,EAAa,EAAW,QAAQ,GAAU,GAG5C,IAAM,EAAW,EAAW,OAAO,GACnC,EAAW,SAEI,KAAK,MAAM,SAAS,IAEjC,EAAS,WAlCf,CAAI,IAAJ,SAAI,MAAJ,WAuCS,KAAK,MAAM,UACd,KAAK,MAAM,aAxCjB,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAiB,oBAFjB,aACA,eACA,gBAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACP,IAAM,EAAU,KAAK,MAAM,MAAM,WAE3B,EAAM,KAAK,MAAM,MAAM,MACvB,EAAM,EAAI,IAAI,KAAK,SAAS,GAC5B,EAAM,EAAI,OAAO,GAIjB,GAHA,EAAF,SACE,EAAF,UAEC,KAAK,SAAU,CAGV,IAFA,IAAF,EAAW,GACb,EAAO,EACF,EAAI,EAAG,EAAI,EAAQ,OAAQA,IAC9B,EAAO,KAAK,QAAQ,QAAU,KAAK,QAAQ,KAAU,GACvD,EAAS,KAAK,GACd,KAEA,EAAS,KAAK,EAAQ,IAI1B,EAAO,EAAK,QAAQ,GAAU,GAGhC,IAAM,EAAW,EAAK,OAAO,GAEvB,EAAgB,EAAS,SAAS,GAClC,EAAG,SACM,KAAK,MAAM,SAAS,IAEjC,EAAc,WArCpB,CAAI,IAAJ,SAAI,MAAJ,WA0CS,KAAK,MAAM,UACd,KAAK,MAAM,aA3CjB,KCAa,GAAb,WACI,SAAF,EACS,EACA,EACA,GAAc,oBAFd,aACA,aACA,cAJT,uBAAE,IAAJ,WAAI,MAAJ,SAOW,GACP,IAAM,EAAM,KAAK,MAAM,MAAM,YAC3B,KAAK,MAAQ,EACb,KAAK,OAAS,KAAK,OAEf,EAAS,EAAK,SAAS,GAC7B,EAAI,SACW,KAAK,MAAM,SAAS,IAEjC,EAAO,WAhBb,CAAI,IAAJ,SAAI,MAAJ,WAqBS,KAAK,MAAM,UACd,KAAK,MAAM,aAtBjB,KCsEa,GAAb,YAAE,qBAAF,iBAYI,SAAF,EAAmB,EAAqB,GAA+B,kCACrE,cAAM,EAAM,QADK,aAGD,IAAZ,IACF,EAAU,IAGZ,EAAK,KAAO,EAAQ,UAEK,IAArB,EAAQ,WACJ,EAAD,SAAW,EAAQ,UAGtB,EAAC,OAAS,EAAQ,SAAU,EAbqC,EAZvE,uBAAE,IAAJ,OAAI,MAAJ,SAmE4B,GACxB,MAAM,IAAI,MAAM,6BApEpB,CAAI,IAAJ,WAAI,MAAJ,SA0EW,GACP,QAAa,IAAT,EAAoB,CACtB,IAAM,EAAW,KAAK,MAAM,WAC5B,GAAwB,IAApB,EAAS,QAAgC,IAAhB,EAAS,GAGpC,MAAM,IAAI,MACR,kFAHF,EAAO,KAAK,MAQhB,IAAI,GAAS,EAEb,QAAkB,IAAd,KAAK,KAAoB,CAC3B,IAAM,EAAU,KAAK,KACrB,KAAK,KAAO,KAAK,KAAK,IAAI,GAC1B,EAAQ,cAER,KAAK,KAAO,EACZ,GAAS,EAMX,YAHsB,IAAlB,KAAK,UACP,KAAK,SAAS,SAAS,GAElB,IApGX,CAAI,IAAJ,SAAI,MAAJ,WAwGU,YAAmB,IAAlB,KAAK,WAxGhB,CAAI,IAAJ,eAAI,MAAJ,SA2Ge,GACX,OAAO,IAAI,EAAS,KAAK,MAAM,aAAa,GAAQ,CAAC,QAAQ,MA5GjE,CAAI,IAAJ,iBAAI,MAAJ,SA+GiB,GACb,OAAO,IAAI,EAAS,KAAK,MAAM,eAAe,GAAQ,CAAC,QAAQ,MAhHnE,CAAI,IAAJ,YAAI,MAAJ,WAoHU,OAAC,KAAK,MAAM,cApHtB,CAAI,IAAJ,WAAI,MAAJ,WAwHI,OAAO,KAAK,MAAM,aAxHtB,CAAI,IAAJ,SAAI,MAAJ,WA4HI,KAAK,MAAM,cACO,IAAd,KAAK,MACP,KAAK,KAAK,cAEU,IAAlB,KAAK,UACP,KAAK,SAAS,WAjIpB,CAAI,IAAJ,eAAI,MAAJ,SAsII,EACA,GAEA,OAAO,IAAI,EAAS,KAAK,MAAM,QAAQ,GAAQ,CAC7C,SAAU,KAAK,YAAS,EAAY,IAAI,GAAY,MACpD,OAAQ,KAAK,WA3InB,CAAI,IAAJ,MAAI,MAAJ,WAgJI,IAAM,EAAM,KAAK,MAAM,MACjB,OAAC,IAAI,EAAS,EAAK,CACvB,SAAU,KAAK,YAAS,EAAY,IAAI,GAAQ,KAAM,GACtD,OAAQ,KAAK,WAnJnB,CAAI,IAAJ,MAAI,MAAJ,WAwJU,OAAC,IAAI,EAAS,KAAK,MAAM,MAAO,CACpC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAQ,MAChD,OAAQ,KAAK,WA1JnB,CAAI,IAAJ,OAAI,MAAJ,WA+JU,MAAO,KAAK,MAAM,OACxB,OAAO,IAAI,EAAS,EAAM,CACxB,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,KAAM,GACvD,OAAQ,KAAK,WAlKnB,CAAI,IAAJ,MAAI,MAAJ,WAuKI,OAAO,IAAI,EAAS,KAAK,MAAM,MAAO,CACpC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAQ,MAChD,OAAQ,KAAK,WAzKnB,CAAI,IAAJ,MAAI,MAAJ,WA8KI,OAAO,IAAI,EAAS,KAAK,MAAM,MAAO,CACpC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAQ,MAChD,OAAQ,KAAK,WAhLnB,CAAI,IAAJ,MAAI,MAAJ,WAqLI,OAAO,IAAI,EAAS,KAAK,MAAM,MAAO,CACpC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAQ,MAChD,OAAQ,KAAK,WAvLnB,CAAI,IAAJ,MAAI,MAAJ,WA4LI,OAAO,IAAI,EAAS,KAAK,MAAM,MAAO,CACpC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAQ,MAChD,OAAQ,KAAK,WA9LnB,CAAI,IAAJ,OAAI,MAAJ,WAmMI,OAAO,IAAI,EAAS,KAAK,MAAM,OAAQ,CACrC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,MACzC,YAAK,WArMnB,CAAI,IAAJ,OAAI,MAAJ,WA0MI,OAAO,IAAI,EAAS,KAAK,MAAM,OAAQ,CACrC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,MACjD,OAAQ,KAAK,WA5MnB,CAAI,IAAJ,OAAI,MAAJ,WAiNI,OAAO,IAAI,EAAS,KAAK,MAAM,OAAQ,CACrC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,MACjD,OAAQ,KAAK,WAnNnB,CAAI,IAAJ,OAAI,MAAJ,WAwNI,OAAO,IAAI,EAAS,KAAK,MAAM,OAAQ,CACrC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,MACjD,OAAQ,KAAK,WA1NnB,CAAI,IAAJ,OAAI,MAAJ,WA+NI,OAAO,IAAI,EAAS,KAAK,MAAM,OAAQ,CACrC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,MACjD,OAAQ,KAAK,WAjOnB,CAAI,IAAJ,OAAI,MAAJ,WAsOU,MAAO,KAAK,MAAM,OACxB,OAAO,IAAI,EAAS,EAAM,CACxB,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,KAAM,GACvD,OAAQ,KAAK,WAzOnB,CAAI,IAAJ,QAAI,MAAJ,WA8OI,OAAO,IAAI,EAAS,KAAK,MAAM,QAAS,CACtC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAU,MAClD,OAAQ,KAAK,WAhPnB,CAAI,IAAJ,QAAI,MAAJ,WAqPU,OAAC,IAAI,EAAS,KAAK,MAAM,QAAS,CACtC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAU,MAClD,OAAQ,KAAK,WAvPnB,CAAI,IAAJ,QAAI,MAAJ,WA4PI,OAAO,IAAI,EAAS,KAAK,MAAM,QAAS,CACtC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAU,MAClD,OAAQ,KAAK,WA9PnB,CAAI,IAAJ,UAAI,MAAJ,WAmQI,IAAM,EAAU,KAAK,MAAM,UAC3B,OAAO,IAAI,EAAS,EAAS,CAC3B,SAAU,KAAK,YAAS,EAAY,IAAI,GAAY,KAAM,GAC1D,OAAQ,KAAK,WAtQnB,CAAI,IAAJ,cAAI,MAAJ,SA0Qc,EAAe,GACzB,MAAM,IAAI,MAAM,6BA3QpB,CAAI,IAAJ,OAAI,MAAJ,WAgRI,OAAO,IAAI,EAAS,KAAK,MAAM,UAhRnC,CAAI,IAAJ,SAAI,MAAJ,WAoRI,OAAO,IAAI,EAAS,KAAK,MAAM,SAAU,CACvC,SAAU,KAAK,YAAS,EAAY,IAAI,GAAW,MACnD,OAAQ,KAAK,WAtRnB,CAAI,IAAJ,oBAAI,MAAJ,SA0RoB,EAAgB,GAC1B,OAAC,IAAI,EAAS,KAAK,MAAM,kBAAkB,EAAQ,GAAM,CACrD,SAAE,KAAK,YACX,EACA,IAAI,GAAsB,KAAM,GACpC,OAAQ,KAAK,WA/RnB,CAAI,IAAJ,cAAI,MAAJ,SAmSc,EAAe,GACzB,OAAO,IAAI,EAAS,KAAK,MAAM,YAAY,EAAO,GAAS,CACzD,SAAU,KAAK,YACX,EACA,IAAI,GAAgB,KAAM,EAAO,GAC7B,YAAK,WAxSnB,CAAI,IAAJ,YAAI,MAAJ,SA4SY,EAAsB,GAC9B,MAAM,IAAI,MAAM,6BA7SpB,CAAI,IAAJ,SAAI,MAAJ,SAgTS,GACL,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,iDAGZ,MAAS,KAAK,QAAU,EAAO,OAErC,OAAO,IAAI,EAAS,KAAK,MAAM,OAAO,EAAO,OAAQ,CACnD,SAAU,OAAS,EAAY,IAAI,GAAW,KAAM,GACpD,aAzTN,CAAI,IAAJ,SAAI,MAAJ,SA6TS,EAAsB,GAC3B,KAAM,aAAkB,GACtB,MAAM,IAAI,MAAM,iDAGZ,MAAS,KAAK,QAAU,EAAO,OAErC,OAAO,IAAI,EAAS,KAAK,MAAM,OAAO,EAAO,MAAO,GAAO,CACzD,SAAU,OAAS,EAAY,IAAI,GAAW,KAAM,EAAQ,GAC5D,aAtUN,CAAI,IAAJ,OAAI,MAAJ,SA0UO,EAAc,GACjB,OAAO,IAAI,EAAS,KAAK,MAAM,KAAK,EAAK,GAAM,CAC7C,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,KAAM,EAAK,GAC5D,OAAQ,KAAK,WA7UnB,CAAI,IAAJ,eAAI,MAAJ,SAiVe,EAAoB,EAAc,GAC7C,MAAM,IAAI,MAAM,gDAlVpB,CAAI,IAAJ,SAAI,MAAJ,SAqVS,GACC,OAAC,IAAI,EAAS,KAAK,MAAM,OAAO,GAAU,CAC9C,SAAU,KAAK,YAAS,EAAY,IAAI,GAAW,KAAM,GACzD,OAAQ,KAAK,WAxVnB,CAAI,IAAJ,SAAI,MAAJ,SA4VS,GACC,OAAC,IAAI,EAAS,KAAK,MAAM,OAAO,GAAQ,CAC5C,SAAU,KAAK,YAAS,EAAY,IAAI,GAAW,KAAM,GACzD,OAAQ,KAAK,WA/VnB,CAAI,IAAJ,OAAI,MAAJ,WAoWI,OAAO,IAAI,EAAS,KAAK,MAAM,OAAQ,CACrC,UAAoB,IAAd,KAAK,KAAqB,KAAK,KAAK,YAAS,MArWzD,CAAI,IAAJ,SAAI,MAAJ,SAyWS,EAAc,GACb,UAAI,MAAM,6BA1WpB,CAAI,IAAJ,QAAI,MAAJ,WA8WI,OAAO,IAAI,EAAS,KAAK,MAAM,WA9WnC,CAAI,IAAJ,OAAI,MAAJ,WAkXU,OAAC,IAAI,EAAS,KAAK,MAAM,UAlXnC,CAAI,IAAJ,QAAI,MAAJ,WAsXI,OAAO,IAAI,EAAS,KAAK,MAAM,WAtXnC,CAAI,IAAJ,WAAI,MAAJ,SAyXW,GACP,MAAM,IAAI,MAAM,6BA1XpB,CAAI,IAAJ,YAAI,MAAJ,SA8XI,EACA,EACA,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BApYpB,CAAI,IAAJ,WAAI,MAAJ,SAwYI,EACA,EACA,EACA,EACA,GAEM,kBAAkB,MAAe,aAAc,GACnD,MAAM,IAAI,MAAM,mDAGlB,IAAM,EAAS,EAAG,QAAU,EAAO,OAE7B,OAAC,IAAI,EACT,EAAG,MAAM,SAAS,EAAG,MAAO,EAAO,MAAO,EAAa,EAAO,GAC9D,CACE,SAAU,OACN,EACA,IAAI,GAAQ,EAAI,EAAQ,EAAa,EAAO,GAChD,aA1ZR,CAAI,IAAJ,gBAAI,MAAJ,SAgaI,EACA,EACA,EACA,EACA,GAEM,kBAAkB,MAAe,aAAc,GACnD,MAAM,IAAI,MAAM,mDAGlB,IAAM,EAAS,EAAG,QAAU,EAAO,OAEnC,OAAO,IAAI,EACT,EAAG,MAAM,cAAc,EAAG,MAAO,EAAO,MAAO,EAAa,EAAO,GACnE,CACE,SAAU,OACN,EACA,IAAI,GAAa,EAAI,EAAQ,EAAa,EAAO,GACrD,aAlbR,CAAI,IAAJ,gBAAI,MAAJ,SAwbI,EACA,EACA,EACA,GAEM,kBAAkB,MAAe,aAAc,GACnD,MAAM,IAAI,MAAM,mDAGlB,IAAM,EAAS,EAAG,QAAU,EAAO,OAE7B,OAAC,IAAI,EACT,EAAG,MAAM,cAAc,EAAG,MAAO,EAAO,MAAO,EAAa,GAC5D,CACE,SAAU,OACN,EACA,IAAI,GAAa,EAAI,EAAQ,EAAa,GAC9C,aAzcR,CAAI,IAAJ,cAAI,MAAJ,SA+cI,EACA,EACA,EACA,GAEA,KAAM,aAAkB,MAAe,aAAc,GACnD,MAAM,IAAI,MAAM,sDAGlB,IAAM,EAAY,EAAG,MAAM,YACzB,EAAG,MACH,EAAO,MACP,EACA,GAEI,EAAS,EAAG,QAAU,EAAO,OAEnC,OAAO,IAAI,EAAS,EAAW,CAC7B,SAAU,OACN,EACA,IAAI,GAAW,EAAI,EAAQ,EAAW,EAAa,GACvD,aApeN,CAAI,IAAJ,aAAI,MAAJ,SAyeI,EACA,EACA,GAEA,KAAM,aAAkB,MAAe,aAAc,GACnD,MAAM,IAAI,MACR,6DAIJ,IAAM,EAAc,EAAG,MAAM,WAC3B,EAAG,MACH,EAAO,MACP,GAGI,EAAS,EAAG,QAAU,EAAO,OAEnC,OAAO,IAAI,EAAS,EAAa,CAC/B,SAAU,OACN,EACA,IAAI,GAAU,EAAI,EAAQ,EAAa,GAC3C,aA/fN,CAAI,IAAJ,YAAI,MAAJ,SAogBI,EACA,EACA,EACA,EACA,EACA,GAEA,KACI,aAAa,SACR,IAAN,KAAqB,aAAa,GAEnC,MAAM,IAAI,MAAM,0CAGlB,IAAM,EACJ,KAAK,QAAU,EAAE,cAAiB,IAAN,GAAkB,EAAE,QAElD,OAAO,IAAI,EACT,KAAK,MAAM,UACT,EAAE,MACF,EACA,EACA,EACA,OACM,IAAN,EAAkB,EAAE,WAAQ,GAE9B,CACE,SAAU,OACN,EACA,IAAI,GAAS,KAAM,EAAG,EAAY,EAAY,EAAO,EAAM,GAC/D,aAliBR,CAAI,IAAJ,WAAI,MAAJ,SAuiBqB,EAAgB,GACjC,OAAO,IAAI,EAAS,KAAK,MAAM,IAAI,EAAM,GAAW,CAClD,SAAU,KAAK,YAAS,EAAY,IAAI,GAAQ,KAAM,EAAM,GAC5D,OAAQ,KAAK,WA1iBnB,CAAI,IAAJ,iBAAI,MAAJ,SA8iB2B,EAAgB,GACvC,OAAO,IAAI,EAAS,KAAK,MAAM,UAAU,EAAM,GAAW,CACxD,SAAU,KAAK,YACX,EACA,IAAI,GAAc,KAAM,EAAM,GAClC,OAAQ,KAAK,WAnjBnB,CAAI,IAAJ,eAAI,MAAJ,SAujByB,EAAgB,GACrC,IAAM,EAAU,KAAK,MAAM,QAAQ,EAAM,GACzC,OAAO,IAAI,EAAS,EAAS,CAC3B,SAAU,KAAK,YACX,EACA,IAAI,GAAY,KAAM,EAAS,EAAM,GACzC,OAAQ,KAAK,WA7jBnB,CAAI,IAAJ,WAAI,MAAJ,SAikBqB,EAAgB,GACjC,MAAM,IAAI,MAAM,6BAlkBpB,CAAI,IAAJ,WAAI,MAAJ,SAokBqB,EAAgB,GACjC,MAAM,IAAI,MAAM,6BArkBpB,CAAI,IAAJ,kBAAI,MAAJ,SAwkB4B,EAAgB,GACxC,OAAO,IAAI,EAAS,KAAK,MAAM,WAAW,EAAM,GAAW,CACzD,SAAU,KAAK,YAAS,EAAY,IAAI,GAAS,KAAM,EAAM,GAC7D,OAAQ,KAAK,WA3kBnB,CAAI,IAAJ,wBAAI,MAAJ,SAglBI,EACA,GAEA,OAAO,IAAI,EAAS,KAAK,MAAM,iBAAiB,EAAM,GAAW,CAC/D,SAAU,KAAK,YACX,EACA,IAAI,GAAe,KAAM,EAAM,GACnC,OAAQ,KAAK,WAvlBnB,CAAI,IAAJ,oBAAI,MAAJ,SA2lB8B,EAAgB,GAC1C,OAAO,IAAI,EAAS,KAAK,MAAM,aAAa,EAAM,GAAW,CAC3D,SAAU,KAAK,YAAS,EAAY,IAAI,GAAW,KAAM,EAAM,GAC/D,OAAQ,KAAK,WA9lBnB,CAAI,IAAJ,uBAAI,MAAJ,SAmmBI,EACA,GAEA,OAAO,IAAI,EAAS,KAAK,MAAM,gBAAgB,EAAM,GAAW,CAC9D,SAAU,KAAK,YACX,EACA,IAAI,GAAc,KAAM,EAAM,GAClC,OAAQ,KAAK,WA1mBnB,CAAI,IAAJ,YAAI,MAAJ,SA+mBI,EACA,EACA,EACA,EACA,EACA,EACA,GAEA,KACI,aAAkB,SACV,IAAT,KAAwB,aAAgB,GAEzC,MAAM,IAAI,MACR,4DAIJ,GAAmB,OAAf,EACF,MAAM,IAAI,MAAM,0DAGlB,IAAM,EACJ,KAAK,QAAU,EAAO,cAAoB,IAAT,GAAqB,EAAK,QAE7D,OAAO,IAAI,EACT,KAAK,MAAM,KACT,EAAO,WACE,IAAT,EAAqB,EAAK,WAAQ,EAClC,EACA,EACA,EACA,GAEF,CACE,SAAU,OACN,EACA,IAAI,GAAS,KAAM,EAAQ,EAAS,EAAM,EAAW,EAAO,GAChE,aAppBR,CAAI,IAAJ,qBAAI,MAAJ,SA0pBI,EACA,EACA,EACA,EACA,GAEA,MAAM,IAAI,MAAM,6BAhqBpB,CAAI,IAAJ,WAAI,MAAJ,SAoqBI,EACA,EACA,GAEA,OAAO,IAAI,EAAS,KAAK,MAAM,IAAI,EAAM,EAAM,GAAQ,CACrD,SAAU,KAAK,YAAS,EAAY,IAAI,GAAQ,KAAM,EAAM,EAAM,GAClE,OAAQ,KAAK,WA1qBnB,CAAI,IAAJ,mBAAI,MAAJ,SA+qBI,EACA,EACA,EACA,GAEA,OAAO,IAAI,EACT,KAAK,MAAM,YAAY,EAAa,EAAM,EAAS,GACnD,CACE,SAAU,KAAK,YACX,EACA,IAAI,GAAgB,KAAM,EAAa,EAAM,EAAS,GAC1D,OAAQ,KAAK,WA1rBrB,CAAI,IAAJ,iBAAI,MAAJ,SA+rB2B,GACvB,OAAO,IAAI,EAAS,KAAK,MAAM,UAAU,GAAc,CACrD,SAAU,KAAK,YAAS,EAAY,IAAI,GAAc,KAAM,GAC5D,OAAQ,KAAK,WAlsBnB,CAAI,IAAJ,aAAI,MAAJ,SAusBI,EACA,EACA,EACA,GAEA,OAAO,IAAI,EAAS,KAAK,MAAM,MAAM,EAAQ,EAAM,EAAM,GAAQ,CAC/D,SAAU,KAAK,YACX,EACA,IAAI,GAAU,KAAM,EAAQ,EAAM,EAAM,GAC5C,OAAQ,KAAK,YAhtBnB,EAAI,IAAJ,SAAI,MAAJ,SA6BI,EACA,EACA,EACA,EACA,GAmBA,YAjBc,IAAV,IACF,EAAQ,WAgBH,IAAI,EAZK,QAAZ,EACM,IAAI,EAAU,EAAO,GACR,SAAZ,EACD,IAAI,GACV,EACA,IAAI,YAAY,GAChB,GAGM,IAAI,GAAU,EAAQ,EAAO,GAGZ,KApD/B,CAAI,IAAJ,WAAI,MAAJ,SA2DI,EACA,GAIA,OAAO,IAAI,EAFI,GAAU,SAAS,GAEN,OAhEhC,GACU,G,uSCjDJ,SAAgB,GACpB,G,gIAEI,aAAkB,I,iBACA,O,KAAT,G,SAAe,GAAM,EAAO,O,2BACf,IAAhB,EAAO,K,iBAAqB,O,SAAM,GAAM,EAAO,M,qDAAQ,E,+BAA7D,K,0DAEO,aAAkB,I,iBAEzB,O,KADS,G,UACH,GAAM,EAAO,Q,QACnB,O,sBAAM,GAAM,EAAO,S,gCACnB,EAAO,M,KACP,EAAO,U,8DAGP,aAAkB,G,0CACb,G,QAEM,O,UAAM,EAAO,Y,eAAtB,E,yBACC,IAAI,EAAU,EAAO,WAAY,EAAQ,EAAO,Q,4CAGnD,SAAgB,GACpB,G,8HAEqB,YAAjB,EAAO,M,sBACH,IAAI,MAAM,iD,YAEd,aAAkB,I,iBACA,O,KAAT,G,SAAe,GAAO,EAAO,O,2BAChB,IAAhB,EAAO,K,iBAAqB,O,UAAM,GAAO,EAAO,M,sDAAQ,E,+BAA9D,K,0DAEO,aAAkB,I,iBAEzB,O,KADS,G,UACH,GAAO,EAAO,Q,QACpB,O,sBAAM,GAAO,EAAO,S,gCACpB,EAAO,M,KACP,EAAO,U,8DAGP,aAAkB,I,0CACb,G,QAEM,O,UAAM,EAAO,Y,WAAtB,E,SACF,aAAkB,GAAa,aAAkB,Y,0CAC5C,G,iCAGF,IAAI,GACT,MAAM,KAAK,GACX,IAAI,YAAY,EAAO,YACvB,EAAO,Q,4CAIL,SAAgB,GACpB,G,8HAEqB,YAAjB,EAAO,M,sBACH,IAAI,MAAM,kD,YAEd,aAAkB,I,iBACA,O,KAAT,G,SAAe,GAAM,EAAOiB,O,2BACf,IAAhB,EAAO,K,iBAAqB,O,UAAM,GAAM,EAAO,M,sDAAQ,E,+BAA7D,K,0DAEO,aAAkB,I,iBAEzB,O,KADS,G,UACH,GAAM,EAAO,Q,QACnB,O,sBAAM,GAAM,EAAO,S,gCACnB,EAAO,M,KACP,EAAO,U,8DAGP,aAAkB,I,0CACb,G,QAEM,O,UAAM,EAAO,Y,WAAtB,E,SACF,aAAkB,GAAa,aAAkB,Y,0CAC5C,G,iCAEF,IAAI,GACT,MAAM,KAAK,GACX,EAAO,WACP,EAAO,Q,4CAOL,SAAU,GACd,EACA,GAEA,OAAK,EAAE,QAAmB,EAAE,QAGxB,aAAa,IAAY,aAAa,GACjC,GAAS,EAAE,MAAO,EAAE,OAEzB,aAAa,GAAa,aAAa,IAGvC,aAAa,IAAc,aAAa,IAGxC,aAAa,IAAa,aAAa,K,wUCrHvB,GAAtB,4CACS,aAAmB,MAEnB,UAAa,QAHtB,4DASI,IADA,IAAM,EAAoB,GAC1B,MAAgB,OAAO,KAAK,MAA5B,eAAmC,CAA9B,IAAM,EAAC,KAEN,KAAK,aAAc,GAErB,EAAQ,KAAK,KAAK,IAGhB,OAAC,IAhBX,sCAsBI,IAFA,IAAI,EAA8B,GAElC,MAAgB,OAAO,KAAK,MAA5B,eAAmC,CAA9B,IAAM,EAAC,KAEN,KAAK,aAAc,IAErB,EAAW,KAAK,KAAK,IAIzB,IAXW,EAWL,EAAU,KAAK,gBAXV,cAYU,GAZV,IAYX,2BAA8B,KACtB,EADsB,QACN,gBACtB,EAAa,EAAW,OAAO,IAdtB,8BAiBX,OAAO,IApCX,gCAuCYC,GACF,MAAU,QAAZ,EACK,KAAK,QACS,SAAZ,EACF,KAAK,SAEL,KAAK,UA7ClB,8B,uIAkDU,EAAa,KAAK,gB,cACA,G,yDACtB,OADS,E,iBACH,EAAU,Q,oJAGF,OAAO,KAAK,M,+CAEtB,KAFK,E,gBAEc,G,iBAEX,O,UAAM,GAAM,KAAK,I,QAA3B,KAAK,G,2CAIT,KAAK,QAAU,M,kEA/DnB,+B,uIAmEU,EAAa,KAAK,gB,cACA,G,yDACtB,OADS,E,iBACH,EAAU,S,oJAGF,OAAO,KAAK,M,+CAEtB,KAFK,E,gBAEc,G,iBAEX,O,UAAM,GAAO,KAAK,I,QAA5B,KAAK,G,2CAIT,KAAK,QAAU,O,kEAhFnB,8B,uIAoFU,EAAa,KAAK,gB,cACA,G,yDACtB,OADS,E,iBACH,EAAU,Q,oJAGF,OAAO,KAAK,M,+CAEtB,KAFK,E,gBAEc,G,iBAEX,O,UAAM,GAAM,KAAK,I,QAA3B,KAAK,G,2CAIT,KAAK,QAAU,M,oEAjGnB,K,uSCJsB,GAAtB,kDASI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,2BAEN,EAAJ,cAVK,WAAqD,GAYtD,EAAC,KAAO,EAEZ,IAAK,IAAI,EAAI,EAAG,EAAI,EAAW,OAAQ,IACrC,EAAK,WAAW,EAAW,GAAG,MAAkB,EAAW,GAE7D,EAAK,OAAS,EACV,EAAC,QAAU,EAEf,EAAK,YAAc,EAEf,EAAC,eAAiB,EAClB,IAAC,IAAI,EAAI,EAAG,EAAI,EAAK,OAAO,OAAQ,SACJ,IAA9B,EAAU,EAAK,OAAO,KAChB,EAAH,iBAjBC,SAfd,uDAsCa,MAtCb,mCAwCe,GACX,OAAO,KAAK,WAAW,KAzC3B,yCA4CqB,GACX,MAAO,KAAK,WAAW,GACvB,QAAO,IAAT,EAAoB,CACd,IAAF,EAAM,EAAK,EACT,YAAI,IAAR,GAA6B,OAAR,EAEhB,IAAI,YAAY,SAAS,OAAO,QAEzC,KApDN,uCAyDmB,GAET,QAAO,IADA,KAAK,WAAW,GACL,CACtB,IAAM,EAAS,KAAK,WAAW,GAAM,KACrC,QAAe,IAAX,GAAmC,OAAX,EAAiB,CAC3C,IAAK,IAAI,EAAI,EAAG,EAAI,EAAO,OAAQ,IAC7B,KAAK,OAAO,EAAO,MACrB,EAAO,GAAM,EAAO,GAAY,YAGpC,OAAO,MAnEf,sCAyEkB,GACd,IAAM,EAAO,KAAK,WAAW,GAC7B,QAAa,IAAT,EAAoB,CACtB,IAAI,EAAS,EAAK,EAIlB,OAHI,KAAK,OAAO,KACd,EAAU,EAAgB,YAErB,KAhFb,wCAqFoB,GAChB,IAAM,EAAO,KAAK,WAAW,GAC7B,QAAa,IAAT,EAEF,OADe,EAAK,IAxF1B,yCA8FqB,GACX,MAAO,KAAK,WAAW,GACvB,QAAO,IAAT,EAEM,OADO,EAAK,SAjG1B,yCAuGqB,GACjB,IAAM,EAAO,KAAK,WAAW,GAC7B,QAAa,IAAT,EAEF,OADe,EAAK,IA1G1B,+BAgHqC,G,kIAC3B,aAAkB,E,gBAEb,OADT,QAAQ,KAAK,mD,SACE,GAAM,G,OAArB,E,cAMF,IAHM,EAAK,EAEL,EAAS,IAAI,MAAM,EAAG,MACnB,EAAI,EAAG,EAAI,EAAG,KAAM,IAC3B,EAAO,GAAK,EAAG,IAAI,G,yBAEd,G,6CA5HX,8B,uCA+HiB,oHA/HjB,+B,uCAgIkB,oHAhIlB,8B,uCAiIiB,sHAjIjB,GAAuC,ICFjC,SAAU,GACd,GACkB,IAAlB,EAAkB,wDAEhB,QAA0B,IAAxB,EAAY,SAAiD,OAAxB,EAAY,QACnD,MAAM,IAAI,MAAM,wDAGhB,IAAE,EAAkB,EAAY,KAChC,QAAY,IAAV,GAAiC,OAAV,EACzB,MAAM,IAAI,MAAM,kCAEhB,IAAG,IAAI,EAAI,EAAG,EAAI,EAAM,OAAQ,IAC5B,KAAK,OAAO,EAAM,MAEpB,EAAM,GAAM,EAAM,GAAW,YAGZ,IAAjB,EAAM,SACJ,EAAI,CAAC,IAGT,IAAI,EAAO,EAAQ,GAEnB,GCfwB,IDetB,EAAY,SAA2B,CACrC,KAAY,WAAa,EAAY,UAAU,OAAS,EAC1D,OAAO,IAAI,EAAU,EAAO,EAAY,WACnC,GAAI,EAAY,SAAW,EAAY,QAAQ,OAAS,EAAG,CAC1D,MAAS,EAAY,QAAQ,OAAO,MACxC,EAAY,QAAQ,WACpB,EAAY,QAAQ,WAAa,EAAY,QAAQ,YAEjD,EAAS,IAAI,aAAa,GAC1B,OAAC,IAAI,EAAU,EAAO,EAAQ,EAAa,UAAY,WACxD,GAAa,IAAT,EACH,OAAC,IAAI,EAAU,GAEf,UAAI,MAAM,uDAEb,GCxBmB,IDwBf,EAAY,SAA2B,CAC5C,KAAY,SAAW,EAAY,QAAQ,OAAS,EAAG,CAEzD,IADM,MAAS,IAAI,WAAW,EAAY,QAAQ,OAAS,GAClD,EAAI,EAAG,EAAI,EAAY,QAAQ,OAAQ,GAAK,EAAG,CACtD,IAAM,EAAQ,KAAK,YACjB,MAAM,KAAK,EAAY,QAAQ,MAAM,EAAG,EAAI,KAC5C,WACF,EAAO,EAAI,GAAK,EAGlB,OAAO,IAAI,EAAU,EAAO,EAAQ,SAEpC,MAAM,IAAI,MAAM,8CAGlB,MAAM,IAAI,MAAJ,kCACuB,EAAY,SADnC,yB,2SEtDG,GAAb,kDAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,0BAIN,IAAE,GAFF,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEvC,mBAAmB,SACnC,QAAW,IAAX,GAAmC,OAAX,EAO1B,MAAM,IAAI,MACR,sEAbM,OAMR,EAAK,OAAS,GAAa,GAEd,UAAT,QAAoC,IAAhB,EAAK,SAC3B,EAAK,OAAS,IAAI,GAAS,EAAK,SAT1B,EATd,oDA4BgB,G,6HACQ,IAAhB,KAAK,O,yCACA,CAAC,KAAK,S,aAET,IAAI,MAAM,6C,kDAhCpB,8B,6HAoCwB,IAAhB,KAAK,O,gBACO,O,SAAM,GAAM,KAAK,Q,OAA/B,KAAK,O,yDArCX,+B,6HA0CwB,IAAhB,KAAK,O,gBACO,O,SAAM,GAAO,KAAK,Q,OAAhC,KAAK,O,yDA3CX,8B,6HAgDwB,IAAhB,KAAK,O,gBACO,O,SAAM,GAAM,KAAK,Q,OAA/B,KAAK,O,yDAjDX,gCAsDU,MAAC,aAtDX,oCA0DwB,IAAhB,KAAK,QACP,KAAK,OAAO,aA3DlB,GAAkC,I,uSCDrB,GAAb,YAAE,qBAAF,iBAWI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,EACA,EACA,EACA,GAAuB,MAKnB,GALmB,yBAKP,KAHZ,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtC,mBAAmB,WAEhC,UAAI,MAAM,qCANK,YASJ,IAAf,IACF,EAAa,MAEX,EAAC,WAAa,EAEd,EAAC,MAAQ,EAAK,gBAAgB,UAAY,EAC1C,EAAC,UAAY,EAAK,iBAAiB,aACnC,EAAC,KAAO,EAAK,iBAAiB,QAC9B,EAAC,QAAU,EAAK,iBAAiB,WAErC,EAAK,OAAS,EACV,EAAC,KAAO,EACC,UAAT,QAAoC,IAAhB,EAAK,SAC3B,EAAK,OAAS,IAAI,GAAS,EAAK,SAErB,UAAT,QAAkC,IAAd,EAAK,OAC3B,EAAK,KAAO,IAAI,GAAS,EAAK,OAzBT,EApBzB,uBAAE,IAAJ,UAAI,MAAJ,SAkDI,G,sIAEM,EAAI,EAAO,GACX,OAAoB,IAAhB,KAAK,OAAuB,KAAK,OAAS,EAAO,GACrD,EAAI,EAAO,OAAS,EAAI,EAAO,GAAK,KAAK,K,kBAExC,CACL,EAAE,KACA,EACA,EACA,KAAK,UACL,KAAK,MACL,KAAK,KACL,KAAK,QACL,KAAK,c,kDAhEb,CAAI,IAAJ,eAAI,MAAJ,SAqEe,GACX,YAAuB,IAAnB,KAAK,UACA,KAAK,UAEP,IAAI,MAAM,GAAM,KAAK,KAzEhC,CAAI,IAAJ,UAAI,MAAJ,SA4EU,GACA,YAAY,IAAd,KAAK,KACA,KAAK,KAEP,IAAI,MAAa,EAAP,GAAU,KAAK,KAhFpC,CAAI,IAAJ,aAAI,MAAJ,SAmFa,GACH,YAAe,IAAjB,KAAK,QACA,KAAK,QAEP,IAAI,MAAM,GAAM,KAAK,KAvFhC,CAAI,IAAJ,UAAI,MAAJ,WA2FI,MAAO,SA3FX,CAAI,IAAJ,QAAI,MAAJ,W,6HA+FwB,IAAhB,KAAK,O,gBACO,O,SAAM,GAAM,KAAK,Q,OAA/B,KAAK,O,sBAEW,IAAd,KAAK,K,gBACK,O,SAAM,GAAM,KAAK,M,OAA7B,KAAK,K,yDAnGX,CAAI,IAAJ,SAAI,MAAJ,W,6HAwGwB,IAAhB,KAAK,O,gBACO,O,SAAM,GAAO,KAAK,Q,OAAhC,KAAK,O,sBAEW,IAAd,KAAK,K,gBACK,O,SAAM,GAAO,KAAK,M,OAA9B,KAAK,K,yDA5GX,CAAI,IAAJ,QAAI,MAAJ,W,6HAiHwB,IAAhB,KAAK,O,gBACO,O,SAAM,GAAM,KAAK,Q,OAA/B,KAAK,O,sBAEW,IAAd,KAAK,K,gBACK,O,SAAM,GAAM,KAAK,M,OAA7B,KAAK,K,yDArHX,CAAI,IAAJ,SAAI,MAAJ,gBA0HwB,IAAhB,KAAK,QACP,KAAK,OAAO,cAEI,IAAd,KAAK,MACP,KAAK,KAAK,aA9HhB,GAA8B,ICiBR,GAAtB,YAAE,qBAAF,iBAGI,SAAF,EAAY,GAAmB,kCACzB,EAAJ,cACK,UAAY,EAFY,EAH/B,uBAAE,IAAJ,mBAAI,MAAJ,SAQmB,GACT,MAAsB,GAEtB,EAAQ,EAAM,WAEd,IAAD,IAAM,KAAU,OAAO,KAAK,GAAQ,CAC/B,IAAF,EAAO,EAAM,GAEX,QAAK,IAAT,GAAsB,EAAK,YAAc,KAAK,UAAU,GAAI,CAC9D,IAAM,EAAM,KAAK,iBAAiB,EAAO,QAC7B,IAAR,GAEF,EAAQ,KAAK,IAKnB,OAAO,IAzBX,CAAI,IAAJ,mBAAI,MAAJ,SA4BmB,EAAmB,GAOlC,IANA,IAAM,EAAQ,EAAM,WAEd,EAAU,CAAC,GAEb,EAAW,EAAM,GACf,EAAgB,CAAC,GACd,EAAI,EAAG,EAAI,KAAK,UAAU,OAAQ,IAAK,CAC9C,IAAM,EAAa,EAAM,iBAAiB,EAAS,QAAQ,IAC3D,QAAmB,IAAf,EAYF,OAXA,IAAM,EAAW,EAAM,GAEvB,GAAI,EAAS,YAAc,KAAK,UAAU,GAMxC,OAJA,EAAQ,KAAK,GACb,EAAW,EACX,EAAc,KAAK,GAQzB,OAAI,KAAK,SAAS,GACT,OAEP,IAvDN,CAAI,IAAJ,WAAI,MAAJ,SA4DW,GACP,OAAO,MA7DX,IAnBA,aAAE,uBCEW,GAAb,YAAE,qBAAF,iBACI,SAAF,IAAI,2BAAJ,YACQ,CAAC,OAAQ,uBAFjB,uBAAE,IAAJ,QAAI,MAAJ,SAMI,EACA,EACA,EACA,GAEA,IAAM,EAAO,EAAM,GACb,EAAY,EAAM,GAElB,EAAa,EAAgB,EAAK,OAAO,IACzC,EAAW,EAAgB,EAAK,OAAO,IAEvC,EAAU,EAAgB,EAAU,OAAO,IAC3C,EAAS,EAAgB,EAAU,OAAO,IAC1C,EAAS,EAAgB,EAAU,OAAO,IAG1C,EAFa,EAAgB,EAAU,OAAO,IAEzB,IAAI,EAAU,WAAW,OAE9C,EAAQ,EAAQ,OAAO,GAC7B,EAAQ,SACR,IAAM,EAAO,EAAO,SAAS,EAAO,SAAS,IAEvC,EAAQ,sBACT,EAAM,YADG,YAET,IAAI,MAAM,EAAW,WAAW,OAAS,EAAM,WAAW,QAAQ,KACnE,KAIE,EAAY,EAAW,SAAS,EAAM,QAAQ,GAAU,IAC1D,EAAU,EACd,QAAiB,IAAb,EAAwB,CAC1B,IAAM,EAAa,EAAS,SAAS,GACrC,EAAU,EAAQ,IAAI,GACtB,EAAW,SAGb,OAAO,IAAI,GACT,OAAO,QAAQ,EAAK,YAAY,KAAI,YAAC,OAAI,EAAE,MAC3C,CAAC,EAAK,OAAO,IACb,EAAU,QACV,EACA,EACA,EAAK,KACL,EACA,OAnDN,GAAmC,ICAtB,GAAb,YAAE,qBAAF,iBACI,SAAF,IAAI,2BAAJ,YACQ,CAAC,OAAQ,SAFjB,uBAAE,IAAJ,QAAI,MAAJ,SAMI,EACA,EACA,EACA,GAEA,IAAM,EAAO,EAAM,GACb,EAAO,EAAM,GAEnB,OAAO,IAAI,GACT,OAAO,QAAQ,EAAK,YAAY,KAAI,YAAC,OAAI,EAAE,MAC3C,EAAK,OACL,EAAK,QACL,EACA,EACA,EAAK,KACL,EAAK,OACL,EAAK,KACL,YAvBN,GAA8B,ICAjB,GAAb,YAAE,qBAAF,iBACI,SAAF,IAAI,2BAAJ,YACQ,CAAC,OAAQ,SAFjB,uBAAE,IAAJ,QAAI,MAAJ,SAMI,EACA,EACA,EACA,GAEA,IAAM,EAAO,EAAM,GACb,EAAO,EAAM,GAEnB,OAAO,IAAI,GACT,OAAO,QAAQ,EAAK,YAAY,KAAI,YAAC,OAAI,EAAE,MAC3C,EAAK,OACL,EAAK,QACL,EACA,EACA,EAAK,KACL,EAAK,OACL,EAAK,KACL,WAvBN,CAAI,IAAJ,WAAI,MAAJ,SA2BW,GACP,IAAM,EAAO,EAAM,GACnB,OAAoB,IAAb,EAAK,KAA0B,IAAb,EAAK,QA7BlC,GAA+B,ICFlB,GAAuC,CAClD,IAAI,GACJ,IAAI,GACJ,IAAI,I,uSCHgB,GAAtB,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,KAAO,EAJF,EAVZ,uBAAE,IAAJ,UAAI,MAAJ,SAuBI,G,kIAEI,KAAK,YAAc,IAAM,KAAK,aAAe,G,uBACzC,EAAI,EAAO,GACX,EAAI,EAAO,G,kBAEV,CAAC,KAAK,QAAQ,EAAG,K,aAEpB,IAAI,MAAJ,UACD,KAAK,KADJ,6CAC6C,KAAK,c,kDAhC5D,CAAI,IAAJ,UAAI,MAAJ,WAqCI,OAAO,KAAK,OArChB,CAAI,IAAJ,SAAI,MAAJ,mBAAyC,ICA5B,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,EAAiB,GAC3C,OAAO,EAAE,IAAI,OAbjB,GAA6B,I,uSCGhB,GAAb,kDAMI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,QAAU,EAAK,kBAAkB,YAAc,KAChD,EAAC,SAAW,EAAK,kBAAkB,aAAe,GAElD,EAAC,UAAY,IAAI,EAAU,CAAC,GAAI,CAAC,EAAK,UAC7B,UAAT,IACI,EAAD,UAAY,IAAI,GAAS,EAAK,YAT3B,EAZd,oDA4BI,G,gJAEM,EAAI,EAAO,GAEb,EAAQ,EAAO,GACf,EAAI,EAAO,GACX,EAAO,EAAO,GACd,EAAW,EAAO,GAIhB,EAAI,EAAM,WAAW,GAErB,E,CAAY,EAAG,G,mBAAM,IAAI,MAAM,EAAE,WAAW,OAAS,GAAG,KAAK,KAEnE,EAAQ,EAAM,QAAQ,GAAU,GAChC,EAAI,EAAE,QAAQ,GAAU,GACxB,EAAO,EAAK,QAAQ,GAAU,GAC9B,EAAW,EAAS,QAAQ,GAAU,GAEhC,EAAS,EAAE,UAAU,EAAM,EAAU,KAAK,QAAS,EAAO,G,kBAEzD,CAAC,I,mDAlDZ,gCAsDI,MAAO,uBAtDX,8B,qHA0DqB,O,SAAM,GAAM,KAAK,W,OAAlC,KAAK,U,yDA1DT,+B,qHA8DqB,O,SAAM,GAAO,KAAK,W,OAAnC,KAAK,U,yDA9DT,8B,qHAkEqB,O,SAAM,GAAM,KAAK,W,OAAlC,KAAK,U,yDAlET,+BAsEI,KAAK,UAAU,aAtEnB,GAA4C,I,uSCF/B,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAGtD,GAAK,EAAK,mBAAmB,MALxB,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAiBgB,G,kIACN,EAAI,EAAO,G,kBAGV,CAAC,EAAE,KAAK,KAAK,M,kDArBxB,CAAI,IAAJ,UAAI,MAAJ,WAyBI,MAAO,SAzBX,CAAI,IAAJ,SAAI,MAAJ,mBAA8B,I,uSCDR,GAAtB,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAYC,EAAQ,EAAS,EAAW,EAAa,IACtD,KAAO,EAHF,EAVZ,uBAAE,IAAJ,UAAI,MAAJ,SAmBI,G,kIAEM,EAAI,EAAO,G,kBAEV,CAAC,KAAK,QAAQ,K,kDAvBzB,CAAI,IAAJ,UAAI,MAAJ,WA2BI,OAAO,KAAK,OA3BhB,CAAI,IAAJ,SAAI,MAAJ,mBAAwC,ICA3B,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATrE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,I,uSCAjB,GAAb,YAAE,qBAAF,iBAIE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,iCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,GAEvD,EAAc,KAEhB,EAAK,IAAM,EAAK,kBAAkB,OAE5B,EAAD,IAAM,EAAK,kBAAkB,QAR1B,EAVZ,uBAAE,IAAJ,UAAI,MAAJ,SAsBgB,G,kIACN,EAAI,EAAO,KAEb,KAAK,YAAc,I,yCACd,CAAC,EAAE,KAAK,KAAK,IAAK,KAAK,O,UAExB,EAAM,EAAO,OAAS,EAAI,EAAO,QAAK,EACtC,EAAM,EAAO,OAAS,EAAI,EAAO,QAAK,OAChC,IAAR,QAA6B,IAAR,E,yCAChB,CAAC,EAAE,S,aAEN,IAAI,MAAM,oD,mDAjCtB,CAAI,IAAJ,UAAI,MAAJ,WAsCI,MAAO,SAtCX,CAAI,IAAJ,SAAI,MAAJ,mBAA8B,I,uSCAjB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,iCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,GAEvD,EAAc,KAEV,EAAD,KAAO,EAAK,gBAAgB,SANzB,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAoBI,G,kIAEI,EAAO,SAKP,KAAK,YAAc,SAAoB,IAAd,KAAK,M,gBAEhC,IADI,EAAS,EAAO,GACX,EAAI,EAAG,EAAI,EAAO,OAAQ,IAC3B,EAAS,EAAO,OAAO,EAAO,GAAI,KAAK,MACzC,EAAI,GACN,EAAO,SAET,EAAS,E,yBAGJ,CAAC,I,aAEJ,IAAI,MAAJ,kDACuC,KAAK,c,kDAxCtD,CAAI,IAAJ,UAAI,MAAJ,WA6CI,MAAO,WA7CX,CAAI,IAAJ,SAAI,MAAJ,mBAAgC,I,uSCKnB,GAAb,kDAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,MAIN,GAJM,oBAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,GAEvD,EAAc,GAAI,CACpB,IAAM,EAAS,EAAK,mBAAmB,SACxB,OAAX,QAA8B,IAAX,IACrB,EAAK,OAAS,GAAa,IAPrB,SATd,oDAqBgB,G,sIACN,EAAS,EAAO,KAElB,KAAK,YAAc,SAAsB,IAAhB,KAAK,Q,mBAC1B,aAAkB,E,sBAChB,IAAI,MAAM,oD,OAGlB,IADM,EAAQ,IAAI,MAAM,EAAO,MACtB,EAAI,EAAG,EAAI,EAAO,KAAM,IAC/B,EAAM,GAAK,EAAO,IAAI,G,OAGlB,EAAO,EAAQ,GACf,EAAS,IAAI,aAAa,GAAM,KAAK,KAAK,OAAO,IAAI,I,kBAEpD,CAAC,IAAI,EAAU,EAAO,EAAQ,KAAK,OAAO,S,aAE7C,IAAI,MAAJ,2DACgD,KAAK,c,mDAvC/D,gCA4CI,MAAO,oBA5CX,oCAgDwB,IAAhB,KAAK,QACP,KAAK,OAAO,aAjDlB,GAAyC,ICL5B,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,EAAiB,GAC3C,OAAO,EAAE,OAAO,OAbpB,GAA6B,ICAhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,UAbb,GAA6B,I,uSCChB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAYgB,G,sIACR,KAAK,YAAc,I,mBACf,EAAS,EAAO,IAEhB,EAAS,EAAO,cACE,E,sBAChB,IAAI,MAAM,2C,OAGlB,IADM,EAAQ,IAAI,MAAM,EAAO,MACtB,EAAI,EAAG,EAAI,EAAO,KAAM,IAC/B,EAAM,GAAK,EAAO,IAAI,G,yBAGjB,CAAC,EAAO,OAAO,K,aAElB,IAAI,MAAJ,sDAC2C,KAAK,c,kDA5B1D,CAAI,IAAJ,UAAI,MAAJ,WAiCI,MAAO,WAjCX,CAAI,IAAJ,SAAI,MAAJ,mBAAgC,ICDnB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,QAAS,GATtE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,YAbb,GAA+B,I,uSCClB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,KAAO,EAAK,gBAAgB,SAAW,EAJlC,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAgBgB,G,gIACN,EAAI,EAAO,IACX,EAAU,EAAO,cAEE,E,sBACjB,IAAI,MAAM,8C,gCAGX,CAAC,EAAE,OAAO,KAAK,KAAM,K,kDAxBhC,CAAI,IAAJ,UAAI,MAAJ,WA4BI,MAAO,WA5BX,CAAI,IAAJ,SAAI,MAAJ,mBAAgC,I,uSCDnB,GAAb,YAAE,qBAAF,iBAMI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,2BAEV,cAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,MAAQ,EAAK,kBAAkB,UAAY,EAC5C,EAAC,KAAO,EAAK,kBAAkB,SAAW,EAE1C,IAAE,EAAS,EAAK,gBAAgB,UAC9B,EAAS,EAAK,gBAAgB,UAR1B,OAUV,EAAK,OAAoB,IAAX,EACd,EAAK,OAAoB,IAAX,EAXJ,EAZZ,uBAAE,IAAJ,UAAI,MAAJ,SA2BI,G,sIAEM,EAAI,EAAO,GACX,EAAI,EAAO,GACX,EAAI,EAAO,G,kBAEV,CAAC,EAAE,KAAK,EAAG,KAAK,OAAQ,KAAK,OAAQ,KAAK,MAAO,EAAG,KAAK,Q,kDAjCpE,CAAI,IAAJ,UAAI,MAAJ,WAqCI,MAAO,SArCX,CAAI,IAAJ,SAAI,MAAJ,mBAA8B,I,uSCCjB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEV,cAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,QAAU,EAAK,kBAAkB,YAAc,KAJ1C,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAmBI,G,iJAgBA,IAdM,EAAI,EAAO,GACb,EAAQ,EAAO,GACf,EAAI,EAAO,GAET,EAAW,EAAE,WAAW,OAAS,EAEjC,EAAI,EAAM,WAAW,GAErB,E,CAAY,EAAG,G,mBAAM,IAAI,MAAM,GAAU,KAAK,KAEpD,EAAQ,EAAM,QAAQ,GAAU,GAChC,EAAI,EAAE,QAAQ,GAAU,GAElB,EAAa,IAAI,MAAM,EAAE,WAAW,OAAS,GAC1C,EAAI,EAAG,EAAI,EAAU,IAC5B,EAAW,GAAK,EAAI,E,OAGhB,EAAO,EAAE,WAAW,GAAY,GACtC,EAAU,QACJ,EAAO,EAAE,SAAS,GACxB,EAAU,QACJ,EAAW,EAAK,iBAAiB,GAAY,GACnD,EAAU,QAEJ,EAAS,EAAE,UAAU,EAAM,EAAU,KAAK,QAAS,EAAO,GAEhE,EAAK,SACL,EAAK,SACL,EAAS,S,kBAEF,CAAC,I,mDApDZ,CAAI,IAAJ,UAAI,MAAJ,WAwDI,MAAO,0BAxDX,CAAI,IAAJ,SAAI,MAAJ,mBAA+C,I,uSCDlC,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAaI,G,gIAEM,EAAI,EAAO,GACX,EAAI,EAAO,KAEb,KAAK,YAAc,I,mBACjB,EAAE,WAAW,SAAW,EAAE,WAAW,O,sBACjC,IAAI,MAAM,sD,gCAGX,CAAC,EAAE,KAAK,K,aAEX,IAAI,MAAH,4BAAD,OACwB,KAAK,YAD7B,yB,kDAzBV,CAAI,IAAJ,UAAI,MAAJ,WA+BI,MAAO,WA/BX,CAAI,IAAJ,SAAI,MAAJ,mBAAgC,ICAnB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,EAAiB,GAC3C,OAAO,EAAE,SAAS,OAbtB,GAA6B,I,uSCAhB,GAAb,YAAE,qBAAF,iBAKE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEV,cAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,QAAW,EAAK,mBAAmB,SAAW,WAE/C,EAAC,KAAO,EAAK,iBAAiB,QAC9B,EAAC,MAAQ,EAAK,kBAAkB,UAAY,EAPtC,EAXZ,uBAAE,IAAJ,UAAI,MAAJ,SAsBI,G,0HAEI,KAAK,YAAc,I,yCACd,CAAC,EAAO,GAAG,IAAI,KAAK,KAAM,KAAK,QAAS,KAAK,S,aAGhD,IAAI,MAAJ,+CAAkD,KAAK,c,kDA5BjE,CAAI,IAAJ,UAAI,MAAJ,WAgCI,MAAO,QAhCX,CAAI,IAAJ,SAAI,MAAJ,mBAA6B,I,uSCCP,GAAtB,YAAE,qBAAF,iBAMI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,EACA,GAAU,2BAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,KAAO,EAAK,iBAAiB,QAC9B,IAAE,EAAO,EAAK,gBAAgB,YALxB,OAON,EAAC,SAAoB,IAAT,QAAuB,IAAT,EAE1B,EAAC,KAAO,EATF,EAbZ,uBAAE,IAAJ,UAAI,MAAJ,SA2BwC,GAC9B,QAAY,IAAd,KAAK,KACP,OAAO,KAAK,KAKZ,IAHA,IAAM,EAAO,EAAM,WAAW,OAExB,EAAM,IAAI,MAAM,GACb,EAAI,EAAG,EAAI,EAAM,IACxB,EAAI,GAAK,EAEX,OAAO,IArCb,CAAI,IAAJ,UAAI,MAAJ,SA0CI,G,0HAEI,KAAK,YAAc,I,yCACd,CAAC,KAAK,KAAK,EAAO,M,aAErB,IAAI,MAAJ,UACD,KAAK,KADJ,gDACgD,KAAK,c,kDAhD/D,CAAI,IAAJ,UAAI,MAAJ,WAqDI,OAAO,KAAK,OArDhB,CAAI,IAAJ,SAAI,MAAJ,mBAAyC,ICD5B,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,YACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,OAAO,EAAM,IAAI,KAAK,KAAM,KAAK,cArBrC,GAAmC,ICAtB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,aACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,OAAO,EAAM,WAAW,KAAK,KAAM,KAAK,cArB5C,GAAoC,ICAvB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,YACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,OAAO,EAAM,IAAI,KAAK,KAAM,KAAK,cArBrC,GAAmC,ICAtB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,kBACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,OAAO,EAAM,UAAU,KAAK,KAAM,KAAK,cArB3C,GAAyC,ICD5B,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEV,cAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,SAAM,EACX,EAAK,IAAM,EALD,EAPZ,uBAAE,IAAJ,UAAI,MAAJ,WAgBI,MAAO,WAhBX,GAA8B,I,uSCEjB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAYgB,G,oIACN,EAAI,EAAO,IACX,EAAQ,EAAO,cAEE,E,sBACf,IAAI,MAAM,sD,YAGd,KAAK,YAAc,I,gBAErB,IADM,EAAS,IAAI,MAAM,EAAM,MACtB,EAAI,EAAG,EAAI,EAAM,KAAM,IAC9B,EAAO,GAAK,EAAM,IAAI,G,yBAGjB,CAAC,EAAE,QAAQ,K,aAEd,IAAI,MAAJ,oCACyB,KAAK,YAD9B,yB,kDA5BV,CAAI,IAAJ,UAAI,MAAJ,WAkCI,MAAO,YAlCX,CAAI,IAAJ,SAAI,MAAJ,mBAAiC,I,uSCApB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAYgB,G,kIACR,KAAK,YAAc,I,uBACf,EAAI,EAAO,GAEX,EAAQ,EAAE,W,kBAET,CAAC,IAAI,EAAU,CAAC,EAAM,QAArB,YAAkC,GAAQ,W,aAE9C,IAAI,MAAJ,iDACsC,KAAK,c,kDArBrD,CAAI,IAAJ,UAAI,MAAJ,WA0BI,MAAO,UA1BX,CAAI,IAAJ,SAAI,MAAJ,mBAA+B,I,uSCDlB,GAAb,YAAE,qBAAF,iBAKE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,KAAO,EAAK,iBAAiB,QAC9B,EAAC,OAAS,EAAK,iBAAiB,UAChC,EAAC,KAAO,EAAK,iBAAiB,QANxB,EAXZ,uBAAE,IAAJ,UAAI,MAAJ,SAoBgB,G,kJACR,KAAK,YAAc,I,wBACD,IAAhB,KAAK,aAAsC,IAAd,KAAK,K,sBAC9B,IAAI,MACR,4E,cAGE,EAAI,EAAO,G,kBACV,CAAC,EAAE,MAAM,KAAK,OAAQ,KAAK,KAAM,KAAK,Q,OAQzB,OANd,EAAI,EAAO,GACX,EAAS,EAAO,GAChB,EAAO,EAAO,GACd,EAAO,EAAO,GACd,EAAQ,EAAO,G,UAEK,KAAK,SAAS,G,QACtB,OADZ,E,iBACkB,KAAK,SAAS,G,WAAhC,E,YAEK,IAAT,E,iBAAqB,O,UAAM,KAAK,SAAS,G,sDAAQ,E,WAD7C,E,UAGM,IAAV,E,iBAAsB,O,UAAM,KAAK,SAAS,G,sDAAS,E,eAD/C,E,uBAGC,CAAC,EAAE,MAAM,EAAa,EAAW,EAAY,K,mDA3C1D,CAAI,IAAJ,UAAI,MAAJ,WAgDI,MAAO,UAhDX,CAAI,IAAJ,SAAI,MAAJ,mBAA+B,I,uSCAlB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEV,cAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAGtD,KAAO,EAAK,gBAAgB,QALvB,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAkBI,G,oJAEM,EAAI,EAAO,GAEX,EAAS,EAAE,gBAGN,KADP,EAAK,KAAK,QAGV,EADE,KAAK,YAAc,GAChB,EAEA,EAAO,OAAS,GAInB,EAAM,EAAO,MAAM,EAAG,GAAI,QAAO,SAAC,EAAG,GAAJ,OAAU,EAAI,IAAG,GAElD,EAAW,EAAE,QAAQ,CAAC,GAAM,IAAI,GAEhC,EAAM,EAAS,IAAI,GAAG,GACtB,EAAa,EAAS,SAAS,GAC/B,EAAM,EAAW,MACjB,EAAM,EAAI,IAAI,GAAG,GACjB,EAAS,EAAI,OAAO,GAE1B,EAAI,SACJ,EAAW,SACX,EAAI,SACJ,EAAI,S,kBAEG,CAAC,EAAO,QAAQ,GAAQ,K,mDAhDnC,CAAI,IAAJ,UAAI,MAAJ,WAoDI,MAAO,YApDX,CAAI,IAAJ,SAAI,MAAJ,mBAAiC,ICApB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,EAAiB,GAC3C,OAAO,EAAE,SAAS,OAbtB,GAA6B,I,uSCChB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAYgB,G,oIACN,EAAI,EAAO,IACX,EAAU,EAAO,cAEE,E,sBACjB,IAAI,MAAM,8C,YAGd,KAAK,YAAc,IAAM,KAAK,aAAe,G,gBAE/C,IADM,EAAW,IAAI,MAAM,EAAQ,MAC1B,EAAI,EAAG,EAAI,EAAQ,KAAM,IAChC,EAAS,GAAK,EAAQ,IAAI,G,yBAGrB,CAAC,EAAE,OAAO,K,aAEb,IAAI,MAAJ,iCACsB,KAAK,YAD3B,yB,kDA5BV,CAAI,IAAJ,UAAI,MAAJ,WAkCI,MAAO,SAlCX,CAAI,IAAJ,SAAI,MAAJ,mBAA8B,I,uSCDjB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,YAAc,EAAK,iBAAiB,QAJ/B,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAiBI,G,kIAEM,EAAI,EAAO,G,kBAEV,CAAC,EAAE,UAAU,KAAK,e,kDArB7B,CAAI,IAAJ,UAAI,MAAJ,WAyBI,MAAO,cAzBX,CAAI,IAAJ,SAAI,MAAJ,mBAAmC,I,uSCAtB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,iCAEV,cAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,GAEvD,EAAc,KACV,EAAD,KAAO,EAAK,iBAAiB,SAL1B,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAmBI,G,sIAEM,EAAI,EAAO,KAEb,KAAK,YAAc,SAAoB,IAAd,KAAK,M,gBAIhC,IAHM,EAAY,EAAE,WACd,EAAW,GACb,EAAO,EACF,EAAI,EAAG,EAAI,EAAU,OAAQ,IAChC,EAAO,KAAK,KAAK,QAAU,KAAK,KAAK,KAAU,IACjD,EAAS,KAAK,GACd,KAEF,EAAS,KAAK,EAAU,I,OAEtB,KAAK,KAAK,KAAK,KAAK,OAAS,KAAO,EAAU,QAChD,EAAS,KAAK,G,kBAGT,CAAC,EAAE,QAAQ,K,aAEd,IAAI,MAAJ,sCAC2B,KAAK,YADhC,yB,kDAxCV,CAAI,IAAJ,UAAI,MAAJ,WA8CI,MAAO,cA9CX,CAAI,IAAJ,SAAI,MAAJ,mBAAmC,I,uSCEtB,GAAb,YAAE,qBAAF,iBAIE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,MAON,GAPM,qBAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAGtD,WAAa,EAAK,mBAAmB,QAElB,YAApB,EAAK,WACD,UAAI,MACR,4DAIJ,GAAI,EAAK,YAAc,EAAG,CACxB,IAAM,EAAS,EAAK,mBAAmB,UACjC,QAAS,IAAX,GAAmC,OAAX,EAG1B,MAAM,IAAI,MAAJ,0CAC+B,EAAK,YADpC,iCAFE,EAAH,OAAS,EAhBR,SAVZ,uBAAE,IAAJ,YAAI,MAAJ,SAmCkB,G,oIACV,KAAK,YAAc,G,yCACd,KAAK,Q,UAGR,aAAiB,E,gBAEb,OADR,QAAQ,KAAK,4D,SACC,GAAM,G,OAApB,E,cAMF,IAHM,EAAK,EAEL,EAAS,IAAI,MAAM,EAAG,MACnB,EAAI,EAAG,EAAI,EAAG,KAAM,IAC3B,EAAO,GAAK,EAAG,IAAI,G,yBAEd,G,mDAnDX,CAAI,IAAJ,UAAI,MAAJ,SAsDgB,G,+HAIG,OAHT,EAAI,EAAO,GACX,EAAQ,EAAO,G,SAEA,KAAK,UAAU,G,cAA9B,E,yBAEC,CAAC,EAAE,SAAS,K,kDA5DvB,CAAI,IAAJ,UAAI,MAAJ,WAgEI,MAAO,aAhEX,CAAI,IAAJ,SAAI,MAAJ,mBAAkC,I,uSCFrB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAaI,G,+HAKA,IAHM,EAAI,EAAO,GAEX,EAAO,IAAI,MAAM,EAAE,WAAW,OAAS,GACpC,EAAI,EAAG,EAAI,EAAE,WAAW,OAAS,EAAG,IAC3C,EAAK,GAAK,EAAI,E,yBAGT,CAAC,EAAE,WAAW,GAAM,K,6CAtB/B,CAAI,IAAJ,UAAI,MAAJ,WA0BI,MAAO,sBA1BX,CAAI,IAAJ,SAAI,MAAJ,mBAA2C,ICA9B,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,UAbb,GAA6B,ICAhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,UAbb,GAA6B,ICAhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATrE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,ICAjB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATrE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,ICAjB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,UAbb,GAA6B,IAiBhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATrE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,IAiBjB,GAAb,kDACE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATvE,oDAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,IAiBjB,GAAb,kDACE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,QAAS,GATxE,oDAY8B,GAC1B,OAAO,EAAE,YAbb,GAA+B,ICnDlB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,UAbb,GAA6B,IAiBhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATrE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,IAiBjB,GAAb,kDACE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATvE,oDAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,IAiBjB,GAAb,kDACE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,QAAS,GATxE,oDAY8B,GAC1B,OAAO,EAAE,YAbb,GAA+B,ICnDlB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,UAbb,GAA6B,IAiBhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATrE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,IAiBjB,GAAb,kDACE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,OAAQ,GATvE,oDAY8B,GAC1B,OAAO,EAAE,WAbb,GAA8B,IAiBjB,GAAb,kDACE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,QAAS,GATxE,oDAY8B,GAC1B,OAAO,EAAE,YAbb,GAA+B,ICnDlB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,UAAW,GATxE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,cAbb,GAAiC,ICApB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,YACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,OAAO,EAAM,IAAI,KAAK,KAAM,KAAK,cArBrC,GAAmC,ICAtB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,aACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,OAAO,EAAM,QAAQ,KAAK,KAAM,KAAK,cArBzC,GAAoC,ICAvB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,eACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,OAAO,EAAM,aAAa,KAAK,KAAM,KAAK,cArB9C,GAAsC,ICAzB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,kBACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,OAAO,EAAM,gBAAgB,KAAK,KAAM,KAAK,cArBjD,GAAyC,ICA5B,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,WACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,IAAM,EAAY,EAAM,UAAU,KAAK,KAAM,KAAK,UAC5C,EAAS,EAAU,OAEzB,OADA,EAAU,SACH,MAxBX,GAAkC,ICArB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,WACA,GAhBJ,uBAAE,IAAJ,OAAI,MAAJ,SAoB2B,GACvB,IAAM,EAAM,EAAM,MACZ,EAAS,EAAI,IAAI,KAAK,KAAM,KAAK,UAEvC,OADA,EAAI,SACG,MAxBX,GAAkC,ICArB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,EAAiB,GAC3C,OAAO,EAAE,MAAM,OAbnB,GAA6B,ICAhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,WACA,GAhBJ,uBAAE,IAAJ,UAAI,MAAJ,SAoB8B,GAC1B,OAAO,EAAE,WArBb,GAAkC,ICArB,GAAb,YAAE,qBAAF,iBAII,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEV,cACE,EACA,EACA,EACA,EACA,EACA,cACA,IAGG,MAAQ,EAAK,kBAAkB,UAAY,GAChD,EAAK,KAAO,EAAK,kBAAkB,SAAW,GAbpC,EAVZ,uBAAE,IAAJ,UAAI,MAAJ,SA0B8B,GAC1B,OAAO,EAAE,YAAY,KAAK,MAAO,KAAK,UA3B1C,GAAqC,ICAxB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,MAAO,GATpE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,aAbb,GAA6B,ICAhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAGR,EACA,EACA,EACA,EACA,EACA,aACA,GAhBJ,uBAAE,IAAJ,UAAI,MAAJ,SAoB8B,GAC1B,OAAO,EAAE,aAAa,EAAG,OArB7B,GAAoC,I,uSCAvB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAaI,G,kIAEM,EAAS,EAAO,G,kBAEf,CAAC,EAAO,Y,6CAjBnB,CAAI,IAAJ,UAAI,MAAJ,WAqBI,MAAO,YArBX,CAAI,IAAJ,SAAI,MAAJ,mBAAiC,I,uSCEpB,GAAb,YAAE,qBAAF,iBACE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAYgB,G,sIACN,EAAI,EAAO,GAEX,EAAQ,EAAE,WACV,EAAO,EAAQ,G,kBAEd,CAAC,IAAI,EAAU,CAAC,GAAI,CAAC,GAAO,Y,6CAlBvC,CAAI,IAAJ,UAAI,MAAJ,WAsBI,MAAO,SAtBX,CAAI,IAAJ,SAAI,MAAJ,mBAA8B,I,uSCFjB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,MAAQ,EAAK,kBAAkB,UAAY,IAJtC,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAiBI,G,wIAEM,EAAS,EAAO,GAEhB,EAAQ,EAAO,UAAK,EAAW,GAC/B,EAAQ,EAAO,KAAK,OAAG,GAEvB,EAAS,EAAM,IAAI,EAAO,KAAK,OAErC,EAAM,SACN,EAAM,S,kBACC,CAAC,I,kDA5BZ,CAAI,IAAJ,SAAI,MAAJ,eAAI,IAAJ,UAAI,MAAJ,WAkCI,MAAO,gBAlCX,GAAmC,I,uSCAtB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,MAAQ,EAAK,kBAAkB,UAAY,EAJtC,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAiBI,G,4IAEM,EAAS,EAAO,GAEhB,EAAQ,EAAO,UAAK,EAAW,GAC/B,EAAQ,EAAO,KAAK,OAAG,GAEvB,EAAW,EAAM,MACvB,EAAM,SACA,EAAI,EAAS,kBAAkB,KAAK,OAAQ,KAAK,OACvD,EAAS,SAEH,EAAS,EAAE,IAAI,GAErB,EAAE,SACF,EAAM,S,kBACC,CAAC,I,mDAjCZ,CAAI,IAAJ,SAAI,MAAJ,eAAI,IAAJ,UAAI,MAAJ,WAuCI,MAAO,UAvCX,GAA6B,I,uSCAhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAaI,G,4IAEM,EAAS,EAAO,GAChB,EAAQ,EAAO,GAEf,EAAQ,EAAO,UAAK,EAAW,GAC/B,EAAQ,EAAO,KAAK,OAAG,GAEvB,EAAW,EAAM,SAAS,GAChC,EAAM,SAEA,EAAS,EAAS,IAAI,GAE5B,EAAS,SACT,EAAM,S,kBACC,CAAC,I,8CA5BZ,CAAI,IAAJ,SAAI,MAAJ,eAAI,IAAJ,UAAI,MAAJ,WAkCI,MAAO,YAlCX,GAA+B,I,uSCAlB,GAAb,YAAE,qBAAF,iBAIE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,MAAQ,EAAK,kBAAkB,UAAY,mBAC5C,EAAC,MAAQ,EAAK,kBAAkB,UAAY,mBALtC,EAVZ,uBAAE,IAAJ,UAAI,MAAJ,SAmBI,G,4IAEM,EAAS,EAAO,GAEhB,EAAQ,EAAO,UAAK,EAAW,GAC/B,EAAQ,EAAO,KAAK,OAAG,GAEvB,EAAW,EAAM,MACvB,EAAM,SACA,EAAI,EAAS,kBACjB,KAAK,MAAQ,KAAK,OACjB,KAAK,MAAQ,KAAK,OAErB,EAAS,SAEH,EAAS,EAAE,IAAI,EAAO,EAAK,KAAK,OAEtC,EAAE,SACF,EAAM,S,kBACC,CAAC,I,mDAtCZ,CAAI,IAAJ,SAAI,MAAJ,eAAI,IAAJ,UAAI,MAAJ,WA4CI,MAAO,WA5CX,GAA8B,I,uSCAjB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,0BAIV,IAAM,GAFF,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEzC,gBAAgB,QAJxB,OAKG,OAAT,IACI,EAAD,KAAO,GANJ,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAoBI,G,kIAEM,EAAS,EAAO,G,kBAEf,CAAC,EAAO,QAAQ,KAAK,Q,kDAxBhC,CAAI,IAAJ,UAAI,MAAJ,WA4BI,MAAO,YA5BX,CAAI,IAAJ,SAAI,MAAJ,mBAAiC,I,uSCApB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAaI,G,wIAEM,EAAI,EAAO,GAEX,EAAM,EAAE,MACR,EAAK,EAAI,UAAU,GACzB,EAAI,SACE,EAAS,EAAG,MAClB,EAAG,S,kBAEI,CAAC,I,6CAvBZ,CAAI,IAAJ,UAAI,MAAJ,WA2BI,MAAO,aA3BX,CAAI,IAAJ,SAAI,MAAJ,mBAAkC,I,uSCArB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAaI,G,wIAEM,EAAI,EAAO,GAEX,EAAM,EAAE,MACR,EAAK,EAAI,UAAU,GACzB,EAAI,SACE,EAAS,EAAE,OAAO,GACxB,EAAG,S,kBAEI,CAAC,I,6CAvBZ,CAAI,IAAJ,UAAI,MAAJ,WA2BI,MAAO,aA3BX,CAAI,IAAJ,SAAI,MAAJ,mBAAkC,I,uSCArB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAaI,G,+HAQA,IANI,EAAO,OAKP,EAAS,EAAO,GACX,EAAI,EAAG,EAAI,EAAO,OAAQ,IAC3B,EAAS,EAAO,IAAI,EAAO,IAC7B,EAAI,GACN,EAAO,SAET,EAAS,E,yBAGJ,CAAC,I,6CA7BZ,CAAI,IAAJ,UAAI,MAAJ,WAiCI,MAAO,QAjCX,CAAI,IAAJ,SAAI,MAAJ,mBAA6B,I,uSCAhB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,GAT7D,uBAAE,IAAJ,UAAI,MAAJ,SAaI,G,iIAQA,IANI,EAAO,OAKP,EAAS,EAAO,GACX,EAAI,EAAG,EAAI,EAAO,OAAQ,IAC3B,EAAS,EAAO,IAAI,EAAO,IAC7B,EAAI,GACN,EAAO,SAET,EAAS,E,OAGL,EAAO,EAAO,eAAe,EAAI,EAAO,QAC1C,EAAO,OAAS,GAClB,EAAO,S,kBAGF,CAAC,I,6CAlCZ,CAAI,IAAJ,UAAI,MAAJ,WAsCI,MAAO,SAtCX,CAAI,IAAJ,SAAI,MAAJ,mBAA8B,I,uSCAjB,GAAb,YAAE,qBAAF,iBAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAEtD,MAAQ,EAAK,kBAAkB,UAAY,EAJtC,EATZ,uBAAE,IAAJ,UAAI,MAAJ,SAiBI,G,8IAEM,EAAS,EAAO,GAEhB,EAAQ,EAAO,UAAK,EAAW,GAC/B,EAAQ,EAAO,KAAK,OAAG,GAIvB,EAAW,EAAM,eAAe,EAAI,KAAK,OAC/C,EAAM,SACA,EAAW,EAAS,MAC1B,EAAS,SACH,EAAW,EAAS,kBAAkB,KAAK,OAAQ,KAAK,OAC9D,EAAS,SAEH,EAAS,EAAS,IAAI,GAE5B,EAAS,SACT,EAAM,S,kBACC,CAAC,I,mDArCZ,CAAI,IAAJ,SAAI,MAAJ,eAAI,IAAJ,UAAI,MAAJ,WA2CI,MAAO,WA3CX,GAA8B,ICAjB,GAAb,YAAE,qBAAF,iBACI,SAAF,EACE,EACA,EACA,EACA,EACA,EACA,GAAU,uCAEJ,EAAY,EAAQ,EAAS,EAAW,EAAa,QAAS,GATtE,uBAAE,IAAJ,UAAI,MAAJ,SAY8B,GAC1B,OAAO,EAAE,YAbb,GAA+B,I,uSCIlB,GAAb,kDAGE,WACE,EACA,EACA,EACA,EACA,EACA,GAAU,kCAEN,EAAJ,YAAM,EAAY,EAAQ,EAAS,EAAW,EAAa,IAV7D,QAAmB,MAQP,EATd,oDAcgB,G,qIAKQ,OAJd,EAAQ,EAAO,GACf,EAAQ,EAAO,GACf,EAAQ,EAAO,G,SAEK,KAAK,SAAS,G,OACpB,OADd,E,OAA0C,G,SACtB,KAAK,SAAS,G,OACpB,OADd,E,OAA0C,G,UACtB,KAAK,SAAS,G,WAAlC,E,OAA0C,GAE3B,QAAjB,KAAK,Q,0CACA,CAAC,EAAU,MAAM,EAAY,EAAY,K,WACtB,SAAjB,KAAK,Q,0CACP,CAAC,GAAW,MAAM,EAAY,EAAY,K,iCAE1C,CAAC,GAAU,MAAM,EAAY,EAAY,K,mDA5BtD,kEAmCI,MAAO,UAnCX,8B,qHAuCI,KAAK,QAAU,M,kDAvCnB,+B,qHA0CI,KAAK,QAAU,O,kDA1CnB,8B,qHA6CI,KAAK,QAAU,M,oDA7CnB,GAA+B,IC4DlB,GAAmD,CAC5D,KAAI,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IAClE,mBAAkB,SAClB,EACA,EACA,EACA,EACA,EACA,GANkB,OAQlB,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEF,KAAI,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IAClE,IAAG,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACjE,IAAG,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACjE,WAAU,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACV,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEF,KAAI,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IAClE,SAAQ,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACR,IAAI,GAAa,EAAY,EAAQ,EAAS,EAAW,EAAa,IACtE,QAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACP,IAAI,GAAY,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,MAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,QAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACP,IAAI,GAAY,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,KAAI,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IAClE,OAAM,SAACC,EAAYD,EAAQE,EAASC,EAAWC,EAAaC,GAAtD,OACN,IAAI,GAAW,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,IAAG,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACjE,IAAG,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACjE,KAAI,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IAClE,KAAI,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IAClE,UAAS,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACT,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEF,WAAU,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACV,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEF,UAAS,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACT,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,UAAW,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACT,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,gBAAiB,SACf,EACA,EACA,EACA,EACA,EACA,GANe,OAQf,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,SAAU,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACR,IAAI,GAAa,EAAY,EAAQ,EAAS,EAAW,EAAa,IACxE,SAAU,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACR,IAAI,GAAa,EAAY,EAAQ,EAAS,EAAW,EAAa,IACxE,aAAc,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACZ,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,gBAAiB,SACf,EACA,EACA,EACA,EACA,EACA,GANe,OAQf,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,UAAW,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACT,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,OAAQ,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACN,IAAI,GAAW,EAAY,EAAQ,EAAS,EAAW,EAAa,IACtE,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,gBAAiB,SACf,EACA,EACA,EACA,EACA,EACA,GANe,OAQf,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,OAAQ,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACN,IAAI,GAAW,EAAY,EAAQ,EAAS,EAAW,EAAa,IACtE,QAAS,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACP,IAAI,GAAY,EAAY,EAAQ,EAAS,EAAW,EAAa,IACvE,sBAAuB,SACrB,EACA,EACA,EACA,EACA,EACA,GANqB,OAQrB,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,UAAW,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACT,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,MAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,MAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,OAAQ,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACN,IAAI,GAAW,EAAY,EAAQ,EAAS,EAAW,EAAa,IACtE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,MAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,MAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,WAAY,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACV,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,SAAU,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACR,IAAI,GAAa,EAAY,EAAQ,EAAS,EAAW,EAAa,IACxE,QAAS,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACP,IAAI,GAAY,EAAY,EAAQ,EAAS,EAAW,EAAa,IACvE,YAAa,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACX,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,IAAK,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACH,IAAI,GAAQ,EAAY,EAAQ,EAAS,EAAW,EAAa,IACnE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,KAAM,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACJ,IAAI,GAAS,EAAY,EAAQ,EAAS,EAAW,EAAa,IACpE,MAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,MAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,MAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,MAAO,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACL,IAAI,GAAU,EAAY,EAAQ,EAAS,EAAW,EAAa,IACrE,SAAU,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACR,IAAI,GAAa,EAAY,EAAQ,EAAS,EAAW,EAAa,IACxE,UAAW,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACT,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,IAEJ,QAAS,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACP,IAAI,GAAY,EAAY,EAAQ,EAAS,EAAW,EAAa,IACvE,SAAU,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACR,IAAI,GAAa,EAAY,EAAQ,EAAS,EAAW,EAAa,IACxE,SAAU,SAAC,EAAY,EAAQ,EAAS,EAAW,EAAa,GAAtD,OACR,IAAI,GAAa,EAAY,EAAQ,EAAS,EAAW,EAAa,IACxE,kBAAmB,SACjB,EACA,EACA,EACA,EACA,EACA,GANiB,OAQjB,IAAI,GACF,EACA,EACA,EACA,EACA,EACA,K,uSCjUO,GAAb,YAAE,qBAAF,iBA8BI,SAAF,EAAY,EAAkC,GAAgB,MAgBxD,EAhBwD,qBACxD,EAAJ,cA7BM,SAAwB,IAAI,IAE5B,QAAkC,GAClC,UAAoB,GACpB,eAAyB,GAEzB,iBAAiD,GAEjD,YAAuB,GAOvB,gBAAgB,SAeT,IAAT,IACI,EAAC,IAGL,EAAC,mBAAqB,IAAI,SACA,IAA5B,EAAK,mBAAmC,EAAK,mBAAqB,IAEhE,EAAC,eAAiB,IAAI,SACA,IAAxB,EAAK,eAA+B,EAAK,eAAiB,IAGxD,EAAC,KAAO,EAAK,MAAQ,YACrB,EAAC,UAAY,EAAK,WAAa,GAI3B,EADJ,aAAkB,YACd,IAAI,WAAW,GAEf,EAEJ,EAAC,WAAa,QAAK,WAAW,OAAO,GAEzC,IAAI,EAAM,EAAK,WAAW,YAAY,GAAG,QACrC,KAAK,OAAO,KACd,EAAO,EAAa,YAGlB,EAAC,QAAU,EAGX,EAAC,OAAS,EAAK,WAAW,MAAM,MAChC,IAAC,IAAI,EAAI,EAAG,EAAI,EAAK,OAAO,OAAQ,IACtC,EAAK,SAAS,IAAI,EAAK,OAAO,GAAG,MAlCyB,OAqC5D,EAAK,QAAU,EAAK,WAAW,MAAM,OAAO,KAAI,YAAC,OAAI,EAAE,QAGvD,EAAK,YAAY,EAAK,WAAW,MAAM,aAEvC,EAAK,UAAU,EAAK,YA1CwC,EA9B9D,uBAAE,IAAJ,YAAI,MAAJ,SA2EoB,GAEV,IAFqC,WAElC,EAAI,EAAG,EAAI,EAAW,MAAM,KAAK,OAAQ,IAAK,CAErD,IAAM,EAAW,EAAW,MAAM,KAAK,GAEjC,EAAM,GAAY,EAAS,QAEjC,QAAY,IAAR,EACF,MAAM,IAAI,MAAJ,wBAA2B,EAAS,OAApC,yBAGA,IAAF,EAAa,EAAS,WAAa,GACnC,EAAS,EAAS,OAAS,GAG3B,EAAO,EACX,EACA,EAJc,EAAS,QAAU,GAMjC,KAAK,UACL,KAAK,QACL,KAAK,MAEC,KAAH,MAAM,GAAK,EACR,KAAH,QAAQ,KAAK,GAEV,IAAH,IAAI,EAAI,EAAG,EAAI,EAAO,OAAQ,IAAK,CAC5B,IAAJ,EAAQ,EAAO,QACc,IAA/B,KAAK,eAAe,KACV,KAAP,eAAe,GAAS,CAC3B,GAAI,GACJ,WAAW,IAGL,KAAL,eAAe,GAAO,GAAG,KAAK,GAGT,IAAxB,EAAK,gBACP,KAAK,aAAa,KAAK,GAGD,aAApB,EAAS,cAEqC,IAA5C,KAAK,eAAe,EAAS,OAAO,IAEtC,KAAK,eAAe,EAAS,OAAO,IAAM,CACxC,GAAI,GACJ,WAAW,GAIb,KAAK,eAAe,EAAS,OAAO,IAAI,WAAY,GApDf,oBAyDtB,KAAK,SAzDiB,IAyDnC,IAAR,uBAAmC,KAAxB,EAAwB,QACvB,KAAL,MAAM,GAAQ,YAAW,YAAI,OAAI,EAAK,gBAAgB,OA1DlB,iCA3E/C,CAAI,IAAJ,cAAI,MAAJ,SAyIsB,GAClB,IAAK,IAAI,EAAI,EAAG,EAAI,EAAY,OAAQ,IAAK,CAC3C,IAAM,EAAc,EAAY,GAE5B,EAAsB,GACxB,EACmB,KAAnB,KAAK,WAEW,UAAd,KAAK,OACP,EAAS,IAAI,GAAS,IAIhB,KAAH,UAAU,EAAY,MAAQ,KAtJzC,CAAI,IAAJ,UAAI,MAAJ,SAmKgB,EAAuB,G,8JAC7B,EAAqD,GAErD,EAAkD,G,cACxC,KAAK,S,IAArB,2BAAW,EAAmB,QAC5B,EAAM,GAAK,CACT,eAAgB,G,8BAId,E,YAA2B,KAAK,cAEtB,KAAX,kBAAkB,EAAQ,EAAiB,EAAO,G,YAEhD,EAAW,OAAS,G,iBASb,OARN,EAAS,EAAW,QAEpB,EAAO,KAAK,MAAM,G,EAEG,KAAK,gBAAgB,EAAM,GAA/C,E,EAAA,OAAQ,E,EAAA,SAEX,O,sBAEc,EAAK,QAAQ,G,QAA7B,E,gEAEA,QAAQ,MAAR,iCAC4B,EAD5B,wBAEI,EAAK,OAFT,uBAGiB,EAAK,OAAO,KAAI,SAAC,GAAD,OAC7B,EAAK,kBAAkB,Q,aAS7B,IAJA,EAAU,QAEV,KAAK,iBAAiB,EAAM,EAAiB,EAAS,EAAO,GAEpD,EAAI,EAAG,EAAI,EAAS,OAAQ,IAC9B,KAAK,SAAS,IAAI,EAAS,MAChB,EAAgB,EAASxB,IACjC,MAAM,gBACL,EAAgB,EAAS,K,QAIvB,IAAT,E,iBACF,O,UAAM,IAAI,SAAQ,YAChB,WAAW,EAAS,M,+BAM1B,IADM,EAAyB,GACtB,EAAI,EAAG,EAAI,KAAK,QAAQ,OAAQ,IACvC,EAAQ,KAAK,EAAgB,KAAK,QAAQ,IAAI,O,yBAGzC,G,6DA7NX,CAAI,IAAJ,oBAAI,MAAJ,SAiOI,EACA,EACA,EACA,GAEA,IAAK,IAAI,EAAI,EAAG,EAAI,EAAO,OAAQ,IAAK,CAEtC,EAAgB,KAAK,OAAO,GAAG,MAAQ,CACrC,MAAO,EAAO,GACd,KAAM,GAMA,IAFR,IAAM,EAAQ,KAAK,eAAe,KAAK,OAAO,GAAG,MAExC,EAAI,EAAG,EAAI,EAAM,GAAG,OAAQ,IAAK,CACxC,IAAM,EAAK,EAAM,GAAG,GACpB,EAAM,GAAI,iBAEN,EAAM,GAAI,iBAAmB,KAAK,MAAM,GAAI,iBAC9C,EAAW,KAAK,UACT,EAAM,QAtPvB,CAAI,IAAJ,kBAAI,MAAJ,SA6PI,EACA,GAIM,IAFA,MAAwB,GACxB,EAAqB,GAClB,EAAI,EAAGA,EAAI,EAAK,OAAO,OAAQ,IAAK,CACnC,IAAF,EAAQ,EAAK,OAAO,GAC1B,QAA8B,IAA1B,KAAK,UAAU,GACjB,EAAO,KAAK,KAAK,UAAU,QACtB,CACL,IAAM,EAAQ,EAAgB,GAC9B,EAAM,OAEJ,EAAM,MAAQ,KAAK,eAAe,GAAO,GAAG,QAC5C,KAAK,eAAe,GAAO,WAE3B,EAAS,KAAK,GAEhB,EAAO,KAAK,EAAM,QAIhB,MAAC,CAAC,SAAQ,cAnRpB,CAAI,IAAJ,mBAAI,MAAJ,SAuRI,EACA,EACA,EACA,EACA,GAEM,IAAD,IAAI,EAAI,EAAG,EAAI,EAAK,QAAQ,OAAQA,IAAK,CACpC,IAAF,EAAS,EAAK,QAAQ,GACpB,EAAQ,GAAU,CACxB,MAAO,EAAQ,GACL,KAAJ,GAGA,IAAF,EAAQ,KAAK,eAAe,GAE1B,QAAM,IAAV,EACF,IAAK,IAAI,EAAI,EAAG,EAAI,EAAM,GAAG,OAAQ,IAAK,CACxC,IAAM,EAAK,EAAM,GAAG,GACpB,EAAM,GAAI,iBAEN,EAAM,GAAI,iBAAmB,KAAK,MAAM,GAAI,iBAC9C,EAAW,KAAK,UACT,EAAM,QA7SzB,CAAI,IAAJ,QAAI,MAAJ,W,+IAwToB,KAAK,W,gDAAV,E,WACJ,KAAK,mBAAmB,IAAI,G,gBACX,O,SAAM,GAAM,KAAK,UAAU,I,OAA/CQ,KAAK,UAAU,G,kDAIH,KAAK,S,8DAAV,E,QACJ,KAAK,eAAe,IAAI,G,iBAC3B,O,UAAM,KAAK,MAAM,GAAG,Q,4MAhU5B,CAAI,IAAJ,SAAI,MAAJ,W,+IAyUoB,KAAK,W,gDAAV,E,WACJ,KAAK,mBAAmB,IAAI,G,gBACX,O,SAAM,GAAO,KAAK,UAAU,I,OAAhD,KAAK,UAAUR,G,kDAIH,KAAK,S,8DAAV,E,QACJ,KAAK,eAAe,IAAI,G,iBAC3B,O,UAAM,KAAK,MAAM,GAAG,S,4MAjV5B,CAAI,IAAJ,QAAI,MAAJ,W,+IA0VoB,KAAK,W,gDAAV,E,WACJ,KAAK,mBAAmB,IAAI,G,gBACX,O,SAAM,GAAM,KAAK,UAAU,I,OAA/C,KAAK,UAAU,G,kDAIH,KAAK,S,8DAAV,E,QACJ,KAAK,eAAe,IAAI,G,iBAC3B,O,UAAM,KAAK,MAAM,GAAG,Q,4MAlW5B,CAAI,IAAJ,WAAI,MAAJ,WA0WiB,2BACc,IADd,IACb,2BAAiD,OAAtC,EAAsC,QAEzC,EAAe,EAAa,iBAAiB,MAFJ,cAIzB,GAJyB,IAI/C,2BAAoC,OAAzB,EAAyB,QAC5B,EAAQ,EAAQ,KAAI,YAAC,OAAI,EAAK,MAAM,MACpC,EAAU,EAAa,MAC3B,GACA,YAAI,OAAI,EAAK,gBAAgB,KAC7B,KAAK,UACL,KAAK,SAGD,EAAU,IAAI,IAAI,EAAQ,SATE,cAWb,GAXa,IAWlC,2BAA8B,KAAnB,EAAmB,QAC5B,KAAK,WAAW,EAAQ,IAZQ,8BAelC,KAAK,WAAW,IAnB6B,gCADpC,8BAwBb,KAAK,UAlYT,CAAI,IAAJ,QAAI,MAAJ,SAqYe,GAEX,IAF4C,aAE/B,CACX,IAAM,EAAgB,KAAK,oBAAoB,GAI/C,GAFA,EAAyB,KAErB,EAAc,KAAO,GAQvB,MAPA,EAAc,SAAQ,YACpB,IAAM,EAAgB,EAAK,WAAW,EAAI,IAAI,KAC9C,EAA+C,OAAtB,QAAsB,IAAtB,OAAsB,EAAtB,EAAwB,OAC/C,SAhZZ,CAAI,IAAJ,sBAAI,MAAJ,SAyZ8B,GAAiC,WACrD,EAAgB,IAAI,SAEK,IAA3B,IACF,EAAyB,IAG3B,IAAK,IAAI,EAAI,EAAG,EAAI,EAAuB,OAAQ,IAAK,CACtD,IAAM,EAAK,EAAuB,GAC5B,EAAe,KAAK,kBAAkB,QACvB,IAAjB,GACF,EAAc,IAAI,GAEpB,IAAM,EAAc,KAAK,iBAAiB,QACtB,IAAhB,GACF,EAAc,IAAI,GAfqC,eAmBhD,GAET,GAC6B,IAFR,EAAK,eAAe,GAE1B,GAAG,aACqB,IAArC,EAAK,QAAQ,MAAK,YAAC,OAAI,IAAM,KAC7B,CACA,EAAuB,KAAK,GAC5B,IAAM,EAAe,EAAK,kBAAkB,QACvB,IAAjB,GACF,EAAc,IAAI,GAEpB,IAAM,EAAc,EAAK,iBAAiB,QACtB,IAAhB,GACF,EAAc,IAAI,KAbxB,IAAK,IAAM,KAAM,KAAK,eAAgB,EAA3B,GAnBgD,oBAqC1C,GArC0C,IAqC3D,2BAAyC,KAA9B,EAA8B,eAChC,KAAK,eAAe,IAtC8B,8BAyC3D,OAAO,IAlcX,CAAI,IAAJ,aAAI,MAAJ,SAscI,EACA,GAEA,IAFmC,EAE7B,EAAO,KAAK,MAAM,GAFW,cAGf,EAAK,QAHU,IAGnC,2BAAiC,KAAtB,EAAsB,aACI,IAA/B,KAAK,eAAe,KACtB,KAAK,eAAe,GAAO,GAAK,KAAK,eAAe,GAAO,GAAG,QAC5D,YAAC,OAAI,EAAE,aAAe,EAAO,gBANA,8BAWnC,IAAM,EAAyB,GAW/B,OAVK,EAAuB,IAAI,EAAK,QAAQ,KAC3C,EAAuB,KAAK,EAAK,QAAQ,IAG3C,KAAK,QAAU,KAAK,QAAQ,QAAO,YAAC,OAAI,EAAE,aAAe,EAAO,cAChE,KAAK,MAAM,GAAQ,gBACZ,KAAK,MAAM,GAElB,KAAK,aAAe,KAAK,aAAa,QAAO,YAAC,OAAI,IAAM,KAEjD,IA7dX,CAAI,IAAJ,aAAI,MAAJ,SAgeqB,GACjB,IAAM,EAAK,KAAK,gBAEhB,KAAK,QAAQ,KAAK,GAClB,KAAK,MAAM,GAAM,EAJc,oBAMX,EAAK,QANM,IAM/B,2BAAiC,KAAtB,EAAsB,QAC/B,KAAK,eAAe,GAAO,GAAG,KAAK,IAPN,iCAhenC,CAAI,IAAJ,oBAAI,MAAJ,SA6e2B,GAAc,oBACpB,KAAK,SADe,IACrC,2BAA+B,KAApB,EAAoB,QAC7B,IAA6D,IAAzD,KAAK,MAAM,GAAI,QAAQ,WAAU,YAAC,OAAI,IAAM,KAC9C,OAAO,GAH0B,iCA7ezC,CAAI,IAAJ,mBAAI,MAAJ,SAsf0B,GAAc,oBACnB,KAAK,SADc,IACpC,2BAA+B,KAApB,EAAoB,QAC7B,IAA4D,IAAxD,KAAK,MAAM,GAAI,OAAO,WAAU,YAAC,OAAI,IAAM,KAC7C,OAAO,GAHyB,iCAtfxC,CAAI,IAAJ,kBAAI,MAAJ,SA+fyB,GACrB,QAA6B,IAAzB,KAAK,UAAU,GACjB,OAAO,KAAK,UAAU,GAExB,IAAM,EAAY,KAAK,kBAAkB,GAEnC,EAAU,KAAK,MAAM,GAC3B,OAAI,aAAmB,GACd,EAAQ,YADjB,IAtgBJ,CAAI,IAAJ,WAAI,MAAJ,WA6gBI,OAAO,KAAK,QA7gBhB,CAAI,IAAJ,SAAI,MAAJ,WAshBI,IAAK,IAAM,KAAK,KAAK,UACnB,KAAK,UAAU,GAAG,SAFT,oBAKU,KAAK,SALf,IAKX,2BAAmC,KAAxB,EAAwB,QACjC,KAAK,MAAM,GAAQ,UANV,iCArhBf,CAAI,IAAJ,gBAAI,MAAJ,WAgiBI,IADW,EACL,EAAO,sEADF,cAEU,KAAK,SAFf,IAEX,2BAAmC,KAAxB,EAAwB,QACjC,EAAQ,KAAK,KAAK,MAAM,KAHf,8BAKX,OAAO,IApiBX,CAAI,IAAJ,gBAAI,MAAJ,WAwiBI,IAAM,EAAU,sEAChB,IAAK,IAAM,KAAK,KAAK,UACf,KAAK,UAAU,aAAc,IAC/B,EAAW,KAAK,KAAK,UAAU,IAGnC,OAAO,MA9iBX,GAA+B,I,uSChDlB,GAAb,kDAUI,SAAF,EAAYyB,EAAe,EAAgB,GAAc,0BACvD,eAEA,OAAgB,IAAT,GAA4B,EAE/B,IAAE,EzOiCJ,SAAiB,GAEnB,IAFoD,IAAtB,EAAsB,uDAAf,EAAG,EAAY,uDAAD,EAC/C,EAAS,GACN,EAAI,EAAG,EAAI,EAAG,GAAK,EAAG,OACZ,IADY,mBACtB,EADsB,KAClB,EADkB,KAE7B,EAAO,KAAK,EAAK,EAAW,GACxB,EAAI,EAAI,GACV,EAAO,KAAK,EAAK,EAAW,GAGhC,OAAO,EyO1Cc,CAAO,EAAQ,EAAQ,EAAG,GAAK,EAAQ,IACpD,EAAS,IAAI,EAAU,CAAC,EAAO,GAAS,GAG1C,GAFA,EAAC,QAAU,IAAI,GAAS,GAExB,EAAM,CACR,IAAM,EAAW,IAAI,MAAM,GAAQ,KAAK,GAClC,EAAa,IAAI,EAAU,CAAC,EAAG,GAAS,GACxC,EAAD,KAAO,IAAI,GAAS,GAZ4B,SAV3D,oDA0BgB,G,8IACL,CAAC,EAAO,GAAG,KAAK,KAAK,SAAS,GAAO,EAAO,EAAG,KAAK,Q,oDA3B/D,GAA4B,ICNN,GAAtB,WAMI,SAAF,EAAmB,GAAa,oBAAb,aACjB,KAAK,WAAa,EAAM,gBAP1B,uBAAE,IAAJ,YAAI,MAAJ,WAoBW,oBACiB,KAAK,YADtB,IACP,2BAAyC,KAA9B,EAA8B,aAChB,IAAnB,EAAU,OACZ,EAAU,KAAK,SACf,EAAU,UAAO,IAJd,mCApBX,KC8Ba,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,kCAE1B,EAAJ,YAAM,EAAmB,EAAO,IAPxB,cAAgB,IAKM,EARhC,uBAAE,IAAJ,eAAI,MAAJ,WAcI,sBACE,KAAK,eAAe,SADtB,8BAEE,KAAK,eAAe,SAFtB,8BAGE,KAAK,eAAe,KAHtB,mBAdJ,CAAI,IAAJ,oBAAI,MAAJ,SAsBoB,GAChB,4lBAvBJ,CAAI,IAAJ,kBAAI,MAAJ,WA6CI,MAAO,CAAC,OAAQ,aA7CpB,CAAI,IAAJ,kBAAI,MAAJ,WAiDI,MAAO,CACL,CAAC,KAAM,QAAS,KAAM,SACtB,CAAC,KAAM,QAAS,KAAM,SACtB,CAAC,KAAM,IAAK,KAAM,UApDxB,CAAI,IAAJ,OAAI,MAAJ,SAwDO,GACH,OAAO,KAAK,QACV,EAAM,QAAQ,MACd,CAAC,KAAM,EAAM,KAAM,QAAS,EAAM,SAClC,CACE,MAAO,EAAM,MACb,MAAO,EAAM,MACb,EAAG,EAAM,MA/DjB,CAAI,IAAJ,iBAAI,MAAJ,SAoEiB,GACb,OAAO,EAAM,QAAQ,QArEzB,CAAI,IAAJ,UAAI,MAAJ,SAwEU,QACoB,IAAtB,EAAK,eACP,KAAK,QAAU,EAAK,aAAa,QAGnC,+DAAc,KA7ElB,CAAI,IAAJ,qBAAI,MAAJ,SAgFqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,UAAW,EAAM,KAAK,MACtB,UAAW,EAAM,KAAK,OAAO,MAC7B,WAAY,EAAM,KAAK,OAAO,OAE9B,aAAc,EAAM,QAAQ,MAC5B,aAAc,EAAM,QAAQ,OAAO,MACnC,cAAe,EAAM,QAAQ,OAAO,OAEpC,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,MAAO,EAAM,MACb,MAAO,EAAM,SArGnB,CAAI,IAAJ,qBAAI,MAAJ,SAyGqB,GACjB,gBAAU,EAAM,KAAK,MAArB,YAA8B,EAAM,QAAQ,MAA5C,YAAqD,EAAM,MAA3D,YAAoE,EAAM,WA1G9E,GAEU,IA4GG,GAAwB,IAAI,IACvC,SAAC,GAAD,OAAqB,IAAI,GAAsB,GAAgB,MC9GpD,GAAb,YAAE,qBAAF,iBAKI,SAAF,EACE,EACA,EACA,GAA8B,kCAE9B,cAAM,EAAmB,EAAO,IAPxB,cAAgB,IAKM,EARhC,uBAAE,IAAJ,eAAI,MAAJ,WAcI,sBACE,KAAK,eAAe,SADtB,8BAEE,KAAK,eAAe,WAFtB,2BAdJ,CAAI,IAAJ,oBAAI,MAAJ,SAqBoB,GAChB,g9CAtBJ,CAAI,IAAJ,kBAAI,MAAJ,WAgEI,MAAO,CAAC,QAAS,aAhErB,CAAI,IAAJ,kBAAI,MAAJ,WAoEI,MAAO,CACL,CAAC,KAAM,QAAS,KAAM,SACtB,CAAC,KAAM,UAAW,KAAM,YAtE9B,CAAI,IAAJ,OAAI,MAAJ,SA0EO,GACH,OAAO,KAAK,QACV,EAAM,MAAM,MACZ,CAAC,MAAO,EAAM,MAAO,QAAS,EAAM,SACpC,CACE,MAAO,EAAM,MACb,QAAS,EAAM,YAhFvB,CAAI,IAAJ,iBAAI,MAAJ,SAqFiB,GACb,OAAO,EAAM,MAAM,QAtFvB,CAAI,IAAJ,UAAI,MAAJ,SAyFU,QACoB,IAAtB,EAAK,eACP,KAAK,QAAU,EAAK,aAAa,QAGnC,+DAAc,KA9FlB,CAAI,IAAJ,qBAAI,MAAJ,SAiGqB,GACjB,IAAM,EAAc,KAAK,eAAe,GAClC,EAAa,GAAiB,wBAClC,EAAQ,GACR,KAAK,OAGP,MAAO,CACL,WAAY,EAAM,MAAM,MACxB,WAAY,EAAM,MAAM,OAAO,MAC/B,YAAa,EAAM,MAAM,OAAO,OAEhC,aAAc,EAAM,QAAQ,MAC5B,aAAc,EAAM,QAAQ,OAAO,MACnC,cAAe,EAAM,QAAQ,OAAO,OAEpC,YAAa,EACb,YAAa,EAAW,MACxB,aAAc,EAAW,OAEzB,MAAO,EAAM,MACb,QAAS,EAAM,WAtHrB,CAAI,IAAJ,qBAAI,MAAJ,SA0HqB,GACjB,gBAAU,EAAM,MAAM,MAAtB,YAA+B,EAAM,QAAQ,MAA7C,YAAsD,EAAM,MAA5D,YAAqE,EAAM,aA3H/E,GAEU,IA6HG,GAAsB,IAAI,IACrC,SAAC,GAAD,OAAqB,IAAI,GAAqB,GAAgB,MCtJnD,GAAb,YAAE,qBAAF,iBAQI,SAAF,EACE,GAIsB,MAHf,EAGe,uDAHV,KACL,EAEe,uDAFP,GACR,EACe,uDADP,KACR,EAAe,uDAAL,KAAK,qBAEtB,cAAM,IALC,KACA,UACA,UACA,YAPF,IAAI,EAWL,IAAE,EAAS,EAAK,WAChB,KAAO,GAAG,iBAAiB,GAAW,CAClC,EAAD,QAAU,IAAI,MAAM,EAAO,QAC1B,IAAD,IAAI,EAAI,EAAG,EAAI,EAAO,OAAQ,IACzB,EAAH,QAAQ,GAAK,IAAI,GACpB,IAAI,MAAiD,EAA1C,EAAO,GAAG,MAAyB,MAAU,KAAK,GAD7C,sBAEZ,EAAO,GAAG,YAFE,CAEU,IAC1B,EAAO,GAAG,MAAM,YAId,EAAD,QAAU,IAAI,MAAM,EAAO,QAAQ,UAAK,GACvC,EAAD,QAAU,IAAI,MAAM,EAAO,QAAQ,UAAK,GAhBzB,SAbxB,uBAAE,IAAJ,OAAI,MAAJ,WAoCU,GAFA,KAAD,SAEgB,IAAjB,KAAK,cAA0C,IAAjB,KAAK,QAC7B,IAAH,IAAI,EAAI,EAAG,EAAI,KAAK,WAAW,OAAQ,IAAK,CACrC,IAAJ,EAAY,KAAK,WAAW,GACxB,QAAa,IAAnB,EAAU,KAAoB,CAChC,IAAM,EAAW,EAAU,MADK,EAGK,KAAK,UACxC,EAAU,MACV,EAAU,KACV,KAAK,QAAQ,GACb,KAAK,QAAQ,IAJR,EAHyB,EAGzB,SAAU,EAHe,EAGf,QAAS,EAHM,EAGN,QAM1B,EAAU,MAAQ,EAClB,KAAK,QAAQ,GAAK,EAClB,KAAK,QAAQ,GAAK,EAElB,EAAS,eAGR,QAAqB,IAAjB,KAAK,QACd,IAAK,IAAI,EAAI,EAAG,EAAI,KAAK,WAAW,OAAQ,IAAK,CAC/C,IAAM,EAAY,KAAK,WAAW,GAClC,QAAuB,IAAnB,EAAU,KAAoB,CAChC,IAAM,EAAW,EAAU,MADK,EAGJ,KAAK,aAC/B,EAAU,MACV,EAAU,KACV,KAAK,QAAQ,IAHR,EAHyB,EAGzB,SAAU,EAHe,EAGf,QAKjB,EAAU,MAAQ,EAClB,KAAK,QAAQ,GAAK,EAElB,EAAS,aArEnB,CAAI,IAAJ,gBAAI,MAAJ,SA4EI,EACA,EACA,GAEA,IAAI,EAQA,EAPJ,QAAgB,IAAZ,EACF,EAAa,EAAK,eAAe,EAAI,KAAK,WACrC,CACL,IAAM,EAAa,EACnB,EAAa,EAAQ,IAAI,EAAM,KAAK,MAAO,EAAI,KAAK,OACpD,EAAW,SAGP,QAAU,IAAZ,EACF,EAAa,EAAK,SAAS,EAAM,EAAI,KAAK,WACrC,CACL,IAAM,EAAc,EAAK,SAAS,GAC5B,EAAa,EACnB,EAAa,EAAQ,IAAI,EAAa,KAAK,MAAO,EAAI,KAAK,OAC3D,EAAY,SACZ,EAAW,SAEP,MAAC,CAAC,aAAY,gBAlGxB,CAAI,IAAJ,sBAAI,MAAJ,SAsGI,EACA,GAYA,MAAO,CAAC,eAVe,EAAQ,kBAC7B,GAAK,EAAI,KAAK,IAAI,KAAK,MAAO,KAAK,IACnC,GAQsB,eALD,EAAQ,kBAC7B,GAAK,EAAI,KAAK,IAAI,KAAK,MAAO,KAAK,IACnC,MAhHN,CAAI,IAAJ,YAAI,MAAJ,SAuHI,EACA,EACA,EACA,GAAiC,MAEA,KAAK,cAAc,EAAM,EAAS,GAA5D,EAF0B,EAE1B,WAMD,EAR2B,EAEd,WAMe,kBAChC,GAAK,EAAI,KAAK,IAAI,KAAK,MAAO,KAAK,IACnC,KAAK,SAED,EAAc,EAAe,OACnC,EAAe,SAEf,IAAM,EAAO,EAAW,OACtB,GACC,KAAK,IAAM,EAAI,KAAK,IAAI,KAAK,MAAO,KAAK,KAE5C,EAAY,SAEZ,IAAM,EAAW,EAAM,IAAI,GAE3B,OADA,EAAK,SACE,CAAC,WAAU,UAAS,aAjJ/B,CAAI,IAAJ,eAAI,MAAJ,SAqJI,EACA,EACA,GAEA,IAAM,EAAa,GAAsB,KACvC,CACE,KAAM,EACN,QAAS,EACT,MAAO,KAAK,MACZ,MAAO,KAAK,MACZ,EAAG,KAAK,GAEV,EAAM,OAcR,OAZA,EAAQ,SAYD,CAAC,SAVS,GAAoB,KACnC,CACE,MAAO,EACP,QAAS,EACT,MAAO,KAAK,GACZ,QAAS,KAAK,SAEhB,EAAM,OAGU,QAAS,OA/K/B,GAA0B,ICL1B,IAAa,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,kDAWQ,EAAW,GACf,gBAAU,EAAV,0BAA6B,EAA7B,uBAA6C,EAA7C,SAZJ,GAEU,IAcG,GAAkB,IAAI,IACjC,SAAC,GAAD,OAAqB,IAAI,GAAiB,GAAgB,MClB/C,GAAb,WACI,SAAF,EAAmB,EAA2B,GAAkB,oBAA7C,SAA2B,SAD9C,uBAAE,IAAJ,WAAI,MAAJ,SAGW,GACD,IAAF,ECRN,EACA,EDQQ,GAAF,aAAgB,EAAW,CAC7B,IAAM,GCVV,EDWM,KAAK,EAAE,MCVb,EDWM,KAAK,EAAE,MCTN,EACL,EACA,GACA,SAAC,EAAW,GACV,OAAU,IAAN,GACM,EAAI,EAEL,GAAK,EAAI,KAGpB,EAAE,QDCA,EAAQ,EAAK,SAAS,GACtB,EAAK,cACA,GAAI,aAAgB,GAAY,CACrC,IAAM,EAAS,KAAK,EAAE,MACnB,WAAmB,SACnB,KAAK,EAAE,MAA0B,YAE5B,MAAI,GACV,EAAK,WAAW,SAAS,EAAM,GAC/B,EAAK,OAEP,EAAK,WACA,CACL,IAAM,EAAO,GAAgB,KAC3B,CACE,EAAG,KAAK,EAAE,MACV,EAAG,KAAK,EAAE,MACV,YAAa,KAAK,EAAE,YAEtB,KAAK,EAAE,MAAM,OAEf,EAAQ,EAAK,SAAS,GACtB,EAAK,SAEQ,KAAK,EAAE,SAAS,IAE7B,EAAM,WApCZ,CAAI,IAAJ,SAAI,MAAJ,WAyCS,KAAK,EAAE,UACV,KAAK,EAAE,SAGJ,KAAK,EAAE,UACV,KAAK,EAAE,aA9Cb,KECA,IAAa,GAAb,kDAGE,WACE,EACA,EACA,GAA8B,uCAExB,EAAmB,EAAO,GARpC,kDAWQ,EAAW,GACf,gBAAU,EAAV,0BAA6B,EAA7B,wBAA8C,EAA9C,SAZJ,GAEU,IAcG,GAAc,IAAI,IAC7B,SAAC,GAAD,OAAqB,IAAI,GAAa,GAAgB,MCLlD,SAAU,GACd,EACA,GAEE,IAAG,GAAS,EAAG,GACX,MAAE,IAAI,MAAM,yDAEhB,OAAE,aAAa,GAAa,aAAa,ECzBvC,SACJ,EACA,GAEE,OAAK,EACL,EACA,GACA,SAAC,EAAW,GACV,OAAU,IAAN,GACM,KAAK,IAAI,IAET,KAAK,IAAI,EAAI,KAGzB,EAAE,ODYK,CAAO,EAAG,GACR,aAAa,IAAc,aAAa,GAC1C,IAAI,GAAW,EAAE,WAAW,IAAI,EAAE,aAChC,aAAa,IAAa,aAAa,GACzC,GAAY,KACjB,CACE,EAAG,EACH,EAAG,EACH,YAAa,EAAE,YAEjB,EAAE,OAGG,IAAI,GACT,GAAK,EAAqB,MAAQ,EAAqB,OACvD,CACE,OAAS,EAAqB,OAC9B,SAAW,EAAqB,YAC5B,EACA,IAAI,GAAQ,EAAqB,KE/CtC,SAAeC,GAAtB,mC,8CAAO,WAAyBC,GAAzB,mBAAAd,EAAA,sEACee,MAAM,UAAD,OAAWD,EAAX,UADpB,cACGE,EADH,gBAEkBA,EAAIC,cAFtB,cAEGC,EAFH,OAIGC,EAAQ,IAAIC,EAASD,MAAME,UAAUH,GAJxC,SAKGC,EAAMG,QALT,gCAOIH,GAPJ,6C,gLCwBDI,GAAa,KAkBnB,SAASC,GAAKC,GACZ,OAAO,IAAIC,SAAc,SAACC,EAASC,GACjCC,YAAW,WACTF,MACCF,M,SAIQK,G,qFAAf,WAAyBvC,EAAWkC,EAAWM,GAA/C,eAAA/B,EAAA,sDACWb,EAAI,EADf,YACkBA,EAAII,GADtB,uBAEIwC,EAAGxC,EAAEJ,GAFT,SAGUqC,GAAKC,GAHf,OACyBtC,IADzB,sBAKE4C,EAAG,GALL,4C,sBAQA,IAAMC,GAAS,CACb,CACEC,KAAM,EACNnB,KAAM,gBAER,CACEmB,KAAM,GACNnB,KAAM,gBAER,CACEmB,KAAM,IACNnB,KAAM,gBAER,CACEmB,KAAM,IACNnB,KAAM,eACNN,QAAS,CAAC,OACV0B,MAAO,CAAC,SAitBGC,G,kDAzrBb,WAAYC,GAAY,IAAD,8BACrB,cAAMA,IApBAjB,WAAmCkB,EAmBpB,EAlBfC,gBAkBe,IAhBfC,UAgBe,IAffC,aAee,IAbfC,cAAgB,IAAIrB,EAAWsB,IAAIC,UAAU,CAAC,KAAO,KAAO,MAAQ,CAAC,EAAE,EAAE,IAa1D,EAZfC,aAAe,IAAIxB,EAAWsB,IAAIC,UAAU,CAAC,KAAO,KAAO,MAAQ,CAAC,EAAE,EAAE,IAYzD,EAVfE,UAUe,IARfC,yBAA2B,EAQZ,EANfC,iBAMe,IAJfC,WAIe,IAFfC,OAAS,EAKf,EAAKC,MAAQ,CACXC,MAAO,gBACPC,cAAe,EACfC,UAAW,GACXC,OAAO,EACPC,cAAc,EACdpC,MAAO,eACPqC,gBAAiB,GACjBC,qBAAsB,GAXH,E,iEAgBrB9D,KAAK+D,SAAS,gBAEd/D,KAAKqD,MAAQ,IAAIW,MAAM,qB,0CAGJ,IAAD,OAClB9B,YAAW,WACT,EAAK+B,WAAWC,MAAK,SAAAC,GACnB,EAAKC,oBAEN,O,2KAIG5C,EAAQ6C,aAAaC,QAAQ,SAC7B1B,EAAOyB,aAAaC,QAAQ,QAC5BzB,EAAUwB,aAAaC,QAAQ,WAC/BC,EAAmBF,aAAaC,QAAQ,oBAEhC,OAAV9C,GAA2B,OAAToB,GAA6B,OAAZC,GAAyC,OAArB0B,E,wBACnDC,EAAmBC,KAAKC,MAAMH,GAEpCvE,KAAK2C,WAAa,IAAIgC,GAAO/C,GAAY,GAAG,GAC5C5B,KAAK2C,WAAWiC,QAAUC,GAASC,OAAO,CAAClD,GAAY,GAAI4C,EAAiB,GAAI,OAChFxE,KAAK2C,WAAWoC,KAAOF,GAASC,OAAO,CAAC,GAAIN,EAAiB,GAAI,OAEjExE,KAAK4C,KAAO,IAAInB,EAAWsB,IAAIC,UAAUyB,KAAKC,MAAM9B,GAAO,CAAChB,KAC5D5B,KAAK6C,QAAU,IAAIpB,EAAWsB,IAAIC,UAAUyB,KAAKC,MAAM7B,GAAU,CAACjB,K,UAE5D5B,KAAK+D,SAASvC,G,eACpBxB,KAAKgF,SAAS,CACZxB,MAAO,aACPyB,WAAY,I,UAGRpD,GAAK,K,QAELqD,EAAWlF,KAAKmF,eAEtB,UAAAnF,KAAKwB,aAAL,SAAY4D,QAAQ,CAACF,IAAWhB,MAAK,SAAAmB,GACnCH,EAASI,SACT,EAAKC,QAAQF,M,4QAMXxD,GAAK,K,UAEc,eAArB7B,KAAKuD,MAAMC,M,iBACP0B,EAAWlF,KAAKmF,eACb3F,EAAI,E,YAAGA,EAAI,G,2CAEGQ,KAAKwB,a,aAAL,EAAY4D,QAAQ,CAACF,I,YAC3BxC,KADThD,E,SAEJA,EAAO,GAAG4F,S,QAJS9F,I,uBAOvB0F,EAASI,SAETtF,KAAKgF,SAAS,CACZxB,MAAO,U,+KAKErC,G,gFACPK,EAAQa,GAAOmD,MAAK,SAAArB,GAAC,OAAIA,EAAEhD,OAASA,KAE1CnB,KAAKgF,SAAS,CACZxD,MAAOL,I,SAGUD,GAAUC,G,OAA7BnB,KAAKwB,M,YAEckB,IAAf1C,KAAKwB,QACPxB,KAAKwB,MAAMX,SAAe,OAALW,QAAK,IAALA,OAAA,EAAAA,EAAOX,UAAW,CAAC,OACxCb,KAAKwB,MAAMe,OAAW,OAALf,QAAK,IAALA,OAAA,EAAAA,EAAOe,QAAS,CAAC,SAGpC,UAAAvC,KAAKwB,aAAL,SAAYiE,W,6PAINC,EAA2BC,SAASC,cAAc,kBAEpDC,UAAUC,aAAaC,a,gCACJF,UAAUC,aAAaC,aAAa,CACvDL,MAAO,CACLM,OAAQ,IACRC,MAAO,O,OAHLC,E,OAMNR,EAAMS,UAAYD,EAElBlG,KAAKgF,SAAS,CACZxB,MAAO,eAGTxD,KAAKoG,S,2IAKP,IAAMV,EAA2BC,SAASC,cAAc,iBAExD5F,KAAKoD,YAAc3B,EAAWsB,IAAIC,UAAUqD,SAASX,GAHxC,MAKS1F,KAAKoD,YAAY9D,MAAMgH,MAAM,EAAE,GALxC,mBAKRN,EALQ,KAKAC,EALA,KAOPM,EAASvG,KAAKoD,YAAYkD,MAAM,CAAC,GAAI,CAAC,GAAI,CAAC,IAEjDtG,KAAKoD,YAAYkC,SAEjB,IAAMkB,EAAaD,EAAOE,UAAU,CAAC,EAAG,EAAG,IAC3CF,EAAOjB,SAEP,IAAMoB,EAASF,EAAWG,SAAS3G,KAAK8C,eACxC0D,EAAWlB,SACX,IAAMsB,EAAaF,EAAOG,OAAO7G,KAAKiD,cAItC,OAHAyD,EAAOpB,SAEUsB,EAAWE,QAAQ,CAAC,EAAE,EAAEd,EAAOC,IAAQ,K,wKAKlDP,EAA2BC,SAASC,cAAc,iBACxD5F,KAAKoD,YAAc3B,EAAWsB,IAAIC,UAAUqD,SAASX,QAC5BhD,IAArB1C,KAAKoD,Y,wBACPpD,KAAKgF,SAAL,2BACKhF,KAAKuD,OADV,IAEEC,MAAO,eAGHuD,EAAa/G,KAAKuD,MAAMM,gBAAkB7D,KAAKuD,MAAMO,qBAErDoB,EAAWlF,KAAKmF,eAEtBnF,KAAKkD,KAAO,IAAIzB,EAAWsB,IAAIC,UAAU,IAAIgE,MAAMD,EAAWnF,IAAYqF,KAAK,GAAI,CAACF,EAAYnF,KAEhGsF,QAAQC,IAAI,sB,UAENhF,GAAU,EAAG,KAAM,SAACvC,GACxB,EAAKoF,SAAS,CACZ7C,UAAWvC,O,QAIfI,KAAKgF,SAAS,CACZxB,MAAO,cACP4D,SAAU,IAGZ,UAAApH,KAAKwB,aAAL,SAAY4D,QAAQ,CAACF,IAAWhB,MAAK,SAAAxE,GACnCwF,EAASI,SACT,EAAK+B,aAAa3H,M,2IAKV4H,GACZ,IAAMC,EAAOD,EAAQ,GAErB,OADAJ,QAAQC,IAAII,GACLA,I,4EAGUD,G,iGACXE,EAAYxH,KAAKyH,cAAcH,GAE/BI,EAAa1H,KAAKkD,KAExBlD,KAAKkD,KAAOlD,KAAKkD,KAAKyE,UAAUH,EAAW,CAACxH,KAAKmD,yBAA0B,IAC3EqE,EAAUlC,SAEVoC,EAAWpC,SAELyB,EAAa/G,KAAKuD,MAAMM,gBAAkB7D,KAAKuD,MAAMO,qBAE3D9D,KAAKmD,2BAEL+D,QAAQC,IAAR,oBAAyBnH,KAAKmD,yBAA9B,eAA6D4D,MAEzD/G,KAAKmD,yBAA2B4D,EAAW,G,wBAC7C/G,KAAKgF,SAAS,CACZoC,SAAUpH,KAAKmD,0BAA0B4D,EAAW,GAAG,M,UAGnDlF,GAAK,I,QAELqD,EAAWlF,KAAKmF,eAEtB,UAAAnF,KAAKwB,aAAL,SAAY4D,QAAQ,CAACF,IAAWhB,MAAK,SAAAmB,GACnCH,EAASI,SACT,EAAK+B,aAAahC,M,2BAEXrF,KAAKmD,2BAA6B4D,EAAW,E,wBACtDG,QAAQC,IAAI,gBAEZnH,KAAKgF,SAAS,CACZxB,MAAO,mB,UAGHrB,GAAU,EAAG,KAAM,SAACvC,GACxB,EAAKoF,SAAS,CACZ7C,UAAWvC,O,QAIfI,KAAKgF,SAAS,CACZxB,MAAO,cACP4D,SAAU,IAGNlC,EAAWlF,KAAKmF,eAEtB,UAAAnF,KAAKwB,aAAL,SAAY4D,QAAQ,CAACF,IAAWhB,MAAK,SAAAmB,GACnCH,EAASI,SACT,EAAK+B,aAAahC,M,6BAEXrF,KAAKmD,yBAA2B4D,G,wBACzC/G,KAAKgF,SAAS,CACZoC,SAA6D,KAAlDpH,KAAKmD,0BAA0B4D,EAAW,GAAK,K,UAGtDlF,GAAK,I,QACLqD,EAAWlF,KAAKmF,eAEtB,UAAAnF,KAAKwB,aAAL,SAAY4D,QAAQ,CAACF,IAAWhB,MAAK,SAAAmB,GACnCH,EAASI,SACT,EAAK+B,aAAahC,M,+BAGpBrF,KAAKgF,SAAL,2BACKhF,KAAKuD,OADV,IAEEC,MAAO,WACP4D,SAAU,K,UAGNvF,GAAK,I,QAEX7B,KAAK4H,kB,wLAIc1E,G,kFACrBlD,KAAK4C,KAAOM,EAAK2E,WAAW,GACtBC,EAAU5E,EAAKyD,SAAS3G,KAAK4C,MAE7BmF,EAAWD,EAAQE,UAAU,GACnChI,KAAK6C,QAAUkF,EAASE,OACxBF,EAASzC,SAEHsB,EAAakB,EAAQjB,OAAO7G,KAAK6C,SACvCiF,EAAQxC,S,kBACDsB,G,4RAIDG,EAAa/G,KAAKuD,MAAMM,gBAAkB7D,KAAKuD,MAAMO,qBAGrDoE,EAAalI,KAAKkD,KAAKoD,MAAM,CAAC,GAAG,CAACtG,KAAKuD,MAAMM,gBAAgB,GAAG,CAAC,IAEjEsE,EAAanI,KAAKkD,KAAKoD,MAAM,CAACS,EAAW,GAAG,CAACA,EAAW,EAAI/G,KAAKuD,MAAMM,gBAAgB,GAAG,CAAC,IAC7FuE,EAAYF,EAAWG,OAAOF,EAAY,GAGxCG,EAAWtI,KAAKkD,KAAKoD,MAAM,CAACtG,KAAKuD,MAAMM,gBAAgB,GAAG,CAACkD,EAAW,GAAG,CAAC,IAE1EwB,EAAWvI,KAAKkD,KAAKoD,MAAM,CAACS,EAAW,EAAI/G,KAAKuD,MAAMM,gBAAgB,GAAG,CAACkD,GAAY,CAAC,IACzFyB,EAAUF,EAASD,OAAOE,EAAU,G,SAEtBvI,KAAKyI,iBAAiBL,G,cAAxCA,E,OAEMM,EAAiBF,EAAQ7B,SAAS3G,KAAK4C,MACvC+F,EAAoBD,EAAe7B,OAAO7G,KAAK6C,SACrD2F,EAAQlD,SACRoD,EAAepD,SACfkD,EAAUG,EAEJC,EAAS,IAAI/D,GAASuD,EAAW,CAACS,QAAQ,IAC1CC,E,sBAAc,IAAI9B,MAAMhH,KAAKuD,MAAMM,gBAAgB,GAAGoD,KAAK,I,YAAM,IAAID,MAAMhH,KAAKuD,MAAMM,gBAAgB,GAAGoD,KAAK,KAC9G8B,EAASlE,GAASC,OAAO,CAAC9E,KAAKuD,MAAMM,gBAAiB,GAAIiF,EAAS,MAAO,CAACD,QAAQ,IAEnFG,EAAO,IAAInE,GAAS2D,EAAS,CAACK,QAAQ,IACtCI,E,sBAAY,IAAIjC,MAAMhH,KAAKuD,MAAMO,qBAAqB,GAAGmD,KAAK,I,YAAM,IAAID,MAAMhH,KAAKuD,MAAMO,qBAAqB,GAAGmD,KAAK,K,kBAErH,CAAC2B,SAAQE,UAASC,SAAQC,OAAMC,U,mTAIvCjJ,KAAK2C,WAAa,IAAIgC,GAAO/C,GAAY,GAAG,G,SACtC5B,KAAK2C,WAAWhB,Q,uBAE+B3B,KAAKkJ,c,gBAAnDN,E,EAAAA,OAAQE,E,EAAAA,QAASC,E,EAAAA,OAAQC,E,EAAAA,KAAMC,E,EAAAA,MAEhCE,EAAY,IAAIC,GAAKpJ,KAAK2C,YAE1BW,EAAS,IAEN9D,EAAI,E,aAAGA,EAAI8D,G,wBAClBtD,KAAKgF,SAAL,2BACKhF,KAAKuD,OADV,IAEE6D,SAAU5H,EAAE8D,EAAO,O,UAGfzB,GAAK,I,yBAES7B,KAAK2C,WAAWyC,QAAQ,CAACwD,I,WAAvCS,E,OAAiD,GACjD9I,EAAU8I,EAAK9I,UAEf+I,EAAOC,GAAIhJ,EAASwI,GAAQlB,aAE9BrI,EAAI,KAAO,E,6BACb0H,Q,KAAY1H,E,UAAU8J,EAAKE,Y,2BAAa,G,KAAhCrC,I,0BAEoBH,M,UAAiBzG,EAAQiJ,Y,2BAA/CC,E,KAA4BC,K,gBAC9BC,EAAe3J,KAAK4J,WAAWH,EAAWX,GAC9C5B,QAAQC,IAAR,UAAewC,EAAf,eAAkCF,EAAUI,OAA5C,yB,UAEuB7J,KAAK2C,WAAWyC,QAAQ,CAAC4D,I,eAA1Cc,E,OAAkD,GAClDC,EAAaD,EAAQvJ,U,KACEyG,M,UAAiB+C,EAAWP,Y,oBAAnDQ,E,KAA6BN,K,gBAC/BO,EAAajK,KAAK4J,WAAWI,EAAYf,GAC7C/B,QAAQC,IAAR,UAAe8C,EAAf,eAAgCD,EAAWH,OAA3C,yBACAE,EAAWzE,S,QAGbgE,EAAKY,WACLf,EAAUgB,OAEVb,EAAKhE,SACL6D,EAAUiB,Y,QAhCgB5K,I,+BAmC5BQ,KAAKgF,SAAL,2BACKhF,KAAKuD,OADV,IAEEC,MAAO,aACPyB,WAAY,K,UAGRpD,GAAK,K,QAEX7B,KAAKqK,oBAECnF,EAAWlF,KAAKmF,eAEtB,UAAAnF,KAAKwB,aAAL,SAAY4D,QAAQ,CAACF,IAAWhB,MAAK,SAAAmB,GACnCH,EAASI,SACT,EAAKC,QAAQF,M,mRAKTiF,E,UAAStK,KAAK2C,kB,aAAL,EAAiB4H,gBAC1BC,EAAc,G,cACFF,G,gEAATG,E,aACPD,E,KAAiBxD,M,UAAiByD,EAAMjB,Y,8BAAjBE,K,qBAAXgB,K,sKAGdrG,aAAasG,QAAQ,mBAAoBlG,KAAKmG,UAAUJ,I,KAErCxD,M,oBAAiBhH,KAAK4C,Y,aAAL,EAAW4G,Y,2BAAzCqB,E,KAAmBnB,K,gBACzBrF,aAAasG,QAAQ,OAAQlG,KAAKmG,UAAUC,I,KAE1B7D,M,oBAAiBhH,KAAK6C,e,aAAL,EAAc2G,Y,oBAA3CsB,E,KAAkBpB,K,gBACxBrF,aAAasG,QAAQ,UAAWlG,KAAKmG,UAAUE,IAE/CzG,aAAasG,QAAQ,QAAS3K,KAAKuD,MAAM/B,O,+JAGxBf,GAAiB,IAAD,OAC7BA,EAC8B,WAA5BsK,aAAaC,YAAuD,YAA5BD,aAAaC,WACvDD,aAAaE,oBAAoB/G,MAAK,SAAC8G,GAClB,YAAfA,IACF,IAAID,aAAa,gCACjB,EAAK/F,SAAS,CACZpB,cAAc,QAIiB,YAA5BmH,aAAaC,YACtBhL,KAAKgF,SAAS,CACZpB,cAAc,IAIlB5D,KAAKgF,SAAS,CACZpB,cAAc,M,uEAKN0D,G,qGACNE,EAAYxH,KAAKyH,cAAcH,GAE/BQ,EAAUN,EAAUb,SAAS3G,KAAK4C,MACxC4E,EAAUlC,SACJsB,EAAa,IAAI/B,GAASiD,EAAQjB,OAAO7G,KAAK6C,SAAiB,CAACgG,QAAQ,IAC9Ef,EAAQxC,S,SAGctF,KAAK2C,WAAWyC,QAAQ,CAACwB,I,cAAzCsE,E,OAAuD,GAC7DtE,EAAWtB,S,UACK4F,EAAO1B,Y,WAAjB2B,E,OACND,EAAO5F,SAED/E,EAAU,GAAG,EAAE6K,KAAKC,KAAKF,EAAE,KAEjCnL,KAAKgF,SAAL,2BACKhF,KAAKuD,OADV,IAEE0B,WAAY1E,KAGVA,EAAUP,KAAKuD,MAAMG,WACvB1D,KAAKsD,SACDtD,KAAKsD,SAAWtD,KAAKuD,MAAME,gBACzBzD,KAAKuD,MAAMI,QACb,UAAA3D,KAAKqD,aAAL,SAAYiI,QAEVtL,KAAKuD,MAAMK,cACb,IAAImH,aAAa,6BAIrB/K,KAAKsD,OAAS,EAGS,eAArBtD,KAAKuD,MAAMC,M,kCACP3B,GAAK,K,QAELqD,EAAWlF,KAAKmF,eAEtB,UAAAnF,KAAKwB,aAAL,SAAY4D,QAAQ,CAACF,IAAWhB,MAAK,SAAAmB,GACnCH,EAASI,SACT,EAAKC,QAAQF,M,yIAKRkG,EAAoBC,GAE7B,IADA,IAAIC,EAAU,EACLjM,EAAI,EAAGA,EAAG+L,EAAS1B,OAAQrK,KACnB,IAAVgM,EAAGhM,IAAY+L,EAAS/L,GAAK,IAAmB,IAAVgM,EAAGhM,IAAY+L,EAAS/L,GAAK,KACtEiM,IAGJ,OAAOA,I,+BAIP,OACE,qBAAKC,UAAU,MAAf,SACE,eAACC,GAAA,EAAD,CAAOC,UAAW,EAAGC,MAAO,CAACC,QAAS,QAAtC,UACE9L,KAAK+L,iBAEL,cAACC,GAAA,EAAD,CAAYC,QAAQ,KAAKC,UAAU,KAAnC,kCAHF,kDAMiD,uBAE/C,sBAAKR,UAAU,cAAf,UACE,uBAAOS,UAAQ,EAACC,GAAG,iBAAgB,uBAClCpM,KAAKqM,wB,uCAQd,OACE,sBAAKX,UAAU,WAAf,UACG1L,KAAKsM,wBACLtM,KAAKuM,2B,8CAKa,IAAD,OACtB,OACE,eAACC,GAAA,EAAD,WACE,cAACC,GAAA,EAAD,CACEC,gBAAc,kBACdN,GAAG,iBAFL,SAIE,cAACJ,GAAA,EAAD,CAAYC,QAAQ,KAApB,wBAEF,cAACU,GAAA,EAAD,UACE,eAACC,GAAA,EAAD,CAAMC,WAAS,EAACC,QAAS,EAAzB,UACE,eAACF,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,GAAf,UACE,cAAChB,GAAA,EAAD,CAAYI,GAAG,yBAAyBa,cAAY,EAApD,uBAEa,2BAEf,cAACL,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,GAAf,SACE,cAACE,GAAA,EAAD,CACEC,kBAAgB,yBAChB1M,MAAOT,KAAKuD,MAAMG,UAClB0J,IAAK,EACLC,IAAK,EACLlD,KAAM,GACNmD,SAAU,SAACC,EAAIpC,GAAL,OAAW,EAAKnG,SAAS,CAACtB,UAAWyH,KAC/CqC,kBAAkB,WAGtB,eAACZ,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,GAAf,UACE,cAAChB,GAAA,EAAD,CAAYI,GAAG,yBAAyBa,cAAY,EAApD,iCAEa,2BAEf,cAACL,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,GAAf,SACE,cAACE,GAAA,EAAD,CACEC,kBAAgB,yBAChB1M,MAAOT,KAAKuD,MAAME,cAClB2J,IAAK,EACLC,IAAK,GACLlD,KAAM,EACNmD,SAAU,SAACC,EAAIpC,GAAL,OAAW,EAAKnG,SAAS,CAACvB,cAAe0H,KACnDqC,kBAAkB,WAGtB,cAACZ,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,EAAf,SACE,cAACS,GAAA,EAAD,CACEC,QACE,cAACC,GAAA,EAAD,CACEC,QAAS5N,KAAKuD,MAAMI,MACpB2J,SAAU,SAACC,GAAD,OAAQ,EAAKvI,SAAS,CAACrB,MAAO4J,EAAGM,OAAOD,WAClDzM,KAAK,WACL2M,MAAM,YAGVC,MAAM,iBAGV,cAACnB,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,EAAf,SACE,cAACS,GAAA,EAAD,CACEC,QACE,cAACC,GAAA,EAAD,CACEC,QAAS5N,KAAKuD,MAAMK,aACpB0J,SAAU,SAACC,GAAD,OAAQ,EAAKS,mBAAmBT,EAAGM,OAAOD,UACpDzM,KAAK,WACL2M,MAAM,YAGVC,MAAM,kC,4CASG,IAAD,OAwBpB,OACE,eAACvB,GAAA,EAAD,WACE,cAACC,GAAA,EAAD,CACEC,gBAAc,kBACdN,GAAG,iBAFL,SAIE,cAACJ,GAAA,EAAD,CAAYC,QAAQ,KAApB,qBAEF,cAACU,GAAA,EAAD,UACE,eAACC,GAAA,EAAD,CAAMC,WAAS,EAACC,QAAS,EAAzB,UACE,eAACF,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,GAAf,UACE,cAAChB,GAAA,EAAD,CAAYI,GAAG,yBAAyBa,cAAY,EAApD,wBAEa,2BAEf,cAACL,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,GAAf,SACE,eAACiB,GAAA,EAAD,WACE,cAACC,GAAA,EAAD,CAAY9B,GAAG,mBAAf,iBACA,cAAC+B,GAAA,EAAD,CACEC,QAAQ,mBACRhC,GAAG,aACH3L,MAAOT,KAAKuD,MAAM/B,MAClB8L,SAAU,SAACe,GAAD,OAAW,EAAKtK,SAASsK,EAAMR,OAAOpN,QAJlD,SAMG4B,GAAOiM,KAAI,SAAAnK,GAAC,OACX,cAACoK,GAAA,EAAD,CAAU9N,MAAO0D,EAAEhD,KAAnB,SAA0BgD,EAAE7B,iBAKpC,eAACsK,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,GAAf,UACE,cAAChB,GAAA,EAAD,CAAYI,GAAG,6BAA6Ba,cAAY,EAAxD,wCAEa,2BAEf,cAACL,GAAA,EAAD,CAAMG,MAAI,EAACC,GAAI,GAAf,SACE,cAACE,GAAA,EAAD,CACEC,kBAAgB,6BAChB1M,MAAOT,KAAKuD,MAAMM,gBAClBsG,KAAM,KACNqD,kBAAkB,OAClBgB,MAhEE,CACZ,CACE/N,MAAO,GACPsN,MAAO,MAET,CACEtN,MAAO,GACPsN,MAAO,MAET,CACEtN,MAAO,GACPsN,MAAO,MAET,CACEtN,MAAO,IACPsN,MAAO,OAET,CACEtN,MAAO,IACPsN,MAAO,QA8CCX,IAAK,GACLC,IAAK,IACLC,SAAU,SAACC,EAAIpC,GAAL,OAAW,EAAKnG,SAAS,CAACnB,gBAAiBsH,mB,oCASpD,IAAD,OACZ,GAAyB,kBAArBnL,KAAKuD,MAAMC,MACb,OAAQ,qEACH,GAAyB,eAArBxD,KAAKuD,MAAMC,MACpB,OAAQ,2DAEL,GAAyB,UAArBxD,KAAKuD,MAAMC,MAClB,OAAQ,cAACiL,GAAA,EAAD,CAAQxC,QAAQ,YAAY6B,MAAM,UAAUY,QAAS,kBAAM,EAAKC,mBAAhE,mBACH,GAAyB,cAArB3O,KAAKuD,MAAMC,MACpB,OAAQ,iGAA4DxD,KAAKuD,MAAMpB,UAAvE,gBACH,GAAyB,gBAArBnC,KAAKuD,MAAMC,OAAgD,gBAArBxD,KAAKuD,MAAMC,MAC1D,OAAQ,cAACoL,GAAA,EAAD,CAAgB3C,QAAQ,cAAcxL,MAAOT,KAAKuD,MAAM6D,SAAUsE,UAAU,aAC/E,GAAyB,mBAArB1L,KAAKuD,MAAMC,MACpB,OAAQ,kGAA6DxD,KAAKuD,MAAMpB,UAAxE,gBACH,GAAyB,aAArBnC,KAAKuD,MAAMC,MACpB,OACE,+EACE,cAACoL,GAAA,EAAD,CAAgB3C,QAAQ,gBAAgBP,UAAU,gBAGjD,GAAyB,eAArB1L,KAAKuD,MAAMC,MAAwB,CAC5C,IAAM6F,EAAQrJ,KAAKuD,MAAM0B,WAGnB6I,EAAK,cAAU1C,KAAKyD,MADT,IACexF,GAArB,YADM,IAC0C+B,KAAKyD,MAD/C,IACqDxF,GAA3D,OAEX,OACE,kGACE,qBAAKwC,MAAO,CACR5F,MAAO,IACPD,OAAQ,GACR8I,gBAAiB,OACjBC,aAAc,EACdC,SAAU,SACVC,OAAQ,QANZ,SAQE,qBAAKpD,MAAO,CACV5F,MAAY,IAALoD,EACPrD,OAAQ,GACR8I,gBAAiBhB,e,GApsBboB,IAAMC,WCnETC,GAZS,SAACC,GACnBA,GAAeA,aAAuBC,UACxC,8BAAqBpL,MAAK,YAAkD,IAA/CqL,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOF,GACPG,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAQN,OCHdO,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,GAAD,MAEFnK,SAASoK,eAAe,SAM1BX,M","file":"static/js/main.c5a751e8.chunk.js","sourcesContent":["export function getSize(shape, zeroSize = 0) {\n    if (shape.length === 0) {\n        return zeroSize;\n    }\n    let size = 1;\n    for (let i = 0; i < shape.length; i += 1) {\n        size *= shape[i];\n    }\n    return size;\n}\nexport function computeStrides(shape) {\n    const rank = shape.length;\n    if (rank === 0) {\n        return [];\n    }\n    if (rank === 1) {\n        if (shape[0] === 1) {\n            return [0];\n        }\n        else {\n            return [1];\n        }\n    }\n    const strides = new Array(rank);\n    strides[rank - 1] = 1;\n    if (shape[rank - 1] === 1) {\n        strides[rank - 1] = 0;\n    }\n    let lastStride = 1;\n    for (let i = rank - 2; i >= 0; i -= 1) {\n        lastStride = shape[i + 1] * lastStride;\n        if (shape[i] === 1) {\n            strides[i] = 0;\n        }\n        else {\n            strides[i] = lastStride;\n        }\n    }\n    return strides;\n}\nexport function indexToPos(index, strides, shape) {\n    let ix = 0;\n    for (let i = 0; i < index.length; i += 1) {\n        if (shape) {\n            if (index[i] < 0 || (index[i] >= shape[i] && shape[i] !== 1)) {\n                throw new Error('Invalid index');\n            }\n        }\n        ix += index[i] * strides[i];\n    }\n    return ix;\n}\nexport function posToIndex(pos, strides) {\n    let res = pos;\n    const rank = strides.length;\n    const index = new Array(rank);\n    for (let i = 0; i < index.length; i += 1) {\n        index[i] = Math.floor(res / strides[i]);\n        res %= strides[i];\n    }\n    return index;\n}\nexport function compareShapes(a, b) {\n    if (a.length !== b.length) {\n        return false;\n    }\n    for (let i = 0; i < a.length; i += 1) {\n        if (a[i] !== b[i]) {\n            return false;\n        }\n    }\n    return true;\n}\nexport function checkEquivShapes(a, b) {\n    if (a.length !== b.length) {\n        return false;\n    }\n    for (let i = 0; i < a.length; i += 1) {\n        if (a[i] !== b[i] && a[i] !== 1 && b[i] !== 1) {\n            return false;\n        }\n    }\n    return true;\n}\nexport function incrementIndex(index, shape) {\n    for (let i = index.length - 1; i >= 0; i--) {\n        index[i] += 1;\n        if (index[i] >= shape[i]) {\n            index[i] = 0;\n        }\n        else {\n            break;\n        }\n    }\n}\nexport function decrementIndex(index, shape) {\n    for (let i = index.length - 1; i >= 0; i--) {\n        index[i] -= 1;\n        if (index[i] < 0) {\n            index[i] = shape[i] - 1;\n        }\n        else {\n            break;\n        }\n    }\n}\n//# sourceMappingURL=shape.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { compareShapes, getSize } from './util/shape';\nexport const tensorValuesConstructor = {\n    float64: Float64Array,\n    float32: Float32Array,\n    float16: Float32Array,\n    int32: Int32Array,\n    int16: Int16Array,\n    int8: Int8Array,\n    uint32: Uint32Array,\n    uint16: Uint16Array,\n    uint8: Uint8Array,\n};\n/**\n * Multi-dimensional array ala numpy.\n *\n * A tensor is any multidimensional array. The number of\n * dimensions is called the rank, and the size of all dimensions the shape.\n *\n * @example\n * ```typescript\n * const a = [[1,2,3],[4,5,6]];\n * ```\n * here a has rank 2 and shape [2,3].\n *\n * Tensors store values of a particular data type like floats or integers.\n * The datatype can be accessed via the dtype property.\n *\n * Many operations can be done on tensors. For fast execution, three different\n * backends exist:\n * - CPU: Simple to use and works in any browser, but not particularly fast\n * - WebAssembly: Reasonably fast and works in most modern browsers\n * - WebGL: Very fast when a GPU is available, but comes with some restrictions\n */\nexport default class Tensor {\n    constructor(dtype) {\n        this.dtype = dtype;\n    }\n    /**\n     * Compares this tensor to another tensor.\n     *\n     * @param tensor Tensor to compare to\n     * @param epsilon Optional maximum difference between the tensors. If not specified the tensors have to be exactly equal\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2], [1,2,3,4]);\n     * const b = new CPUTensor([2,2], [1.1,2.1,2.9,4.05]);\n     * const c = new CPUTensor([4], [1,2,3,4]);\n     * a.compare(b, 0.5).then(equal => {\n     *  //equal will be true\n     * });\n     *\n     * a.compare(b).then(equal => {\n     *  //equal will be false\n     * });\n     *\n     * a.compare(c).then(equal => {\n     *  //equal will be false since the shapes of the tensors do not match\n     * });\n     * ```\n     */\n    compare(tensor, epsilon) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (!compareShapes(this.getShape(), tensor.getShape())) {\n                return false;\n            }\n            const arrA = yield this.getValues();\n            const arrB = yield tensor.getValues();\n            if (epsilon !== undefined) {\n                for (let i = 0; i < arrA.length; i += 1) {\n                    if (Math.abs(arrA[i] - arrB[i]) > epsilon) {\n                        return false;\n                    }\n                }\n            }\n            else {\n                for (let i = 0; i < arrA.length; i += 1) {\n                    if (arrA[i] !== arrB[i]) {\n                        return false;\n                    }\n                }\n            }\n            return true;\n        });\n    }\n    getAxes(axes) {\n        let ax;\n        const sh = this.getShape();\n        if (axes === undefined) {\n            ax = [];\n            for (let i = 0; i < sh.length; i++) {\n                ax.push(i);\n            }\n        }\n        else if (!(axes instanceof Array)) {\n            ax = [axes];\n        }\n        else {\n            ax = axes;\n            for (let i = 0; i < ax.length; i++) {\n                if (ax[i] < 0) {\n                    ax[i] += this.getShape().length;\n                }\n            }\n        }\n        return ax;\n    }\n    /**\n     * Sums over the specified axis/axes.\n     *\n     * @param axes One or multiple axes to sum over. If not specified this will sum over all axes\n     * @param keepDims Wether the summation axes will be kept with size 1\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,3], [1,2,3,4,5,6]);\n     *\n     * a.sum(); //Will be [21]\n     * a.sum(0); //Will be [5,7,9]\n     * a.sum(1); //Will [6,15]\n     * a.sum(0, true); //Will be [[5,7,9]]\n     * ```\n     */\n    sum(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.sum_impl(ax, keepDims);\n    }\n    /**\n     * Sums over the specified axis/axes with the entries of the tensor squared.\n     * This is equal to `a.multiply(a).sum(axes, keepDims)` but faster\n     *\n     * @param axes One or multiple axes to sum over. If not specified this will sum over all axes\n     * @param keepDims Wether the summation axes will be kept with size 1\n     *\n     */\n    sumSquare(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.sumSquare_impl(ax, keepDims);\n    }\n    /**\n     * Takes the product over specified axis/axes.\n     *\n     * @param axes One or multiple axes to take the product over. If not specified this will be all axes\n     * @param keepDims Wether the product axes will be kept with size 1\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,3], [1,2,3,4,5,6]);\n     *\n     * a.product(); //Will be [720]\n     * a.product(0); //Will be [4,10,18]\n     * a.product(1); //Will [6,120]\n     * a.product(0, true); //Will be [[4,10,18]]\n     * ```\n     */\n    product(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.product_impl(ax, keepDims);\n    }\n    /**\n     * Takes the maximum over specified axis/axes.\n     *\n     * @param axes One or multiple axes to take the maximum over. If not specified this will be all axes\n     * @param keepDims Wether the maximum axes will be kept with size 1\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,3], [1,2,3,4,5,6]);\n     *\n     * a.max(); //Will be [6]\n     * a.max(0); //Will be [4,5,6]\n     * a.max(1); //Will [3,6]\n     * a.max(0, true); //Will be [[4,5,6]]\n     * ```\n     */\n    max(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.max_impl(ax, keepDims);\n    }\n    /**\n     * Takes the minimum over specified axis/axes.\n     *\n     * @param axes One or multiple axes to take the minimum over. If not specified this will be all axes\n     * @param keepDims Wether the minimum axes will be kept with size 1\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,3], [1,2,3,4,5,6]);\n     *\n     * a.min(); //Will be [1]\n     * a.min(0); //Will be [1,2,3]\n     * a.min(1); //Will [1,4]\n     * a.min(0, true); //Will be [[1,2,3]]\n     * ```\n     */\n    min(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.min_impl(ax, keepDims);\n    }\n    /**\n     * Takes the mean over the specified axis/axes.\n     * This is equal to `a.sum(axes, keepDims).divide(sumSize)` (where sumSize is the number\n     * of entries in the summation axes) but faster.\n     *\n     * @param axes One or multiple axes to take the mean over. If not specified this will take the mean over all axes\n     * @param keepDims Wether the mean axes will be kept with size 1\n     *\n     */\n    reduceMean(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.reduceMean_impl(ax, keepDims);\n    }\n    /**\n     * Takes the log of the sum over the specified axis\n     * This is equal to `a.sum(axes, keepDims).log()` (where sumSize is the number\n     * of entries in the summation axes) but faster.\n     *\n     * Note that this can only be called on tensors with a float data type (float64, float32, float16)\n     *\n     * @param axes One or multiple axes to take the mean over. If not specified this will take the mean over all axes\n     * @param keepDims Wether the mean axes will be kept with size 1\n     *\n     */\n    reduceLogSum(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.reduceLogSum_impl(ax.sort(), keepDims);\n    }\n    /**\n     * Takes the log of the sum over the exp of the specified axis\n     * This is equal to `a.sum(axes, keepDims).log()` (where sumSize is the number\n     * of entries in the summation axes) but faster.\n     *\n     * Note that this can only be called on tensors with a float data type (float64, float32, float16)\n     *\n     * @param axes One or multiple axes to take the mean over. If not specified this will take the mean over all axes\n     * @param keepDims Wether the mean axes will be kept with size 1\n     *\n     */\n    reduceLogSumExp(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.reduceLogSumExp_impl(ax, keepDims);\n    }\n    /**\n     * Takes the mean over the specified axis/axes with the entries of the tensor squared.\n     * This is equal to `a.multiply(a).sum(axes, keepDims).divide(sumSize)` (where sumSize is the number\n     * of entries in the summation axes) but faster.\n     *\n     * @param axes One or multiple axes to take the mean over. If not specified this will take the mean over all axes\n     * @param keepDims Wether the mean axes will be kept with size 1\n     *\n     */\n    reduceMeanSquare(axes, keepDims) {\n        const ax = this.getAxes(axes);\n        keepDims = keepDims || false;\n        return this.reduceMeanSquare_impl(ax, keepDims);\n    }\n    /**\n     * Convolves this tensor with the specified kernel.\n     *\n     * This tensor should have shape [N,C,D1,D2,...] where D1,D2,... are the spatial dimensions.\n     *\n     * Behaves according to https://github.com/onnx/onnx/blob/master/docs/Operators.md#Conv\n     *\n     * @param kernel Convolution kernel with shape [M,C/G,K1,K2] where G is the group parameter\n     * @param bias Optional bias to add to the result with shape [M]\n     * @param dilations Per axis dilations for the spatial dimension. Defaults to 1 for all axes\n     * @param group Group parameter\n     * @param pads Padding to add to the input for each spatial dimension. Defaults to 0 for all axes\n     * @param strides Convolution stride for each spatial dimension. Defaults to 1 for all axes\n     * @param activation Optional activation to apply. Defaults to the identity (so no activation)\n     */\n    conv(kernel, bias, dilations, group, pads, strides, activation) {\n        const sh = this.getShape();\n        const dataRank = sh.length - 2;\n        dilations = dilations || new Array(dataRank).fill(1);\n        group = group || 1;\n        pads = pads || new Array(dataRank * 2).fill(0);\n        strides = strides || new Array(dataRank).fill(1);\n        if (activation === undefined) {\n            activation = 'id';\n        }\n        return this.conv_impl(kernel, dilations, group, pads, strides, activation, bias);\n    }\n    /**\n     * Calculates the transpose convolution\n     *\n     * This tensor should have shape [N,C,D1,D2,...] where D1,D2,... are the spatial dimensions.\n     *\n     * @param kernel Convolution kernel with shape [M,C/G,K1,K2] where G is the group parameter\n     * @param dilations Per axis dilations for the spatial dimension. Defaults to 1 for all axes\n     * @param group Group parameter\n     * @param pads Padding to add to the input for each spatial dimension. Defaults to 0 for all axes\n     * @param strides Convolution stride for each spatial dimension. Defaults to 1 for all axes\n     */\n    convTranspose(kernel, dilations, group, pads, strides) {\n        const sh = this.getShape();\n        const dataRank = sh.length - 2;\n        dilations = dilations || new Array(dataRank).fill(1);\n        group = group || 1;\n        pads = pads || new Array(dataRank * 2).fill(0);\n        strides = strides || new Array(dataRank).fill(1);\n        return this.convTranspose_impl(kernel, dilations, group, pads, strides);\n    }\n    /**\n     * Pads the input according to the padding mode. The input has shape [D1,D2,..]\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2],[1,2,3,4]);\n     * a.pad([1,1,1,1],'constant',5);\n     * //Result will be:\n     * // [[5,5,5,5],\n     * //  [5,1,2,5],\n     * //  [5,3,4,5],\n     * //  [5,5,5,5]]\n     * a.pad([1,1,1,1],'edge');\n     * //Result will be:\n     * // [[1,1,2,2],\n     * //  [1,1,2,2],\n     * //  [3,3,4,4],\n     * //  [3,3,4,4]]\n     *\n     * a.pad([2,2,2,2],'reflect');\n     * //Result will be:\n     * // [[4,3,3,4,4,3],\n     * //  [2,1,1,2,2,1],\n     * //  [2,1,1,2,2,1],\n     * //  [4,3,3,4,4,3],\n     * //  [4,3,3,4,4,3],\n     * //  [2,1,1,2,2,1]]\n     * ```\n     *\n     * @param pads Padding size of each input. Specified as [startpad_D1,startpad_D2,...,startpad_DN,endpad_D1,endpad_D2,...]\n     * @param mode Padding mode. One of 'constant', 'edge', 'reflect'. Defaults to 'constant'\n     * @param value Value for constant padding. Defaults to 0.0\n     */\n    pad(pads, mode, value) {\n        if (mode === undefined) {\n            mode = 'constant';\n        }\n        if (value === undefined) {\n            value = 0;\n        }\n        return this.pad_impl(pads, mode, value);\n    }\n    /**\n     * Performs average pooling over the spatial dimensions of this tensor with\n     * shape [N,C,D1,D2,..]\n     * @param kernelShape Size of the average pooling dimension\n     * @param pads Padding of the input specified as [startpad_D1,startpad_D2,...,startpad_DN,endpad_D1,endpad_D2,...]\n     *             Padding value will be 0. Defaults to 0 for all axes\n     * @param strides Stride size of the average pooling kernel. Defaults to 1 for all axes\n     * @param includePad Wether padded values should be included in the average (or masked out). Defaults to false\n     */\n    averagePool(kernelShape, pads, strides, includePad) {\n        const sh = this.getShape();\n        const dataRank = sh.length - 2;\n        pads = pads || new Array(dataRank * 2).fill(0);\n        strides = strides || new Array(dataRank).fill(1);\n        includePad = includePad || false;\n        return this.averagePool_impl(kernelShape, pads, strides, includePad);\n    }\n    /**\n     * Reshape the tensor to the specified shape\n     *\n     * At most one value in the shape can be -1, which will be replaced by the inferred size for this dimension.\n     *\n     * @param shape New shape of the tensor\n     * @param copy Wether the tensor values should be copied. Only has an effect on GPU tensors\n     */\n    reshape(shape, copy) {\n        let shSize = 1;\n        let negIndex = -1;\n        for (let i = 0; i < shape.length; i++) {\n            if (shape[i] === -1) {\n                negIndex = i;\n            }\n            else {\n                shSize *= shape[i];\n            }\n        }\n        if (copy === undefined) {\n            copy = true;\n        }\n        if (negIndex !== -1) {\n            const currShape = this.getShape();\n            const currSize = getSize(currShape);\n            const _shape = [...shape];\n            _shape[negIndex] = currSize / shSize;\n            return this.reshape_impl(_shape, copy);\n        }\n        return this.reshape_impl(shape, copy);\n    }\n    alignShapes(shape1, shape2) {\n        if (compareShapes(shape1, shape2)) {\n            return [shape1, shape2, shape1];\n        }\n        if (shape1.length < shape2.length) {\n            shape1 = [...shape1];\n            const prepend = shape2.length - shape1.length;\n            shape1.unshift(...new Array(prepend).fill(1));\n        }\n        else if (shape2.length < shape1.length) {\n            shape2 = [...shape2];\n            const prepend = shape1.length - shape2.length;\n            shape2.unshift(...new Array(prepend).fill(1));\n        }\n        const resultShape = new Array(shape1.length).fill(1);\n        for (let i = 0; i < shape1.length; i++) {\n            resultShape[i] = Math.max(shape1[i], shape2[i]);\n        }\n        return [shape1, shape2, resultShape];\n    }\n    /**\n     * Align the shapes of this tensor and the given tensor according to\n     * the broadcasting rules:\n     * https://github.com/onnx/onnx/blob/master/docs/Broadcasting.md\n     *\n     * @param tensor Tensor of which the shapes should be aligned\n     */\n    alignTensor(tensor) {\n        let thisShape = this.getShape();\n        let thatShape = tensor.getShape();\n        if (compareShapes(thisShape, thatShape)) {\n            return [this, tensor, thisShape];\n        }\n        // eslint-disable-next-line @typescript-eslint/no-this-alias\n        let th = this;\n        if (thisShape.length < thatShape.length) {\n            thisShape = [...thisShape];\n            const prepend = thatShape.length - thisShape.length;\n            thisShape.unshift(...new Array(prepend).fill(1));\n            th = this.reshape(thisShape, false);\n        }\n        else if (thatShape.length < thisShape.length) {\n            thatShape = [...thatShape];\n            const prepend = thisShape.length - thatShape.length;\n            thatShape.unshift(...new Array(prepend).fill(1));\n            tensor = tensor.reshape(thatShape, false);\n        }\n        const resultShape = new Array(thisShape.length).fill(1);\n        for (let i = 0; i < thisShape.length; i++) {\n            resultShape[i] = Math.max(thisShape[i], thatShape[i]);\n        }\n        return [th, tensor, resultShape];\n    }\n    /**\n     * Adds two tensors. Supports broadcasting\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2],[1,2,3,4]);\n     * const b = new CPUTensor([2,2],[5,6,7,8]);\n     * const c = new CPUTensor([1],[2]);\n     *\n     * a.add(b);\n     * //Will be\n     * // [[6,8],\n     * //  [10,12]]\n     *\n     * a.add(c);\n     * //Will be\n     * // [[3,4],\n     * //  [5,6]]\n     * ```\n     */\n    add(tensor, alpha, beta) {\n        if (alpha === undefined) {\n            alpha = 1;\n        }\n        if (beta === undefined) {\n            beta = 1;\n        }\n        const [th, tens, resultShape] = this.alignTensor(tensor);\n        return this.add_impl(th, tens, resultShape, alpha, beta);\n    }\n    /**\n     * Subtracts two tensors. Supports broadcasting\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2],[5,6,7,8]);\n     * const b = new CPUTensor([2,2],[1,2,3,4]);\n     * const c = new CPUTensor([1],[2]);\n     *\n     * a.subtract(b);\n     * //Will be\n     * // [[4,4],\n     * //  [4,4]]\n     *\n     * a.subtract(c);\n     * //Will be\n     * // [[3,4],\n     * //  [5,6]]\n     * ```\n     */\n    subtract(tensor, alpha, beta) {\n        if (alpha === undefined) {\n            alpha = 1;\n        }\n        if (beta === undefined) {\n            beta = 1;\n        }\n        const [th, tens, resultShape] = this.alignTensor(tensor);\n        return this.subtract_impl(th, tens, resultShape, alpha, beta);\n    }\n    /**\n     * Multiplies two tensors. Supports broadcasting\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2],[1,2,3,4]);\n     * const b = new CPUTensor([2,2],[5,6,7,8]);\n     * const c = new CPUTensor([1],[2]);\n     *\n     * a.multiply(b);\n     * //Will be\n     * // [[5,12],\n     * //  [21,32]]\n     *\n     * a.multiply(c);\n     * //Will be\n     * // [[2,4]\n     *     [6,8]]\n     * ```\n     */\n    multiply(tensor, alpha) {\n        if (alpha === undefined) {\n            alpha = 1;\n        }\n        const [th, tens, resultShape] = this.alignTensor(tensor);\n        return this.multiply_impl(th, tens, resultShape, alpha);\n    }\n    multiplyScalar(value) {\n        return this.addMultiplyScalar(value, 0);\n    }\n    addScalar(value) {\n        return this.addMultiplyScalar(1, value);\n    }\n    /**\n     * Divides two tensors. Supports broadcasting\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2],[5,6,7,8]);\n     * const b = new CPUTensor([2,2],[1,2,3,4]);\n     * const c = new CPUTensor([1],[2]);\n     *\n     * a.divide(b);\n     * //Will be\n     * // [[5,3],\n     * //  [2.333,2]]\n     *\n     * a.divide(c);\n     * //Will be\n     * // [[2.5,3],\n     * //  [3.5,4]]\n     * ```\n     */\n    divide(tensor, alpha) {\n        if (alpha === undefined) {\n            alpha = 1;\n        }\n        const [th, tens, resultShape] = this.alignTensor(tensor);\n        return this.divide_impl(th, tens, resultShape, alpha);\n    }\n    /**\n     * Takes the positionwise power. Supports broadcasting\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2],[5,6,7,8]);\n     * const b = new CPUTensor([2,2],[2,3,2,3]);\n     * const c = new CPUTensor([1],[2]);\n     *\n     * a.power(b);\n     * //Will be\n     * // [[25,216],\n     * //  [49,512]]\n     *\n     * a.power(c);\n     * //Will be\n     * // [[25,36],\n     * //  [49,64]]\n     * ```\n     */\n    power(tensor) {\n        const [th, tens, resultShape] = this.alignTensor(tensor);\n        return this.power_impl(th, tens, resultShape);\n    }\n    /**\n     * Transposes the tensor according to the given permutation\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2],[5,6,7,8]);\n     *\n     * a.transpose();\n     * //Will be\n     * // [[5,7],\n     * //  [6,8]]\n     * ```\n     * @param permutation Permutation for the axes. Default is the reverse axis order\n     */\n    transpose(permutation) {\n        if (permutation === undefined) {\n            const shape = this.getShape();\n            const rank = shape.length;\n            permutation = [];\n            for (let i = 0; i < rank; i++) {\n                permutation.push(rank - i - 1);\n            }\n        }\n        return this.transpose_impl(permutation);\n    }\n    /**\n     * Takes the softmax along the given axis\n     * https://en.wikipedia.org/wiki/Softmax_function\n     *\n     * Note that this can only be called on tensors with a float data type (float64, float32, float16)\n     */\n    softmax(axis) {\n        const max = this.max(axis, true);\n        const normalized = this.subtract(max);\n        const exp = normalized.exp();\n        const sum = exp.sum(axis, true);\n        const result = exp.divide(sum);\n        max.delete();\n        normalized.delete();\n        exp.delete();\n        sum.delete();\n        return result;\n    }\n    /**\n     * Calculates the general matrix product.\n     * https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3\n     *\n     * A and B can have batch dimensions. Their last two dimensions should\n     * correspond to the dimensions for the matrix product\n     *\n     * @param b Second matrix for the matrix product\n     * @param aTranspose If the last two dimensions of a are transposed. Defaults to false\n     * @param bTranspose If the last two dimensions of a are transposed. Defaults to false\n     * @param alpha Alpha parameter. Defaults to 1.0\n     * @param c Optional tensor to add to the result.\n     * @param beta Beta parameter, only used if c is specified. Defaults to 1.0\n     */\n    gemm(b, aTranspose, bTranspose, alpha, c, beta) {\n        aTranspose = aTranspose || false;\n        bTranspose = bTranspose || false;\n        alpha = alpha !== undefined ? alpha : 1;\n        beta = beta !== undefined ? beta : 1;\n        if (c !== undefined) {\n            const aShape = this.getShape();\n            let cShape = c.getShape();\n            const aRank = aShape.length;\n            const cRank = cShape.length;\n            if (aRank > cRank) {\n                cShape = [...new Array(aRank - cRank).fill(1), ...cShape];\n                c = c.reshape(cShape, false);\n            }\n        }\n        return this.gemm_impl(b, aTranspose, bTranspose, alpha, beta, c);\n    }\n    /**\n     * Takes a slice of the tensor along the specified axes.\n     *\n     * @example\n     * ```typescript\n     * const a = new CPUTensor([2,2],[5,6,7,8]);\n     *\n     * a.slice([0],[1],[0]);\n     * //Will be\n     * // [[5,6]]\n     *\n     * a.slice([0],[1],[1]);\n     * //Will be\n     * // [[5],\n     *     [6]]\n     * ```\n     *\n     * @param starts Start of the slice for each axis\n     * @param ends End of the slice for each axis - Exclusive (the end index will not be included in the slice)\n     * @param axes Axes to slice. Defaults to all axes\n     */\n    slice(starts, ends, axes, steps) {\n        const shape = this.getShape();\n        const rank = shape.length;\n        if (axes === undefined) {\n            axes = [];\n            for (let i = 0; i < rank; i++) {\n                axes.push(i);\n            }\n        }\n        else {\n            axes = axes.map(x => (x < 0 ? x + rank : x));\n        }\n        if (steps === undefined) {\n            steps = new Array(rank).fill(1);\n        }\n        starts = [...starts];\n        ends = [...ends];\n        for (let i = 0; i < axes.length; i++) {\n            const sh = shape[axes[i]];\n            if (starts[i] < 0) {\n                starts[i] += sh;\n            }\n            else if (starts[i] >= sh) {\n                if (steps[i] > 0) {\n                    starts[i] = sh;\n                }\n                else {\n                    starts[i] = sh - 1;\n                }\n            }\n            if (ends[i] < 0) {\n                ends[i] += sh;\n            }\n            else if (ends[i] >= sh) {\n                ends[i] = sh;\n            }\n        }\n        return this.slice_impl(starts, ends, axes, steps);\n    }\n    squeeze() {\n        const sh = this.getShape();\n        const newShape = [];\n        for (const a of sh) {\n            if (a !== 1) {\n                newShape.push(a);\n            }\n        }\n        return this.reshape(newShape);\n    }\n    flatten(axis) {\n        if (axis === undefined) {\n            axis = 1;\n        }\n        const sh = this.getShape();\n        if (axis < 0) {\n            axis += sh.length;\n        }\n        const newShape = [\n            getSize(sh.slice(0, axis), 1),\n            getSize(sh.slice(axis), 1),\n        ];\n        return this.reshape(newShape);\n    }\n}\n//# sourceMappingURL=types.js.map","export function outputDimSize(inSize, kernel, headPad, tailPad, dilation, stride) {\n    const dkernel = dilation * (kernel - 1) + 1;\n    return Math.floor((inSize + headPad + tailPad - dkernel) / stride + 1);\n}\nexport function outputDimsSize(inSizes, kernels, headPads, tailPads, dilations, strides) {\n    const result = [];\n    for (let i = 0; i < inSizes.length; i++) {\n        result.push(outputDimSize(inSizes[i], kernels[i], headPads[i], tailPads[i], dilations[i], strides[i]));\n    }\n    return result;\n}\n//# sourceMappingURL=conv.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { checkEquivShapes, incrementIndex } from '../../util/shape';\nexport function positionWiseUnaryOp(a, op) {\n    const result = new CPUTensor(a.shape, undefined, a.dtype);\n    for (let i = 0; i < result.size; i += 1) {\n        result.set(i, op(a.get(i)));\n    }\n    return result;\n}\nexport function positionWiseBinaryOp(a, b, op, resultShape) {\n    if (!checkEquivShapes(a.shape, b.shape)) {\n        throw new Error('The shapes of the two tensors should be the same for a binary operation');\n    }\n    const result = new CPUTensor(resultShape, undefined, a.dtype);\n    const index = new Array(resultShape.length).fill(0);\n    for (let i = 0; i < result.size; i += 1) {\n        result.set(index, op(a.get(index), b.get(index)));\n        incrementIndex(index, resultShape);\n    }\n    return result;\n}\nexport function exp(a) {\n    return positionWiseUnaryOp(a, o1 => Math.exp(o1));\n}\nexport function log(a) {\n    return positionWiseUnaryOp(a, o1 => Math.log(o1));\n}\nexport function sqrt(a) {\n    return positionWiseUnaryOp(a, o1 => Math.sqrt(o1));\n}\nexport function abs(a) {\n    return positionWiseUnaryOp(a, o1 => Math.abs(o1));\n}\nexport function sin(a) {\n    return positionWiseUnaryOp(a, o1 => Math.sin(o1));\n}\nexport function cos(a) {\n    return positionWiseUnaryOp(a, o1 => Math.cos(o1));\n}\nexport function tan(a) {\n    return positionWiseUnaryOp(a, o1 => Math.tan(o1));\n}\nexport function asin(a) {\n    return positionWiseUnaryOp(a, o1 => Math.asin(o1));\n}\nexport function acos(a) {\n    return positionWiseUnaryOp(a, o1 => Math.acos(o1));\n}\nexport function atan(a) {\n    return positionWiseUnaryOp(a, o1 => Math.atan(o1));\n}\nexport function sinh(a) {\n    return positionWiseUnaryOp(a, o1 => Math.sinh(o1));\n}\nexport function cosh(a) {\n    return positionWiseUnaryOp(a, o1 => Math.cosh(o1));\n}\nexport function tanh(a) {\n    return positionWiseUnaryOp(a, o1 => Math.tanh(o1));\n}\nexport function asinh(a) {\n    return positionWiseUnaryOp(a, o1 => Math.asinh(o1));\n}\nexport function acosh(a) {\n    return positionWiseUnaryOp(a, o1 => Math.acosh(o1));\n}\nexport function atanh(a) {\n    return positionWiseUnaryOp(a, o1 => Math.atanh(o1));\n}\nexport function floor(a) {\n    return positionWiseUnaryOp(a, o1 => Math.floor(o1));\n}\nexport function ceil(a) {\n    return positionWiseUnaryOp(a, o1 => Math.ceil(o1));\n}\nexport function round(a) {\n    return positionWiseUnaryOp(a, o1 => Math.round(o1));\n}\nexport function sign(a) {\n    return positionWiseUnaryOp(a, o1 => (o1 < 0 ? -1 : o1 === 0 ? 0 : 1));\n}\nexport function negate(a) {\n    return positionWiseUnaryOp(a, o1 => -o1);\n}\nexport function powerScalar(a, power, factor) {\n    return positionWiseUnaryOp(a, o1 => Math.pow(o1, power) * factor);\n}\nexport function addMultiplyScalar(a, factor, add) {\n    return positionWiseUnaryOp(a, o1 => o1 * factor + add);\n}\nexport function sigmoid(a) {\n    return positionWiseUnaryOp(a, o1 => 1 / (1 + Math.exp(-o1)));\n}\nexport function hardSigmoid(a, alpha, beta) {\n    return positionWiseUnaryOp(a, o1 => Math.max(0, Math.min(1, alpha * o1 + beta)));\n}\nexport function clip(a, min, max) {\n    let f = (o1) => o1;\n    if (min !== undefined && max !== undefined) {\n        f = (o1) => Math.min(max, Math.max(min, o1));\n    }\n    else if (max !== undefined) {\n        f = (o1) => Math.min(max, o1);\n    }\n    else if (min !== undefined) {\n        f = (o1) => Math.max(min, o1);\n    }\n    return positionWiseUnaryOp(a, f);\n}\nexport function add(a, b, resultShape, alpha, beta) {\n    return positionWiseBinaryOp(a, b, (o1, o2) => o1 * alpha + o2 * beta, resultShape);\n}\nexport function subtract(a, b, resultShape, alpha, beta) {\n    return positionWiseBinaryOp(a, b, (o1, o2) => o1 * alpha - o2 * beta, resultShape);\n}\nexport function multiply(a, b, resultShape, alpha) {\n    return positionWiseBinaryOp(a, b, (o1, o2) => o1 * o2 * alpha, resultShape);\n}\nexport function divide(a, b, resultShape, alpha) {\n    return positionWiseBinaryOp(a, b, (o1, o2) => (o1 / o2) * alpha, resultShape);\n}\nexport function power(a, b, resultShape) {\n    return positionWiseBinaryOp(a, b, (o1, o2) => Math.pow(o1, o2), resultShape);\n}\nexport function clipBackward(value, grad, resultShape, min, max) {\n    return positionWiseBinaryOp(value, grad, (v, g) => {\n        if (min !== undefined && v < min) {\n            return 0;\n        }\n        if (max !== undefined && v > max) {\n            return 0;\n        }\n        return g;\n    }, resultShape);\n}\n//# sourceMappingURL=basic.js.map","export function outputDimSize(inSize, kernel, headPad, tailPad, dilation, stride) {\n    const kernelSize = dilation * (kernel - 1) + 1;\n    return stride * (inSize - 1) + headPad + tailPad - kernelSize + 2;\n}\nexport function outputDimsSize(inSizes, kernels, headPads, tailPads, dilations, strides) {\n    const result = [];\n    for (let i = 0; i < inSizes.length; i++) {\n        result.push(outputDimSize(inSizes[i], kernels[i], headPads[i], tailPads[i], dilations[i], strides[i]));\n    }\n    return result;\n}\n//# sourceMappingURL=convTranspose.js.map","export function poolResultShape(inputShape, axes, keepDims) {\n    const resultShape = [];\n    const sumShape = [];\n    const ixMap = [];\n    for (let i = 0; i < inputShape.length; i++) {\n        if (!axes.includes(i)) {\n            resultShape.push(inputShape[i]);\n            ixMap.push(i);\n        }\n        else {\n            if (keepDims) {\n                resultShape.push(1);\n                ixMap.push(i);\n            }\n            sumShape.push(inputShape[i]);\n        }\n    }\n    if (resultShape.length === 0) {\n        resultShape.push(1);\n    }\n    return [resultShape, ixMap];\n}\n//# sourceMappingURL=pool.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { getSize, incrementIndex, indexToPos, computeStrides, } from '../../util/shape';\nimport { poolResultShape } from '../util/pool';\nexport function pool(a, axes, operation, keepDims, postProcess) {\n    const inputShape = a.getShape();\n    const inputSize = getSize(inputShape);\n    const [resultShape, ixMap] = poolResultShape(inputShape, axes, keepDims);\n    const resultSize = getSize(resultShape);\n    const resultStrides = computeStrides(resultShape);\n    const result = new CPUTensor(resultShape, undefined, a.dtype);\n    const initialized = new Array(resultSize).fill(false);\n    const index = new Array(inputShape.length).fill(0);\n    const outIndex = new Array(resultShape.length).fill(0);\n    for (let i = 0; i < inputSize; i++) {\n        for (let j = 0; j < ixMap.length; j++) {\n            outIndex[j] = index[ixMap[j]];\n        }\n        const outOffset = indexToPos(outIndex, resultStrides);\n        if (initialized[outOffset]) {\n            result.set(outIndex, operation(a.get(i), result.get(outIndex)));\n        }\n        else {\n            initialized[outOffset] = true;\n            result.set(outIndex, operation(a.get(i)));\n        }\n        incrementIndex(index, inputShape);\n    }\n    if (postProcess) {\n        for (let i = 0; i < result.size; i++) {\n            result.set(i, postProcess(result.get(i)));\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=pool.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function pad(x, pads, mode, value) {\n    const rank = x.shape.length;\n    const resultShape = [...x.shape];\n    for (let i = 0; i < rank; i++) {\n        resultShape[i] += pads[i] + pads[i + rank];\n    }\n    const Y = new CPUTensor(resultShape, undefined, x.dtype);\n    const ix = new Array(rank).fill(0);\n    const inputIx = new Array(rank).fill(0);\n    for (let i = 0; i < Y.size; i++) {\n        let allInRange = true;\n        for (let j = 0; j < rank; j++) {\n            inputIx[j] = ix[j] - pads[j];\n            if (inputIx[j] < 0 || inputIx[j] >= x.shape[j]) {\n                allInRange = false;\n            }\n        }\n        Y.set(i, getPadValue(x, inputIx, mode, value, allInRange));\n        incrementIndex(ix, resultShape);\n    }\n    return Y;\n}\nfunction getPadValue(x, index, mode, value, allInRange) {\n    if (allInRange) {\n        return x.get(index);\n    }\n    const rank = x.shape.length;\n    if (mode === 'constant') {\n        return value;\n    }\n    else if (mode === 'edge') {\n        for (let j = 0; j < rank; j++) {\n            if (index[j] < 0) {\n                index[j] = 0;\n            }\n            else if (index[j] >= x.shape[j]) {\n                index[j] = x.shape[j] - 1;\n            }\n        }\n    }\n    else {\n        for (let j = 0; j < rank; j++) {\n            if (index[j] < 0) {\n                index[j] = -index[j];\n            }\n            else if (index[j] >= x.shape[j]) {\n                index[j] = 2 * x.shape[j] - index[j] - 2;\n            }\n        }\n    }\n    return x.get(index);\n}\n//# sourceMappingURL=pad.js.map","import { averagePool } from '../../ops/cpu/averagePool';\nimport { abs, acos, acosh, add, addMultiplyScalar, asin, asinh, atan, atanh, ceil, clip, clipBackward, cos, cosh, divide, exp, floor, hardSigmoid, log, multiply, negate, power, powerScalar, round, sigmoid, sign, sin, sinh, sqrt, subtract, tan, tanh, } from '../../ops/cpu/basic';\nimport { concat } from '../../ops/cpu/concat';\nimport { conv } from '../../ops/cpu/conv';\nimport { convTranspose } from '../../ops/cpu/convTranspose';\nimport { expand } from '../../ops/cpu/expand';\nimport { gather } from '../../ops/cpu/gather';\nimport { gemm } from '../../ops/cpu/gemm';\nimport { matMul } from '../../ops/cpu/matMul';\nimport { max } from '../../ops/cpu/max';\nimport { min } from '../../ops/cpu/min';\nimport { normalize } from '../../ops/cpu/normalize';\nimport { pad } from '../../ops/cpu/pad';\nimport { product } from '../../ops/cpu/product';\nimport { reduceLogSum } from '../../ops/cpu/reduceLogSum';\nimport { reduceLogSumExp } from '../../ops/cpu/reduceLogSumExp';\nimport { reduceMean } from '../../ops/cpu/reduceMean';\nimport { reduceMeanSquare } from '../../ops/cpu/reduceMeanSquare';\nimport { repeat } from '../../ops/cpu/repeat';\nimport { setValues } from '../../ops/cpu/setValues';\nimport { slice } from '../../ops/cpu/slice';\nimport { sum } from '../../ops/cpu/sum';\nimport { sumSquare } from '../../ops/cpu/sumSquare';\nimport { transpose } from '../../ops/cpu/transpose';\nimport { upsample } from '../../ops/cpu/upsample';\nimport Tensor, { tensorValuesConstructor, } from '../../types';\nimport { compareShapes, computeStrides, getSize, indexToPos, } from '../../util/shape';\nexport class CPUTensor extends Tensor {\n    constructor(shape, values, dtype) {\n        super(dtype || 'float32');\n        /**\n         * If this tensor was already deleted\n         */\n        this.deleted = false;\n        this.shape = shape;\n        this.strides = computeStrides(shape);\n        this.size = getSize(shape);\n        if (values !== undefined) {\n            if (values instanceof Array) {\n                this.values = new tensorValuesConstructor[this.dtype](values);\n            }\n            else {\n                this.values = values;\n            }\n        }\n        else {\n            this.values = new tensorValuesConstructor[this.dtype](this.size);\n        }\n    }\n    static range(start, limit, delta) {\n        const size = Math.max(Math.ceil((limit - start) / delta), 0);\n        const values = new Float32Array(size);\n        for (let i = 0; i < size; i++) {\n            values[i] = start + i * delta;\n        }\n        return new CPUTensor([size], values);\n    }\n    getValues() {\n        return Promise.resolve(this.values);\n    }\n    getShape() {\n        return this.shape;\n    }\n    constantLike(value) {\n        return new CPUTensor(this.shape, new tensorValuesConstructor[this.dtype](this.size).fill(value), this.dtype);\n    }\n    singleConstant(value) {\n        return new CPUTensor([1], [value], this.dtype);\n    }\n    cast(dtype) {\n        // TODO: Catch special cases here\n        // Eg casting to the same type\n        return new CPUTensor(this.shape, Array.from(this.values), dtype);\n    }\n    delete() {\n        //@ts-ignore\n        this.values = undefined;\n        this.deleted = true;\n    }\n    copy(newShape) {\n        if (newShape === undefined) {\n            newShape = [...this.shape];\n        }\n        const values = new tensorValuesConstructor[this.dtype](this.size);\n        for (let i = 0; i < this.size; i++) {\n            values[i] = this.values[i];\n        }\n        return new CPUTensor(newShape, values, this.dtype);\n    }\n    get(index) {\n        let pos;\n        if (Array.isArray(index)) {\n            pos = indexToPos(index, this.strides, this.shape);\n        }\n        else {\n            pos = index;\n        }\n        return this.values[pos];\n    }\n    set(index, value) {\n        let pos;\n        if (Array.isArray(index)) {\n            pos = indexToPos(index, this.strides);\n        }\n        else {\n            pos = index;\n        }\n        this.values[pos] = value;\n    }\n    setValues(values, starts) {\n        if (!(values instanceof CPUTensor)) {\n            throw new Error('Can only set CPU values to CPU values');\n        }\n        return setValues(this, values, starts);\n    }\n    exp() {\n        return exp(this);\n    }\n    log() {\n        return log(this);\n    }\n    sqrt() {\n        return sqrt(this);\n    }\n    abs() {\n        return abs(this);\n    }\n    sin() {\n        return sin(this);\n    }\n    cos() {\n        return cos(this);\n    }\n    tan() {\n        return tan(this);\n    }\n    asin() {\n        return asin(this);\n    }\n    acos() {\n        return acos(this);\n    }\n    atan() {\n        return atan(this);\n    }\n    sinh() {\n        return sinh(this);\n    }\n    cosh() {\n        return cosh(this);\n    }\n    tanh() {\n        return tanh(this);\n    }\n    asinh() {\n        return asinh(this);\n    }\n    acosh() {\n        return acosh(this);\n    }\n    atanh() {\n        return atanh(this);\n    }\n    floor() {\n        return floor(this);\n    }\n    ceil() {\n        return ceil(this);\n    }\n    round() {\n        return round(this);\n    }\n    negate() {\n        return negate(this);\n    }\n    powerScalar(power, factor) {\n        return powerScalar(this, power, factor);\n    }\n    multiplyScalar(value) {\n        return addMultiplyScalar(this, value, 0);\n    }\n    addScalar(value) {\n        return addMultiplyScalar(this, 1, value);\n    }\n    addMultiplyScalar(factor, add) {\n        return addMultiplyScalar(this, factor, add);\n    }\n    sign() {\n        return sign(this);\n    }\n    clip(min, max) {\n        return clip(this, min, max);\n    }\n    clipBackward(grad, min, max) {\n        if (!(grad instanceof CPUTensor)) {\n            throw new Error('Can only do clipBackward with CPUTensor');\n        }\n        return clipBackward(this, grad, this.getShape(), min, max);\n    }\n    sigmoid() {\n        return sigmoid(this);\n    }\n    hardSigmoid(alpha, beta) {\n        return hardSigmoid(this, alpha, beta);\n    }\n    add_impl(th, tensor, resultShape, alpha, beta) {\n        if (!(tensor instanceof CPUTensor) || !(th instanceof CPUTensor)) {\n            throw new Error('Can only add CPU tensor to CPU tensor');\n        }\n        return add(th, tensor, resultShape, alpha, beta);\n    }\n    subtract_impl(th, tensor, resultShape, alpha, beta) {\n        if (!(tensor instanceof CPUTensor) || !(th instanceof CPUTensor)) {\n            throw new Error('Can only subtract CPU tensor to CPU tensor');\n        }\n        return subtract(th, tensor, resultShape, alpha, beta);\n    }\n    multiply_impl(th, tensor, resultShape, alpha) {\n        if (!(tensor instanceof CPUTensor) || !(th instanceof CPUTensor)) {\n            throw new Error('Can only add CPU tensor to CPU tensor');\n        }\n        return multiply(th, tensor, resultShape, alpha);\n    }\n    divide_impl(th, tensor, resultShape, alpha) {\n        if (!(tensor instanceof CPUTensor) || !(th instanceof CPUTensor)) {\n            throw new Error('Can only add CPU tensor to CPU tensor');\n        }\n        return divide(th, tensor, resultShape, alpha);\n    }\n    power_impl(th, tensor, resultShape) {\n        if (!(tensor instanceof CPUTensor) || !(th instanceof CPUTensor)) {\n            throw new Error('Can only take CPU tensor to power of CPU tensor');\n        }\n        return power(th, tensor, resultShape);\n    }\n    matMul(tensor) {\n        if (!(tensor instanceof CPUTensor)) {\n            throw new Error('Can only add CPU tensor to CPU tensor');\n        }\n        return matMul(this, tensor);\n    }\n    gemm_impl(b, aTranspose, bTranspose, alpha, beta, c) {\n        if (!(b instanceof CPUTensor && (c === undefined || c instanceof CPUTensor))) {\n            throw new Error('Can only do gemm with CPU tensors');\n        }\n        return gemm(this, b, aTranspose, bTranspose, alpha, beta, c);\n    }\n    sum_impl(axes, keepDims) {\n        return sum(this, axes, keepDims);\n    }\n    sumSquare_impl(axes, keepDims) {\n        return sumSquare(this, axes, keepDims);\n    }\n    product_impl(axes, keepDims) {\n        return product(this, axes, keepDims);\n    }\n    max_impl(axes, keepDims) {\n        return max(this, axes, keepDims);\n    }\n    min_impl(axes, keepDims) {\n        return min(this, axes, keepDims);\n    }\n    reduceMean_impl(axes, keepDims) {\n        return reduceMean(this, axes, keepDims);\n    }\n    reduceMeanSquare_impl(axes, keepDims) {\n        return reduceMeanSquare(this, axes, keepDims);\n    }\n    reduceLogSum_impl(axes, keepDims) {\n        return reduceLogSum(this, axes, keepDims);\n    }\n    reduceLogSumExp_impl(axes, keepDims) {\n        return reduceLogSumExp(this, axes, keepDims);\n    }\n    conv_impl(kernel, dilations, group, pads, strides, activation, bias) {\n        if (!(kernel instanceof CPUTensor) ||\n            (bias !== undefined && !(bias instanceof CPUTensor))) {\n            throw new Error('Can only do convolution of CPU tensor with CPU tensor');\n        }\n        return conv(this, kernel, dilations, group, pads, strides, activation, bias);\n    }\n    convTranspose_impl(kernel, dilations, group, pads, strides) {\n        if (!(kernel instanceof CPUTensor)) {\n            throw new Error('Can only do transpose convolution of CPU tensor with CPU tensor');\n        }\n        return convTranspose(this, kernel, dilations, group, pads, strides);\n    }\n    pad_impl(pads, mode, value) {\n        return pad(this, pads, mode, value);\n    }\n    averagePool_impl(kernelShape, pads, strides, includePad) {\n        return averagePool(this, kernelShape, pads, strides, includePad);\n    }\n    reshape_impl(shape, copy) {\n        if (copy) {\n            return this.copy(shape);\n        }\n        else {\n            return new CPUTensor(shape, this.values, this.dtype);\n        }\n    }\n    concat(tensor, axis) {\n        if (!(tensor instanceof CPUTensor)) {\n            throw new Error('Can only concat CPU tensor to CPU tensor');\n        }\n        if (axis < 0) {\n            axis += this.shape.length;\n        }\n        return concat(this, tensor, axis);\n    }\n    transpose_impl(permutation) {\n        return transpose(this, permutation);\n    }\n    repeat(repeats) {\n        return repeat(this, repeats);\n    }\n    expand(shape) {\n        // eslint-disable-next-line @typescript-eslint/no-unused-vars\n        const [_shape, goal, resultShape] = this.alignShapes(this.shape, shape);\n        if (compareShapes(this.shape, resultShape)) {\n            return this.copy();\n        }\n        return expand(this.reshape(_shape, false), resultShape);\n    }\n    gather(axis, indices) {\n        return gather(this, axis, indices);\n    }\n    slice_impl(starts, ends, axes, steps) {\n        return slice(this, starts, ends, axes, steps);\n    }\n    upsample(scales) {\n        return upsample(this, scales);\n    }\n    normalize(mean, variance, epsilon, scale, bias) {\n        if (!(mean instanceof CPUTensor) ||\n            !(variance instanceof CPUTensor) ||\n            !(scale instanceof CPUTensor) ||\n            !(bias instanceof CPUTensor)) {\n            throw new Error('Can only normalize with CPU tensors');\n        }\n        return normalize(this, mean, variance, epsilon, scale, bias);\n    }\n}\n//# sourceMappingURL=tensor.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function setValues(a, b, starts) {\n    const result = new CPUTensor(a.shape, undefined, a.dtype);\n    const index = new Array(a.shape.length).fill(0);\n    for (let i = 0; i < result.size; i += 1) {\n        let inB = true;\n        const bIx = new Array(starts.length).fill(0);\n        for (let j = 0; j < starts.length; j++) {\n            if (index[j] < starts[j] || index[j] >= starts[j] + b.shape[j]) {\n                inB = false;\n                break;\n            }\n            else {\n                bIx[j] = index[j] - starts[j];\n            }\n        }\n        if (inB) {\n            result.set(i, b.get(bIx));\n        }\n        else {\n            result.set(i, a.get(i));\n        }\n        incrementIndex(index, a.shape);\n    }\n    return result;\n}\n//# sourceMappingURL=setValues.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nexport function matMul(a, b) {\n    if (a.shape.length !== 2 || b.shape.length !== 2) {\n        throw new Error('Matmul expects both operands to have rank 2');\n    }\n    if (a.shape[1] !== b.shape[0]) {\n        throw new Error('Matmul expects dimension 1 of operand 1 to equal dimension 0 of operand 2');\n    }\n    const m = a.shape[0];\n    const n = a.shape[1];\n    const o = b.shape[1];\n    const result = new CPUTensor([m, o], undefined, a.dtype);\n    for (let i = 0; i < m; i += 1) {\n        for (let k = 0; k < o; k += 1) {\n            let res = 0;\n            for (let j = 0; j < n; j += 1) {\n                res += a.get([i, j]) * b.get([j, k]);\n            }\n            result.set([i, k], res);\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=matMul.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { getSize } from '../../util/shape';\nexport function gemm(a, b, aTranspose, bTranspose, alpha, beta, c) {\n    const rank = a.shape.length;\n    const M = aTranspose ? a.shape[rank - 1] : a.shape[rank - 2];\n    const N = aTranspose ? a.shape[rank - 2] : a.shape[rank - 1];\n    const O = bTranspose ? b.shape[rank - 2] : b.shape[rank - 1];\n    const aBatchMult = M * N;\n    const bBatchMult = N * O;\n    const yBatchMult = M * O;\n    const aNMult = aTranspose ? M : 1;\n    const aMMult = aTranspose ? 1 : N;\n    const bNMult = bTranspose ? 1 : O;\n    const bOMult = bTranspose ? N : 1;\n    let cMMult = 0;\n    let cOMult = 0;\n    let cBatchMult = 0;\n    if (c !== undefined) {\n        cMMult = c.strides[rank - 2];\n        cOMult = c.strides[rank - 1];\n        const cBatchSize = getSize(c.shape.slice(0, rank - 2));\n        if (cBatchSize > 1) {\n            cBatchMult = c.shape[rank - 2] * c.shape[rank - 1];\n            if (cBatchMult === 1) {\n                cBatchMult = 0;\n            }\n        }\n        else {\n            cBatchMult = 0;\n        }\n    }\n    const batchShape = a.shape.slice(0, rank - 2);\n    let batchSize = getSize(batchShape);\n    if (batchSize === 0) {\n        batchSize = 1;\n    }\n    const resultShape = [...batchShape, M, O];\n    const Y = new CPUTensor(resultShape, undefined, a.dtype);\n    for (let i = 0; i < batchSize; i++) {\n        const aBase = i * aBatchMult;\n        const bBase = i * bBatchMult;\n        const yBase = i * yBatchMult;\n        const cBase = i * cBatchMult;\n        for (let m = 0; m < M; m++) {\n            for (let o = 0; o < O; o++) {\n                let result = 0;\n                for (let n = 0; n < N; n++) {\n                    result +=\n                        a.get(aBase + m * aMMult + n * aNMult) *\n                            b.get(bBase + n * bNMult + o * bOMult);\n                }\n                result = alpha * result;\n                if (c !== undefined) {\n                    result += beta * c.get(cBase + m * cMMult + o * cOMult);\n                }\n                Y.set(yBase + m * O + o, result);\n            }\n        }\n    }\n    return Y;\n}\n//# sourceMappingURL=gemm.js.map","import { pool } from './pool';\nexport function sum(a, axes, keepDims) {\n    return pool(a, axes, (a, b) => {\n        return a + (b !== undefined ? b : 0);\n    }, keepDims);\n}\n//# sourceMappingURL=sum.js.map","import { pool } from './pool';\nexport function sumSquare(a, axes, keepDims) {\n    return pool(a, axes, (a, b) => {\n        return a * a + (b !== undefined ? b : 0);\n    }, keepDims);\n}\n//# sourceMappingURL=sumSquare.js.map","import { pool } from './pool';\nexport function product(a, axes, keepDims) {\n    return pool(a, axes, (a, b) => {\n        return a * (b !== undefined ? b : 1);\n    }, keepDims);\n}\n//# sourceMappingURL=product.js.map","import { pool } from './pool';\nexport function max(a, axes, keepDims) {\n    return pool(a, axes, (a, b) => {\n        return Math.max(a, b !== undefined ? b : a);\n    }, keepDims);\n}\n//# sourceMappingURL=max.js.map","import { pool } from './pool';\nexport function min(a, axes, keepDims) {\n    return pool(a, axes, (a, b) => {\n        return Math.min(a, b !== undefined ? b : a);\n    }, keepDims);\n}\n//# sourceMappingURL=min.js.map","import { pool } from './pool';\nexport function reduceMean(a, axes, keepDims) {\n    let poolSize = 1;\n    for (let i = 0; i < axes.length; i++) {\n        poolSize *= a.shape[axes[i]];\n    }\n    return pool(a, axes, (a, b) => {\n        return a + (b !== undefined ? b : 0);\n    }, keepDims, (a) => a / poolSize);\n}\n//# sourceMappingURL=reduceMean.js.map","import { pool } from './pool';\nexport function reduceMeanSquare(a, axes, keepDims) {\n    let poolSize = 1;\n    for (let i = 0; i < axes.length; i++) {\n        poolSize *= a.shape[axes[i]];\n    }\n    return pool(a, axes, (a, b) => {\n        return a * a + (b !== undefined ? b : 0);\n    }, keepDims, (a) => a / poolSize);\n}\n//# sourceMappingURL=reduceMeanSquare.js.map","import { pool } from './pool';\nexport function reduceLogSum(a, axes, keepDims) {\n    return pool(a, axes, (a, b) => {\n        return a + (b !== undefined ? b : 0);\n    }, keepDims, (a) => Math.log(a));\n}\n//# sourceMappingURL=reduceLogSum.js.map","import { pool } from './pool';\nexport function reduceLogSumExp(a, axes, keepDims) {\n    return pool(a, axes, (a, b) => {\n        return Math.exp(a) + (b !== undefined ? b : 0);\n    }, keepDims, (a) => Math.log(a));\n}\n//# sourceMappingURL=reduceLogSumExp.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { getSize, incrementIndex } from '../../util/shape';\nimport { outputDimsSize } from '../util/conv';\nexport function conv(x, w, dilations, group, pads, strides, activation, bias) {\n    const N = x.shape[0];\n    const C = x.shape[1];\n    const D = x.shape.slice(2);\n    const W = w.shape.slice(2);\n    const M = w.shape[0];\n    const CG = C / group;\n    const kernelSize = getSize(W);\n    const R = outputDimsSize(D, W, pads.slice(0, pads.length / 2), pads.slice(pads.length / 2), dilations, strides);\n    const outputSize = getSize(R);\n    let outputShape = [N, M];\n    outputShape = outputShape.concat(R);\n    const Y = new CPUTensor(outputShape, undefined, x.dtype);\n    const dataRank = R.length;\n    // Iterate over all batches\n    for (let n = 0; n < N; n++) {\n        // Iterate over all output channels\n        for (let m = 0; m < M; m++) {\n            if (bias) {\n                const b = bias ? bias.get([m]) : 0;\n                const outputIndices = new Array(R.length).fill(0);\n                outputIndices.unshift(n, m);\n                for (let oIx = 0; oIx < outputSize; oIx++) {\n                    Y.set(outputIndices, b);\n                    incrementIndex(outputIndices, Y.shape);\n                }\n            }\n            for (let cg = 0; cg < CG; cg++) {\n                const c = (m * CG + cg) % C;\n                const outputIndices = new Array(R.length).fill(0);\n                outputIndices.unshift(n, m);\n                for (let oIx = 0; oIx < outputSize; oIx++) {\n                    let result = Y.get(outputIndices);\n                    const kernelIndices = new Array(R.length).fill(0);\n                    kernelIndices.unshift(m, cg);\n                    for (let kIx = 0; kIx < kernelSize; kIx++) {\n                        const inputIx = [n, c];\n                        let skip = false;\n                        for (let axis = 0; axis < dataRank; axis++) {\n                            const stride = strides.length === 0 ? 1 : strides[axis];\n                            const pad = pads.length === 0 ? 0 : pads[axis];\n                            const dilation = dilations.length === 0 ? 1 : dilations[axis];\n                            const ix = outputIndices[axis + 2] * stride -\n                                pad +\n                                kernelIndices[axis + 2] * dilation;\n                            if (ix < 0 || ix >= D[axis]) {\n                                skip = true;\n                                break;\n                            }\n                            inputIx.push(ix);\n                        }\n                        if (!skip) {\n                            const Wi = w.get(kernelIndices);\n                            const Xi = x.get(inputIx);\n                            result += Wi * Xi;\n                        }\n                        incrementIndex(kernelIndices, w.shape);\n                    }\n                    Y.set(outputIndices, result);\n                    incrementIndex(outputIndices, Y.shape);\n                }\n            }\n            if (activation !== 'id') {\n                const outputIndices = new Array(R.length).fill(0);\n                outputIndices.unshift(n, m);\n                for (let oIx = 0; oIx < outputSize; oIx++) {\n                    let result = Y.get(outputIndices);\n                    if (activation === 'relu') {\n                        result = Math.max(0, result);\n                    }\n                    else if (activation === 'relu6') {\n                        result = Math.min(Math.max(0, result), 6);\n                    }\n                    Y.set(outputIndices, result);\n                    incrementIndex(outputIndices, Y.shape);\n                }\n            }\n        }\n    }\n    return Y;\n}\n//# sourceMappingURL=conv.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { getSize, incrementIndex } from '../../util/shape';\nimport { outputDimsSize } from '../util/convTranspose';\nexport function convTranspose(x, w, dilations, group, pads, strides) {\n    const N = x.shape[0];\n    const C = x.shape[1];\n    const D = x.shape.slice(2);\n    const W = w.shape.slice(2);\n    const M = w.shape[0];\n    const CG = C / group;\n    const kernelSize = getSize(W);\n    const R = outputDimsSize(D, W, pads.slice(0, pads.length / 2), pads.slice(pads.length / 2), dilations, strides);\n    const outputSize = getSize(R);\n    let outputShape = [N, M];\n    outputShape = outputShape.concat(R);\n    const Y = new CPUTensor(outputShape, undefined, x.dtype);\n    const dataRank = R.length;\n    // Iterate over all batches\n    for (let n = 0; n < N; n++) {\n        // Iterate over all output channels\n        for (let m = 0; m < M; m++) {\n            for (let cg = 0; cg < CG; cg++) {\n                const c = (m * CG + cg) % C;\n                const outputIndices = new Array(R.length).fill(0);\n                outputIndices.unshift(n, m);\n                for (let oIx = 0; oIx < outputSize; oIx++) {\n                    let result = Y.get(outputIndices);\n                    const kernelIndices = new Array(R.length).fill(0);\n                    kernelIndices.unshift(m, cg);\n                    for (let kIx = 0; kIx < kernelSize; kIx++) {\n                        const inputIx = [n, c];\n                        let skip = false;\n                        for (let axis = 0; axis < dataRank; axis++) {\n                            const stride = strides.length === 0 ? 1 : strides[axis];\n                            const pad = pads.length === 0 ? 0 : pads[axis];\n                            const dilation = dilations.length === 0 ? 1 : dilations[axis];\n                            let ix = outputIndices[axis + 2] -\n                                pad +\n                                kernelIndices[axis + 2] * dilation;\n                            const res = ix % stride;\n                            if (res !== 0) {\n                                skip = true;\n                                break;\n                            }\n                            ix = ix / stride;\n                            if (ix < 0 || ix >= D[axis]) {\n                                skip = true;\n                                break;\n                            }\n                            inputIx.push(ix);\n                        }\n                        if (!skip) {\n                            const transposedKernelIx = [m, cg];\n                            for (let i = 0; i < dataRank; i++) {\n                                transposedKernelIx.push(W[i] - kernelIndices[i + 2] - 1);\n                            }\n                            const Wi = w.get(transposedKernelIx);\n                            const Xi = x.get(inputIx);\n                            result += Wi * Xi;\n                        }\n                        incrementIndex(kernelIndices, w.shape);\n                    }\n                    Y.set(outputIndices, result);\n                    incrementIndex(outputIndices, Y.shape);\n                }\n            }\n        }\n    }\n    return Y;\n}\n//# sourceMappingURL=convTranspose.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { getSize, incrementIndex } from '../../util/shape';\nimport { outputDimsSize } from '../util/conv';\nexport function averagePool(x, kernelShape, pads, strides, includePad) {\n    const N = x.shape[0];\n    const C = x.shape[1];\n    const D = x.shape.slice(2);\n    const dataRank = D.length;\n    const kernelSize = getSize(kernelShape);\n    const R = outputDimsSize(D, kernelShape, pads.slice(0, pads.length / 2), pads.slice(pads.length / 2), new Array(dataRank).fill(1), strides);\n    const outputSize = getSize(R);\n    let outputShape = [N, C];\n    outputShape = outputShape.concat(R);\n    const Y = new CPUTensor(outputShape, undefined, x.dtype);\n    // Iterate over all batches\n    for (let n = 0; n < N; n++) {\n        // Iterate over all output channels\n        for (let c = 0; c < C; c++) {\n            const outputIndices = new Array(R.length).fill(0);\n            outputIndices.unshift(n, c);\n            for (let oIx = 0; oIx < outputSize; oIx++) {\n                let result = 0;\n                const kernelIndices = new Array(R.length).fill(0);\n                let count = 0;\n                for (let kIx = 0; kIx < kernelSize; kIx++) {\n                    const inputIx = [n, c];\n                    let skip = false;\n                    for (let axis = 0; axis < dataRank; axis++) {\n                        const stride = strides.length === 0 ? 1 : strides[axis];\n                        const pad = pads.length === 0 ? 0 : pads[axis];\n                        const ix = outputIndices[axis + 2] * stride - pad + kernelIndices[axis];\n                        if (ix < 0 || ix >= D[axis]) {\n                            skip = true;\n                            break;\n                        }\n                        inputIx.push(ix);\n                    }\n                    if (!skip) {\n                        const Xi = x.get(inputIx);\n                        result += Xi;\n                    }\n                    if (!skip || includePad) {\n                        count += 1;\n                    }\n                    incrementIndex(kernelIndices, kernelShape);\n                }\n                result = result / count;\n                Y.set(outputIndices, result);\n                incrementIndex(outputIndices, Y.shape);\n            }\n        }\n    }\n    return Y;\n}\n//# sourceMappingURL=averagePool.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nexport function concat(x, y, axis) {\n    const outputShape = [...x.shape];\n    outputShape[axis] += y.shape[axis];\n    const result = new CPUTensor(outputShape, undefined, x.dtype);\n    let indexX = 0;\n    let indexY = 0;\n    let ix = 0;\n    const iterXSize = result.strides[axis] * x.shape[axis];\n    const iterYSize = result.strides[axis] * y.shape[axis];\n    const outerIters = result.size / (axis > 0 ? result.strides[axis - 1] : result.size);\n    for (let i = 0; i < outerIters; i++) {\n        for (let j = 0; j < iterXSize; j++) {\n            result.set(ix, x.get(indexX));\n            ix++;\n            indexX++;\n        }\n        for (let j = 0; j < iterYSize; j++) {\n            result.set(ix, y.get(indexY));\n            ix++;\n            indexY++;\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=concat.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function transpose(x, permutation) {\n    const rank = x.shape.length;\n    const outputShape = new Array(rank);\n    const reversePerm = new Array(rank);\n    for (let i = 0; i < rank; i++) {\n        outputShape[i] = x.shape[permutation[i]];\n        reversePerm[permutation[i]] = i;\n    }\n    const result = new CPUTensor(outputShape, undefined, x.dtype);\n    const resultStrides = result.strides;\n    const mappedStrides = new Array(rank);\n    for (let i = 0; i < rank; i++) {\n        mappedStrides[i] = resultStrides[reversePerm[i]];\n    }\n    const index = new Array(rank).fill(0);\n    for (let i = 0; i < x.size; i++) {\n        let outIx = 0;\n        for (let j = 0; j < rank; j++) {\n            outIx += index[j] * mappedStrides[j];\n        }\n        result.set(outIx, x.get(i));\n        incrementIndex(index, x.shape);\n    }\n    return result;\n}\n//# sourceMappingURL=transpose.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function repeat(x, repeats) {\n    const rank = x.shape.length;\n    const outputShape = new Array(rank);\n    for (let i = 0; i < rank; i++) {\n        outputShape[i] = x.shape[i] * repeats[i];\n    }\n    const result = new CPUTensor(outputShape, undefined, x.dtype);\n    const index = new Array(rank).fill(0);\n    for (let i = 0; i < result.size; i++) {\n        const inIndex = new Array(rank);\n        for (let j = 0; j < rank; j++) {\n            inIndex[j] = index[j] % x.shape[j];\n        }\n        result.set(i, x.get(inIndex));\n        incrementIndex(index, result.shape);\n    }\n    return result;\n}\n//# sourceMappingURL=repeat.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function expand(x, resultShape) {\n    const rank = x.shape.length;\n    const result = new CPUTensor(resultShape, undefined, x.dtype);\n    const index = new Array(rank).fill(0);\n    for (let i = 0; i < result.size; i++) {\n        result.set(i, x.get(index));\n        incrementIndex(index, result.shape);\n    }\n    return result;\n}\n//# sourceMappingURL=expand.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function gather(x, axis, indices) {\n    const r = x.shape.length;\n    const q = indices.shape.length;\n    const resultRank = r + q - 1;\n    const resultShape = new Array(resultRank);\n    for (let i = 0; i < axis; i++) {\n        resultShape[i] = x.shape[i];\n    }\n    for (let i = 0; i < q; i++) {\n        resultShape[i + axis] = indices.shape[i];\n    }\n    for (let i = axis + 1; i < r; i++) {\n        resultShape[i + q - 1] = x.shape[i];\n    }\n    const result = new CPUTensor(resultShape, undefined, x.dtype);\n    const outIx = new Array(resultRank).fill(0);\n    let gatherIx;\n    let inputIx;\n    for (let i = 0; i < result.size; i++) {\n        gatherIx = outIx.slice(axis, axis + q);\n        const axIx = indices.get(gatherIx);\n        inputIx = [...outIx.slice(0, axis), axIx, ...outIx.slice(axis + q)];\n        result.set(i, x.get(inputIx));\n        incrementIndex(outIx, resultShape);\n    }\n    return result;\n}\n//# sourceMappingURL=gather.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function slice(x, starts, ends, axis, steps) {\n    const rank = x.shape.length;\n    const resultShape = [...x.shape];\n    let axIx = 0;\n    for (let i = 0; i < rank && axIx < axis.length; i++) {\n        if (i === axis[axIx]) {\n            resultShape[i] = Math.ceil((ends[axIx] - starts[axIx]) / steps[axIx]);\n            axIx++;\n        }\n    }\n    const result = new CPUTensor(resultShape, undefined, x.dtype);\n    const outIx = new Array(rank).fill(0);\n    let inIx;\n    for (let i = 0; i < result.size; i++) {\n        inIx = new Array(rank);\n        for (let j = 0; j < axis.length; j++) {\n            inIx[axis[j]] = outIx[axis[j]] * steps[j] + starts[j];\n        }\n        result.set(i, x.get(inIx));\n        incrementIndex(outIx, resultShape);\n    }\n    return result;\n}\n//# sourceMappingURL=slice.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function upsample(x, scales) {\n    const rank = x.shape.length;\n    const resultShape = [...x.shape];\n    for (let i = 0; i < rank; i++) {\n        resultShape[i] = Math.floor(resultShape[i] * scales[i]);\n    }\n    const result = new CPUTensor(resultShape, undefined, x.dtype);\n    const outIx = new Array(rank).fill(0);\n    const inIx = new Array(rank);\n    for (let i = 0; i < result.size; i++) {\n        for (let j = 0; j < rank; j++) {\n            inIx[j] = Math.floor(outIx[j] / scales[j]);\n        }\n        result.set(i, x.get(inIx));\n        incrementIndex(outIx, resultShape);\n    }\n    return result;\n}\n//# sourceMappingURL=upsample.js.map","import { CPUTensor } from '../../tensor/cpu/tensor';\nimport { incrementIndex } from '../../util/shape';\nexport function normalize(x, mean, variance, epsilon, scale, bias) {\n    const rank = x.shape.length;\n    const resultShape = [...x.shape];\n    const result = new CPUTensor(resultShape, undefined, x.dtype);\n    const outIx = new Array(rank).fill(0);\n    for (let i = 0; i < result.size; i++) {\n        let res = (x.get(outIx) - mean.get(outIx)) /\n            Math.sqrt(variance.get(outIx) + epsilon);\n        res = res * scale.get(outIx) + bias.get(outIx);\n        result.set(i, res);\n        incrementIndex(outIx, resultShape);\n    }\n    return result;\n}\n//# sourceMappingURL=normalize.js.map","export class Dict {\n    constructor(toNumber) {\n        this.toNumber = toNumber;\n        this.numEntries = 0;\n        this.dict = {};\n    }\n    betweenBoundsFirst(query) {\n        if (query.gte !== undefined) {\n            const k = this.toNumber(query.gte);\n            if (this.dict[k] !== undefined && this.dict[k].length > 0) {\n                return [\n                    {\n                        key: query.gte,\n                        value: this.dict[k][this.dict[k].length - 1],\n                    },\n                ];\n            }\n            return [];\n        }\n        else if (query.lte !== undefined) {\n            const k = this.toNumber(query.lte);\n            if (this.dict[k] !== undefined && this.dict[k].length > 0) {\n                return [\n                    {\n                        key: query.lte,\n                        value: this.dict[k][this.dict[k].length - 1],\n                    },\n                ];\n            }\n            return [];\n        }\n        return [];\n    }\n    deleteFirst(key) {\n        const k = this.toNumber(key);\n        if (this.dict[k] !== undefined) {\n            this.dict[k].pop();\n            this.numEntries--;\n        }\n    }\n    insert(key, value) {\n        const k = this.toNumber(key);\n        if (this.dict[k] === undefined) {\n            this.dict[k] = [];\n        }\n        this.dict[k].push(value);\n        this.numEntries++;\n    }\n}\n//# sourceMappingURL=dict.js.map","export function primeFactors(num) {\n    return primeFactorsCompute(num);\n}\nexport function primeFactorsCompute(inputNum, result = [], repeat = true) {\n    if (!Number.isInteger(inputNum))\n        return result;\n    const num = Math.abs(inputNum);\n    if (num < 2)\n        return result;\n    const sqrt = Math.sqrt(num);\n    let x = 2;\n    if (num % x) {\n        x = 3;\n        if (num % x) {\n            x = 5;\n            let add = 2;\n            while (num % x && x < sqrt) {\n                // search numbers: 5, 7, 11, 13, 17, 19, 23...\n                x += add;\n                // add each time: 2, 4, 2, 4, 2, 4, 2...\n                add = 6 - add;\n            }\n        }\n    }\n    x = x <= sqrt ? x : num;\n    if (!repeat) {\n        const index = result.indexOf(x);\n        if (index < 0)\n            result.push(x);\n    }\n    else\n        result.push(x);\n    return x === num ? result : primeFactorsCompute(num / x, result, repeat);\n}\n/**\n * Generate two normally distributed values using the box-muller transform\n */\nexport function boxMuller() {\n    let u1 = 0;\n    let u2 = 0;\n    while (u1 === 0)\n        u1 = Math.random();\n    while (u2 === 0)\n        u2 = Math.random();\n    const z0 = Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);\n    const z1 = Math.sqrt(-2 * Math.log(u1)) * Math.sin(2 * Math.PI * u2);\n    return [z0, z1];\n}\n/**\n * Generates n normally distributed values using the Box-Muller transform\n * @param n Number of values to generate\n * @param mean Mean of the normal distribution, defaults to 0\n * @param variance Variance of the normal distribution, defaults to 1\n */\nexport function normal(n, mean = 0, variance = 1) {\n    const result = [];\n    for (let i = 0; i < n; i += 2) {\n        const [z0, z1] = boxMuller();\n        result.push(z0 * variance + mean);\n        if (i < n - 1) {\n            result.push(z1 * variance + mean);\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=math.js.map","import { primeFactors } from '../../util/math';\nexport const colorType = {\n    float32: 'float',\n    float16: 'half float',\n    int32: 'float',\n    int16: 'float',\n    int8: 'float',\n    uint32: 'float',\n    uint16: 'float',\n    uint8: 'float',\n};\nconst valsPerTexel = 4;\nexport class GPUMemoryAllocator {\n    constructor(regl, orderedDictConstructor, maxSizeFactor) {\n        this.orderedDictConstructor = orderedDictConstructor;\n        this.totalAllocations = 0;\n        this.trees = {};\n        this.regl = regl;\n        this.entryId = 0;\n        this.maxSizeFactor = maxSizeFactor || 2;\n    }\n    getColorType(dtype) {\n        //@ts-ignore\n        return colorType[dtype];\n    }\n    dtypeGroup(dtype) {\n        if (dtype === 'float16') {\n            return 'float16';\n        }\n        else {\n            // We represent all other data types as float32 textures\n            // This is of course technically not correct, but WebGL only\n            // allows writing/reading float values from textures anyway\n            // and the overhead of converting between the correct dtype and float32\n            // in every shader is considered too big.\n            return 'float32';\n        }\n    }\n    allocate(size, dtype) {\n        const group = this.dtypeGroup(dtype);\n        let upperBound = size * this.maxSizeFactor;\n        const texSize = Math.ceil(size / valsPerTexel) * valsPerTexel;\n        if (texSize < upperBound) {\n            upperBound = texSize;\n        }\n        const results = this.trees[group] !== undefined\n            ? this.trees[group].betweenBoundsFirst({\n                gte: texSize,\n                lte: upperBound,\n            })\n            : [];\n        if (results.length === 0) {\n            const textureSize = Math.ceil(size / valsPerTexel);\n            const { width, height } = this.getTextureDims(textureSize);\n            const framebuffer = this.regl.framebuffer({\n                width: width,\n                height: height,\n                depthStencil: false,\n                colorFormat: 'rgba',\n                colorType: colorType[dtype],\n            });\n            const memoryEntry = {\n                width: width,\n                height: height,\n                size: width * height * valsPerTexel,\n                frameBuffer: framebuffer,\n                id: this.entryId++,\n                dtype: dtype,\n            };\n            this.totalAllocations++;\n            return memoryEntry;\n        }\n        else {\n            const first = results[0];\n            this.trees[group].deleteFirst(first.key);\n            first.value.dtype = dtype;\n            return first.value;\n        }\n    }\n    getAllocationDimensions(size, dtype) {\n        const group = this.dtypeGroup(dtype);\n        let upperBound = size * this.maxSizeFactor;\n        const texSize = Math.ceil(size / valsPerTexel) * valsPerTexel;\n        if (texSize < upperBound) {\n            upperBound = texSize;\n        }\n        const results = this.trees[group] !== undefined\n            ? this.trees[group].betweenBoundsFirst({\n                gte: size,\n                lte: upperBound,\n            })\n            : [];\n        if (results.length === 0) {\n            const textureSize = Math.ceil(size / valsPerTexel);\n            return this.getTextureDims(textureSize);\n        }\n        else {\n            const first = results[0];\n            return {\n                width: first.value.width,\n                height: first.value.height,\n            };\n        }\n    }\n    deallocate(entry) {\n        const group = this.dtypeGroup(entry.dtype);\n        if (this.trees[group] === undefined) {\n            this.trees[group] = this.orderedDictConstructor();\n        }\n        this.trees[group].insert(entry.size, entry);\n    }\n    allocateTexture(values, dtype) {\n        const textureSize = Math.ceil(values.length / valsPerTexel);\n        const { width, height } = this.getTextureDims(textureSize);\n        const arraySize = width * height * valsPerTexel;\n        const vals = new Array(arraySize);\n        for (let i = 0; i < values.length; i++) {\n            vals[i] = values[i];\n        }\n        for (let i = values.length; i < arraySize; i++) {\n            vals[i] = 0;\n        }\n        const texture = this.regl.texture({\n            width: width,\n            height: height,\n            format: 'rgba',\n            type: colorType[dtype],\n            // TODO: Convert data!\n            data: vals,\n        });\n        const framebuffer = this.regl.framebuffer({\n            color: texture,\n            width: width,\n            height: height,\n            depthStencil: false,\n        });\n        this.totalAllocations++;\n        return {\n            width: width,\n            height: height,\n            size: arraySize,\n            frameBuffer: framebuffer,\n            id: this.entryId++,\n            dtype: dtype,\n        };\n    }\n    allocateOfDimensions(width, height, dtype) {\n        const arraySize = width * height * valsPerTexel;\n        const texture = this.regl.texture({\n            width: width,\n            height: height,\n            format: 'rgba',\n            type: colorType[dtype],\n        });\n        const framebuffer = this.regl.framebuffer({\n            color: texture,\n            width: width,\n            height: height,\n            depthStencil: false,\n        });\n        this.totalAllocations++;\n        return {\n            width: width,\n            height: height,\n            size: arraySize,\n            frameBuffer: framebuffer,\n            id: this.entryId++,\n            dtype: dtype,\n        };\n    }\n    allocateFramebuffer(texture, dtype) {\n        const framebuffer = this.regl.framebuffer({\n            color: texture,\n            width: texture.width,\n            height: texture.height,\n            depthStencil: false,\n            colorType: colorType[dtype],\n        });\n        this.totalAllocations++;\n        return {\n            width: texture.width,\n            height: texture.height,\n            size: texture.width * texture.height * valsPerTexel,\n            frameBuffer: framebuffer,\n            id: this.entryId++,\n            dtype: dtype,\n        };\n    }\n    getTextureDims(size) {\n        const factors = primeFactors(size);\n        let width = 1;\n        let height = 1;\n        for (let i = 0; i < factors.length; i += 2) {\n            width *= factors[i];\n            if (i + 1 < factors.length) {\n                height *= factors[i + 1];\n            }\n        }\n        return { width, height };\n    }\n    getNumEntries() {\n        let num = 0;\n        for (const dtype in this.trees) {\n            num += this.trees[dtype].numEntries;\n        }\n        return num;\n    }\n}\n//# sourceMappingURL=memory.js.map","import REGL from 'regl';\nimport { Dict } from '../../util/datastructs/dict';\nimport { GPUMemoryAllocator } from './memory';\nconst canvas = document.createElement('canvas');\nexport let glContext;\nexport let gl;\nexport let defaultAllocator;\nfunction setup() {\n    //@ts-ignore\n    glContext = canvas.getContext('webgl', {\n        failIfMajorPerformanceCaveat: false,\n    });\n    gl = REGL({\n        gl: glContext,\n        extensions: [\n            'OES_texture_float',\n            'WEBGL_color_buffer_float',\n            'OES_texture_half_float',\n        ],\n    });\n    defaultAllocator = new GPUMemoryAllocator(gl, () => {\n        return new Dict((key) => key);\n    });\n}\nsetup();\n//# sourceMappingURL=gl.js.map","import { defaultAllocator, gl } from '../../tensor/gpu/gl';\nimport { computeStrides, getSize } from '../../util/shape';\nexport const defaultMaxRank = 10;\nexport const defaultMaxIterations = 10000000;\n/**\n * A GPU operation takes some input of the InputType and\n * calculates a single GPUTensor\n *\n * This is done with WebGL, which takes some input information of type Info,\n * which is passed to the shader in the form of uniforms.\n */\nexport class Operation {\n    constructor(tensorConstructor, dtype, allocator, maxRank) {\n        this.dtype = dtype;\n        this.statics = new Set();\n        this.copyCounter = 0;\n        this.fullyStatic = false;\n        if (allocator === undefined) {\n            allocator = defaultAllocator;\n        }\n        if (maxRank === undefined) {\n            maxRank = defaultMaxRank;\n        }\n        this.allocator = allocator;\n        this.maxRank = maxRank;\n        this.gpuTensorConstructor = tensorConstructor;\n    }\n    registerStatics(info) {\n        let staticTextures = 0;\n        let staticVars = 0;\n        for (const key in info) {\n            if (key.startsWith('shape')) {\n                const texName = key.slice('shape'.length);\n                this.statics.add(`shape${texName}`);\n                this.statics.add(`size${texName}`);\n                this.statics.add(`rank${texName}`);\n                this.statics.add(`strides${texName}`);\n                staticTextures++;\n            }\n            else {\n                this.statics.add(key);\n                if (!key.startsWith('width') && !key.startsWith('height')) {\n                    staticVars++;\n                }\n            }\n        }\n        if (staticTextures - 1 === this.getTextureNames().length &&\n            staticVars === this.getUniformAttrs().length) {\n            this.fullyStatic = true;\n            this.outputShape = info['shapeOutput'];\n        }\n    }\n    /**\n     * Gets the variable modifier for the WebGL variable with the given name\n     */\n    getVarModifier(name) {\n        return this.statics.has(name) ? '' : 'uniform';\n    }\n    /**\n     * Pads an array to the specified length, or the maxRank by default\n     */\n    pad(arr, len = this.maxRank) {\n        while (arr.length < len) {\n            arr.push(-1);\n        }\n        return arr;\n    }\n    copyPad(arr, len = this.maxRank) {\n        const result = Array.from(arr);\n        while (result.length < len) {\n            result.push(-1);\n        }\n        return result;\n    }\n    /**\n     * Gets the variable declarations for the WebGL shader. Overwrite this if you\n     * need extra uniform inputs\n     */\n    getVariables() {\n        return '';\n    }\n    getVariableDeclarations() {\n        const textures = this.getTextureNames();\n        textures.push('Output');\n        return `\n      ${textures\n            .map(x => {\n            return `\n        ${x === 'Output' ? '' : `uniform sampler2D ${x};`}\n        ${this.getVarModifier('size' + x)} int size${x};\n        ${this.getVarModifier('width' + x)} int width${x};\n        ${this.getVarModifier('height' + x)} int height${x};\n        ${this.getVarModifier('strides' + x)} int strides${x}[${this.maxRank}];\n        ${this.getVarModifier('shape' + x)} int shape${x}[${this.maxRank}];\n        ${this.getVarModifier('rank' + x)} int rank${x};\n        `;\n        })\n            .join('\\n')}\n      varying vec2 uv;\n\n      ${this.getVariables()}`;\n    }\n    getVariableInitializations(info) {\n        const textures = this.getTextureNames();\n        textures.push('Output');\n        let inits = '';\n        for (const tex of textures) {\n            if (`shape${tex}` in info) {\n                const shape = info[`shape${tex}`];\n                const strides = computeStrides(shape);\n                const size = getSize(shape);\n                const rank = shape.length;\n                inits += this.getArrayInit(`shape${tex}`, shape);\n                inits += this.getArrayInit(`strides${tex}`, strides);\n                inits += `\\nsize${tex} = ${size};`;\n                inits += `\\nrank${tex} = ${rank};`;\n            }\n        }\n        for (const k in info) {\n            if (!k.startsWith('shape')) {\n                if (Array.isArray(info[k])) {\n                    inits += this.getArrayInit(k, info[k]);\n                }\n                else {\n                    const type = this.getVarType(k);\n                    if (type === 'int') {\n                        inits += `\\n${k} = ${info[k]};`;\n                    }\n                    else {\n                        inits += `\\n${k} = ${info[k].toPrecision(20)};`;\n                    }\n                }\n            }\n        }\n        return inits;\n    }\n    getVarType(name) {\n        const res = this.getUniformAttrs().find(x => x.name === name);\n        if (res !== undefined) {\n            return res.type ? res.type : 'int';\n        }\n        return 'int';\n    }\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    getArrayInit(name, values, len, pad) {\n        if (len === undefined) {\n            len = this.maxRank;\n        }\n        const type = this.getVarType(name);\n        if (pad === undefined) {\n            if (type === 'int') {\n                pad = '-1';\n            }\n            else if (type === 'float') {\n                pad = '-1.0';\n            }\n        }\n        let res = '';\n        for (let i = 0; i < len; i++) {\n            if (i < values.length) {\n                if (type === 'int') {\n                    res += `\\n ${name}[${i}] = ${values[i]};`;\n                }\n                else if (type === 'float') {\n                    res += `\\n ${name}[${i}] = ${values[i].toPrecision(20)};`;\n                }\n            }\n            else {\n                res += `\\n ${name}[${i}] = ${pad};`;\n            }\n        }\n        return res;\n    }\n    getUtilFunctions() {\n        return `\n    int fromFloat(float f) {\n      return int(floor(f+0.5));\n    }\n\n    int coordinateToPos(vec2 coordinate, int textureWidth, int textureHeight) {\n      int x = (fromFloat(coordinate.x*float(textureWidth*2))-1)/2;\n      int y = (fromFloat(coordinate.y*float(textureHeight*2))-1)/2;\n\n      int pos = x + y*textureWidth;\n\n      return pos*4;\n    }\n\n    vec2 posToCoordinate(int pos, int textureWidth, int textureHeight) {\n      pos = pos/4;\n\n      int y = pos/textureWidth;\n      int x = pos - y*textureWidth;\n\n      return vec2(float(x*2+1)/float(textureWidth*2), float(y*2+1)/float(textureHeight*2));\n    }\n\n    int indexToPos(int index[${this.maxRank}], int strides[${this.maxRank}]) {\n      int pos = 0;\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (strides[i] == -1) {\n          break;\n        }\n        pos += index[i]*strides[i];\n      }\n      return pos;\n    }\n\n    // TODO: Change return type based on dtype of operation\n    // TODO: Change values per texel here\n    float getValueAtPos(int pos, int textureWidth, int textureHeight, sampler2D tex) {\n      vec2 coord = posToCoordinate(pos, textureWidth, textureHeight);\n      int res = pos - (pos/4)*4;\n      vec4 val = texture2D(tex, coord);\n      if (res == 0) {\n        return val.r;\n      } else if (res == 1) {\n        return val.g;\n      } else if (res == 2) {\n        return val.b;\n      } else {\n        return val.a;\n      }\n    }\n\n    // TODO: Change return type based on dtype of operation\n    float getValueAt(int index[${this.maxRank}], int strides[${this.maxRank}], int textureWidth, int textureHeight, sampler2D tex) {\n      int pos = indexToPos(index, strides);\n      return getValueAtPos(pos, textureWidth, textureHeight, tex);\n    }`;\n    }\n    getTextureFunctions() {\n        const textures = this.getTextureNames();\n        return textures\n            .map(x => {\n            return `\n        float _${x}(int indices[${this.maxRank}]) {\n          return getValueAt(indices, strides${x}, width${x}, height${x}, ${x});\n        }\n      `;\n        })\n            .join('\\n');\n    }\n    getCompleteFragmentShader(info) {\n        const fragShader = this.getFragmentShader(info);\n        const variableDecls = this.getVariableDeclarations();\n        const varInits = this.getVariableInitializations(info);\n        const utilFunctions = this.getUtilFunctions();\n        const textureFunctions = this.getTextureFunctions();\n        const result = `\n    // TODO: Change between int/float here\n    precision ${this.precisionString()} float;\n\n    ${variableDecls}\n\n    ${utilFunctions}\n    ${textureFunctions}\n\n    void initVars() {\n      ${varInits}\n    }\n\n    ${fragShader}`;\n        return result;\n    }\n    getUniforms(info) {\n        const uniformAttrs = [];\n        const defaultUniformAttrs = this.getUniformAttrs();\n        for (const defaultAttr of defaultUniformAttrs) {\n            if (info[defaultAttr.name] === undefined) {\n                uniformAttrs.push(defaultAttr);\n            }\n        }\n        const textures = this.getTextureNames();\n        textures.push('Output');\n        for (const texture of textures) {\n            uniformAttrs.push({ name: texture });\n            if (info[`shape${texture}`] === undefined) {\n                uniformAttrs.push({ name: `size${texture}` });\n                uniformAttrs.push({ name: `strides${texture}`, length: this.maxRank });\n                uniformAttrs.push({ name: `shape${texture}`, length: this.maxRank });\n                uniformAttrs.push({ name: `rank${texture}` });\n            }\n            if (info[`width${texture}`] === undefined) {\n                uniformAttrs.push({ name: `width${texture}` });\n            }\n            if (info[`height${texture}`] === undefined) {\n                uniformAttrs.push({ name: `height${texture}` });\n            }\n        }\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        const uniforms = {};\n        for (const uniformAttr of uniformAttrs) {\n            if (info[uniformAttr.name] === undefined) {\n                if (uniformAttr.length !== undefined) {\n                    for (let i = 0; i < uniformAttr.length; i++) {\n                        const name = `${uniformAttr.name}[${i}]`;\n                        uniforms[name] = gl.prop(name);\n                    }\n                }\n                else {\n                    uniforms[uniformAttr.name] = gl.prop(uniformAttr.name);\n                }\n            }\n        }\n        return uniforms;\n    }\n    posToIndex(strides, result, pos) {\n        const name = `${pos}_${this.copyCounter++}`;\n        return `\n    int ${name} = ${pos};\n    for (int i = 0; i < ${this.maxRank}; i++) {\n      if (${strides}[i] == -1) {\n        ${result}[i] = -1;\n      } else {\n        if (${strides}[i] == 0) {\n          ${result}[i] = 0;\n        } else {\n          ${result}[i] = ${name}/${strides}[i];\n          ${name} = ${name} - ${strides}[i]*${result}[i]; // Stupid modulo hack\n        }\n      }\n    }`;\n    }\n    initIndex(index, rank) {\n        if (rank === undefined) {\n            return `\n        for (int i = 0; i < ${this.maxRank}; i++) {\n          ${index}[i] = -1;\n        }`;\n        }\n        else {\n            return `\n        for (int i = 0; i < ${this.maxRank}; i++) {\n          if (i < ${rank}) {\n            ${index}[i] = 0;\n          } else {\n            ${index}[i] = -1;\n          }\n        }`;\n        }\n    }\n    incrementIndex(index, shape) {\n        return `\n    for (int i = ${this.maxRank} - 1; i >= 0; i--) {\n      if (${shape}[i] != -1) {\n        ${index}[i] += 1;\n        if (${index}[i] >= ${shape}[i]) {\n          ${index}[i] = 0;\n        } else {\n          break;\n        }\n      }\n    }\n    `;\n    }\n    incrementConditional(index, shape, cond) {\n        return `\n    for (int i = 0; i < ${this.maxRank}; i++) {\n      if (${cond}[i] == 1) {\n        ${index}[i] += 1;\n        if (${index}[i] >= ${shape}[i]) {\n          ${index}[i] = 0;\n        } else {\n          break;\n        }\n      } else if (${cond}[i] == -1) {\n        break;\n      }\n    }\n    `;\n    }\n    /**\n     * The default main function of the fragment shader.\n     * Unless in special cases, you will use this and your fragment shader will look something like this:\n     *\n     * ```\n     * float process(int index[maxRank]) {\n     *   // Calculate the value of the output at the given index\n     * }\n     *\n     * ${this.getDefaultMain()}\n     * ```\n     */\n    getDefaultMain() {\n        return `\n    void main() {\n      initVars();\n\n      int pos = coordinateToPos(uv, widthOutput, heightOutput);\n\n      // TODO: change number of values per pixel here\n      vec4 result = vec4(0,0,0,0);\n\n      if (pos < sizeOutput) {\n        int index[${this.maxRank}];\n        ${this.posToIndex('stridesOutput', 'index', 'pos')}\n        result.r = process(index);\n\n        pos += 1;\n\n        if (pos < sizeOutput) {\n          ${this.posToIndex('stridesOutput', 'index', 'pos')}\n          result.g = process(index);\n\n          pos += 1;\n\n          if (pos < sizeOutput) {\n            ${this.posToIndex('stridesOutput', 'index', 'pos')}\n            result.b = process(index);\n\n            pos += 1;\n\n            if (pos < sizeOutput) {\n              ${this.posToIndex('stridesOutput', 'index', 'pos')}\n              result.a = process(index);\n            }\n          }\n        }\n      }\n\n      gl_FragColor = result;\n    }`;\n    }\n    precisionString() {\n        // TODO: Change based on current dtype\n        return this.dtype === 'float16' ? 'mediump' : 'highp';\n    }\n    getDrawCommand(info) {\n        const fragShader = this.getCompleteFragmentShader(info);\n        const uniforms = this.getUniforms(info);\n        const result = gl({\n            frag: fragShader,\n            vert: `\n        // TODO: Change between float/int\n        precision ${this.precisionString()} float;\n        attribute vec2 position;\n        varying vec2 uv;\n        void main() {\n          uv = 0.5 * (position + 1.0);\n          gl_Position = vec4(position, 0, 1);\n        }`,\n            attributes: {\n                position: [-4, -4, 4, -4, 0, 4],\n            },\n            uniforms: uniforms,\n            framebuffer: gl.prop('framebuffer'),\n            depth: {\n                enable: false,\n            },\n            count: 3,\n        });\n        return result;\n    }\n    /**\n     * Compiles the fragment shader with the given compilation info and precision\n     *\n     * If you need to add extra compilation info, overwrite this method\n     */\n    compile(info) {\n        this.registerStatics(info);\n        this.drawCommand = this.getDrawCommand(info);\n    }\n    compute(resultShape, inputTensors, \n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    inputs) {\n        if (this.drawCommand === undefined) {\n            this.compile({});\n        }\n        const resultSize = getSize(resultShape);\n        //@ts-ignore\n        const result = this.allocator.allocate(resultSize, this.dtype);\n        for (const i in inputTensors) {\n            const t = inputTensors[i];\n            if (t.memory.id === result.id) {\n                throw new Error(`Allocator returned a framebuffer that is also an input\n                         Did you delete a GPU tensor that was still used elsewhere?`);\n            }\n        }\n        const inputTextures = {};\n        for (const name in inputTensors) {\n            inputTextures[name] = inputTensors[name].memory.frameBuffer;\n        }\n        if (inputs === undefined) {\n            inputs = {};\n        }\n        if (!this.fullyStatic) {\n            for (const name in inputTensors) {\n                if (!this.statics.has(`shape${name}`)) {\n                    inputs[`size${name}`] = inputTensors[name].size;\n                    inputs[`strides${name}`] = this.pad(computeStrides(inputTensors[name].shape));\n                    inputs[`shape${name}`] = this.copyPad(inputTensors[name].shape);\n                    inputs[`rank${name}`] = inputTensors[name].shape.length;\n                }\n                if (!this.statics.has(`width${name}`)) {\n                    inputs[`width${name}`] = inputTensors[name].memory.width;\n                }\n                if (!this.statics.has(`height${name}`)) {\n                    inputs[`height${name}`] = inputTensors[name].memory.height;\n                }\n            }\n            if (!this.statics.has('shapeOutput')) {\n                inputs['sizeOutput'] = resultSize;\n                inputs['stridesOutput'] = this.pad(computeStrides(resultShape));\n                inputs['shapeOutput'] = this.copyPad(resultShape);\n                inputs['rankOutput'] = resultShape.length;\n            }\n            if (!this.statics.has('widthOutput')) {\n                inputs['widthOutput'] = result.width;\n            }\n            if (!this.statics.has('heightOutput')) {\n                inputs['heightOutput'] = result.height;\n            }\n        }\n        //@ts-ignore\n        this.drawCommand(Object.assign(Object.assign({ framebuffer: result.frameBuffer }, inputTextures), inputs));\n        //@ts-ignore\n        return this.gpuTensorConstructor(result, resultShape, this.precision);\n    }\n    getUniformAttrs() {\n        return [];\n    }\n}\n//# sourceMappingURL=operation.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class MatMulOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n        this.maxRank = 2;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int ix1[${this.maxRank}];\n      ${this.initIndex('ix1')}\n      ix1[0] = index[0];\n\n      int ix2[${this.maxRank}];\n      ${this.initIndex('ix2')}\n      ix2[1] = index[1];\n\n      int k = shapeA[1];\n\n      float res = 0.0;\n\n      for (int i = 0; i < ${this.maxIterations}; i++) {\n        if (i >= k) {\n          break;\n        }\n        ix1[1] = i;\n        ix2[0] = i;\n\n        float v1 = _A(ix1);\n        float v2 = _B(ix2);\n        res += v1*v2;\n      }\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['A', 'B'];\n    }\n    calc(input) {\n        const outputShape = this.getOutputShape(input);\n        return this.compute(outputShape, { A: input.A, B: input.B });\n    }\n    getOutputShape(input) {\n        return [input.A.shape[0], input.B.shape[1]];\n    }\n    compile(info) {\n        if (info.shapeA !== undefined) {\n            this.maxIterations = info.shapeA[1];\n        }\n        else if (info.shapeB !== undefined) {\n            this.maxIterations = info.shapeB[0];\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeB: input.A.shape,\n            widthB: input.A.memory.width,\n            heightB: input.A.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.B.shape}`;\n    }\n}\n//# sourceMappingURL=matmul.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class UnaryOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    void main() {\n      initVars();\n\n      gl_FragColor = ${this.operation('texture2D(X, uv)')};\n    }\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    calc(input) {\n        return this.compute(input.input.shape, { X: input.input });\n    }\n    getOutputShape(input) {\n        return input.input.shape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeX: input.input.shape,\n            widthX: input.input.memory.width,\n            heightX: input.input.memory.height,\n            shapeOutput: this.getOutputShape(input),\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.input.shape}`;\n    }\n}\n//# sourceMappingURL=unaryOperation.js.map","import { UnaryOperation } from './unaryOperation';\nexport class ExpOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `exp(${input})`;\n    }\n}\n//# sourceMappingURL=exp.js.map","import { getSize } from '../../../util/shape';\nimport { outputDimsSize } from '../../util/conv';\nimport { Operation } from '../operation';\nimport { defaultAllocator } from '../../../tensor/gpu/gl';\nexport class ConvOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n    }\n    updateInputIx() {\n        return `\n    for (int d = 0; d < ${this.maxRank - 2}; d++) {\n      int stride = strides[d];\n      int pad = pads[d];\n      int dilation = dilations[d];\n      if (stride == -1) {\n        break;\n      }\n\n      inputIx[d+2] = index[d+2]*stride - pad + kernelIx[d+2]*dilation;\n      if (inputIx[d+2] < 0 || inputIx[d+2] >= shapeX[d+2]) {\n        skip = true;\n        break;\n      }\n    }\n    `;\n    }\n    getMainBody() {\n        return `\n    int n = index[0];\n    int m = index[1];\n\n    int kernelIx[${this.maxRank}];\n    ${this.initIndex('kernelIx')}\n    for (int i = 0; i < ${this.maxRank}; i++) {\n      if (i >= dataRank) {\n        break;\n      }\n      kernelIx[i+2] = 0;\n    }\n    kernelIx[0] = m;\n    int inputIx[${this.maxRank}];\n    ${this.initIndex('inputIx')}\n    inputIx[0] = n;\n\n    for (int cg = 0; cg < ${this.maxIterations}; cg++) {\n      if (cg >= CG) {\n        break;\n      }\n      int c = m * CG + cg;\n      int d = c/C;\n      c = c - d*C;\n      inputIx[1] = c;\n      kernelIx[1] = cg;\n      for (int kIx = 0; kIx < ${this.maxIterations}; kIx++) {\n        if (kIx >= kernelSize) {\n          break;\n        }\n\n        bool skip = false;\n\n        ${this.updateInputIx()}\n\n        if (!skip) {\n          res += _X(inputIx) * _W(kernelIx);\n        }\n\n        ${this.incrementIndex('kernelIx', 'shapeW')}\n      }\n    }\n    `;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('CG')} int CG;\n    ${this.getVarModifier('kernelSize')} int kernelSize;\n    ${this.getVarModifier('dataRank')} int dataRank;\n    ${this.getVarModifier('C')} int C;\n    ${this.getVarModifier('dilations')} int dilations[${this.maxRank}];\n    ${this.getVarModifier('pads')} int pads[${this.maxRank}];\n    ${this.getVarModifier('strides')} int strides[${this.maxRank}];\n    ${this.getVarModifier('activation')} int activation;\n    `;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      float res = 0.0;\n\n      ${this.getMainBody()}\n\n      if (activation == 1) {\n        res = max(0.0, res);\n      } else if (activation == 2) {\n        res = max(0.0, min(res,6.0));\n      }\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X', 'W'];\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'CG' },\n            { name: 'kernelSize' },\n            { name: 'C' },\n            { name: 'dataRank' },\n            { name: 'pads', length: this.maxRank * 2 },\n            { name: 'strides', length: this.maxRank },\n            { name: 'dilations', length: this.maxRank },\n            { name: 'activation' },\n        ];\n    }\n    getActivationFlag(activation) {\n        if (activation === 'id') {\n            return 0;\n        }\n        else if (activation === 'relu') {\n            return 1;\n        }\n        else {\n            return 2;\n        }\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.X, W: input.W });\n        }\n        const N = input.X.shape[0];\n        const C = input.X.shape[1];\n        const D = input.X.shape.slice(2);\n        const W = input.W.shape.slice(2);\n        const M = input.W.shape[0];\n        const CG = input.W.shape[1];\n        const kernelSize = getSize(W);\n        const R = outputDimsSize(D, W, input.pads.slice(0, input.pads.length / 2), input.pads.slice(input.pads.length / 2), input.dilations, input.strides);\n        let outputShape = [N, M];\n        outputShape = outputShape.concat(R);\n        return this.compute(outputShape, { X: input.X, W: input.W }, {\n            CG,\n            kernelSize,\n            C,\n            dataRank: D.length,\n            pads: this.copyPad(input.pads, this.maxRank * 2),\n            strides: this.copyPad(input.strides),\n            dilations: this.copyPad(input.dilations),\n            activation: this.getActivationFlag(input.activation),\n        });\n    }\n    getOutputShape(input) {\n        const N = input.X.shape[0];\n        const D = input.X.shape.slice(2);\n        const W = input.W.shape.slice(2);\n        const M = input.W.shape[0];\n        const R = outputDimsSize(D, W, input.pads.slice(0, input.pads.length / 2), input.pads.slice(input.pads.length / 2), input.dilations, input.strides);\n        let outputShape = [N, M];\n        outputShape = outputShape.concat(R);\n        return outputShape;\n    }\n    compile(info) {\n        if (info.shapeW !== undefined) {\n            info.CG = info.shapeW[1];\n            info.kernelSize = getSize(info.shapeW.slice(2));\n            info.dataRank = info.shapeW.length - 2;\n            this.maxRank = info.shapeW.length;\n        }\n        if (info.shapeX !== undefined) {\n            info.C = info.shapeX[1];\n            info.dataRank = info.shapeX.length - 2;\n            this.maxRank = info.shapeX.length;\n        }\n        if (info.activation !== undefined && typeof info.activation === 'string') {\n            info.activation = this.getActivationFlag(info.activation);\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        const kernelSize = getSize(input.W.shape.slice(2));\n        const C = input.X.shape[1];\n        const D = input.X.shape.slice(2);\n        return {\n            shapeX: input.X.shape,\n            widthX: input.X.memory.width,\n            heightX: input.X.memory.height,\n            shapeW: input.W.shape,\n            widthW: input.W.memory.width,\n            heightW: input.W.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            pads: input.pads,\n            dilations: input.dilations,\n            strides: input.strides,\n            CG: input.W.shape[1],\n            kernelSize: kernelSize,\n            dataRank: D.length,\n            C: C,\n            activation: this.getActivationFlag(input.activation),\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.X.shape}-${input.W.shape}-${input.dilations}-${input.pads}-${input.dilations}-${input.strides}-${input.activation}`;\n    }\n}\nexport class ConvBiasOperation extends ConvOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int biasIndex[${this.maxRank}];\n      ${this.initIndex('biasIndex')}\n      biasIndex[0] = index[1];\n      float res = _B(biasIndex);\n\n      ${this.getMainBody()}\n\n      if (activation == 1) {\n        res = max(0.0, res);\n      } else if (activation == 2) {\n        res = max(0.0, min(res,6.0));\n      }\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X', 'W', 'B'];\n    }\n    calc(input) {\n        const N = input.X.shape[0];\n        const C = input.X.shape[1];\n        const D = input.X.shape.slice(2);\n        const W = input.W.shape.slice(2);\n        const M = input.W.shape[0];\n        const CG = input.W.shape[1];\n        const kernelSize = getSize(W);\n        const R = outputDimsSize(D, W, input.pads.slice(0, input.pads.length / 2), input.pads.slice(input.pads.length / 2), input.dilations, input.strides);\n        let outputShape = [N, M];\n        outputShape = outputShape.concat(R);\n        return this.compute(outputShape, { X: input.X, W: input.W, B: input.B }, {\n            CG,\n            kernelSize,\n            C,\n            dataRank: D.length,\n            pads: this.copyPad(input.pads, this.maxRank * 2),\n            strides: this.copyPad(input.strides),\n            dilations: this.copyPad(input.dilations),\n            activation: this.getActivationFlag(input.activation),\n        });\n    }\n    getCompilationInfo(input) {\n        const info = super.getCompilationInfo(input);\n        return Object.assign(Object.assign({}, info), { shapeB: input.B.shape, widthB: input.B.memory.width, heightB: input.B.memory.height });\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.B.shape}`;\n    }\n}\n//# sourceMappingURL=conv.js.map","import { UnaryOperation } from './unaryOperation';\nexport class AbsOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `abs(${input})`;\n    }\n}\n//# sourceMappingURL=abs.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from './../operation';\nexport class BinaryOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int[${this.maxRank}] index) {\n      return ${this.getOp('_A(index)', '_B(index)')};\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getOutputShape(input) {\n        return input.outputShape;\n    }\n    getTextureNames() {\n        return ['A', 'B'];\n    }\n    calc(input) {\n        return this.compute(input.outputShape, { A: input.A, B: input.B });\n    }\n    compile(info) {\n        if (info.shapeA !== undefined) {\n            this.maxRank = info.shapeA.length;\n        }\n        if (info.shapeB !== undefined) {\n            this.maxRank = info.shapeB.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(input.outputShape), this.dtype);\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeB: input.B.shape,\n            widthB: input.B.memory.width,\n            heightB: input.B.memory.height,\n            shapeOutput: input.outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.B.shape}`;\n    }\n}\n//# sourceMappingURL=binaryOperation.js.map","import { BinaryOperation } from './binaryOperation';\nexport class AddOperation extends BinaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getOp(a, b) {\n        return `alpha*${a} + beta*${b}`;\n    }\n    calc(input) {\n        return this.compute(input.outputShape, { A: input.A, B: input.B }, { alpha: input.alpha, beta: input.beta });\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('alpha')} float alpha;\n    ${this.getVarModifier('beta')} float beta;\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'alpha', type: 'float' },\n            { name: 'beta', type: 'float' },\n        ];\n    }\n    getCompilationInfo(input) {\n        const info = super.getCompilationInfo(input);\n        return Object.assign(Object.assign({}, info), { alpha: input.alpha, beta: input.beta });\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.alpha}-${input.beta}`;\n    }\n}\n//# sourceMappingURL=add.js.map","import { BinaryOperation } from './binaryOperation';\nexport class MultiplyOperation extends BinaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getOp(a, b) {\n        return `alpha*${a} * ${b}`;\n    }\n    calc(input) {\n        return this.compute(input.outputShape, { A: input.A, B: input.B }, { alpha: input.alpha });\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('alpha')} float alpha;\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'alpha', type: 'float' }];\n    }\n    getCompilationInfo(input) {\n        const info = super.getCompilationInfo(input);\n        return Object.assign(Object.assign({}, info), { alpha: input.alpha });\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.alpha}`;\n    }\n}\n//# sourceMappingURL=multiply.js.map","import { BinaryOperation } from './binaryOperation';\nexport class SubtractOperation extends BinaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getOp(a, b) {\n        return `alpha*${a} - beta*${b}`;\n    }\n    calc(input) {\n        return this.compute(input.outputShape, { A: input.A, B: input.B }, { alpha: input.alpha, beta: input.beta });\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('alpha')} float alpha;\n    ${this.getVarModifier('beta')} float beta;\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'alpha', type: 'float' },\n            { name: 'beta', type: 'float' },\n        ];\n    }\n    getCompilationInfo(input) {\n        const info = super.getCompilationInfo(input);\n        return Object.assign(Object.assign({}, info), { alpha: input.alpha, beta: input.beta });\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.alpha}-${input.beta}`;\n    }\n}\n//# sourceMappingURL=subtract.js.map","import { BinaryOperation } from './binaryOperation';\nexport class DivideOperation extends BinaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getOp(a, b) {\n        return `alpha*${a} / ${b}`;\n    }\n    calc(input) {\n        return this.compute(input.outputShape, { A: input.A, B: input.B }, { alpha: input.alpha });\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('alpha')} float alpha;\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'alpha', type: 'float' }];\n    }\n    getCompilationInfo(input) {\n        const info = super.getCompilationInfo(input);\n        return Object.assign(Object.assign({}, info), { alpha: input.alpha });\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.alpha}`;\n    }\n}\n//# sourceMappingURL=divide.js.map","import { getSize } from '../../../util/shape';\nimport { outputDimsSize } from '../../util/conv';\nimport { Operation } from '../operation';\nimport { defaultAllocator } from '../../../tensor/gpu/gl';\nexport class AveragePoolOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n    }\n    updateInputIx() {\n        return `\n    for (int d = 0; d < ${this.maxRank - 2}; d++) {\n      int stride = strides[d];\n      int pad = pads[d];\n      if (stride == -1) {\n        break;\n      }\n\n      inputIx[d+2] = index[d+2]*stride - pad + kernelIx[d];\n      if (inputIx[d+2] < 0 || inputIx[d+2] >= shapeX[d+2]) {\n        skip = true;\n        break;\n      }\n    }\n    `;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('kernelSize')} int kernelSize;\n    ${this.getVarModifier('dataRank')} int dataRank;\n    ${this.getVarModifier('includePad')} int includePad;\n    ${this.getVarModifier('pads')} int pads[${this.maxRank}];\n    ${this.getVarModifier('strides')} int strides[${this.maxRank}];\n    ${this.getVarModifier('kernelShape')} int kernelShape[${this.maxRank}];\n    `;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      float res = 0.0;\n      int count = 0;\n\n      int n = index[0];\n      int c = index[1];\n\n      int kernelIx[${this.maxRank}];\n      ${this.initIndex('kernelIx')}\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (i >= dataRank) {\n          break;\n        }\n        kernelIx[i] = 0;\n      }\n      int inputIx[${this.maxRank}];\n      ${this.initIndex('inputIx')}\n      inputIx[0] = n;\n      inputIx[1] = c;\n\n      for (int kIx = 0; kIx < ${this.maxIterations}; kIx++) {\n        if (kIx >= kernelSize) {\n          break;\n        }\n\n        bool skip = false;\n\n        ${this.updateInputIx()}\n\n        if (!skip) {\n          res += _X(inputIx);\n        }\n\n        if (!skip || includePad == 1) {\n          count += 1;\n        }\n\n        ${this.incrementIndex('kernelIx', 'kernelShape')}\n      }\n\n      res = res / float(count);\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'kernelSize' },\n            { name: 'dataRank' },\n            { name: 'includePad' },\n            { name: 'pads', length: this.maxRank * 2 },\n            { name: 'strides', length: this.maxRank },\n            { name: 'kernelShape', length: this.maxRank },\n        ];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.X });\n        }\n        const N = input.X.shape[0];\n        const C = input.X.shape[1];\n        const D = input.X.shape.slice(2);\n        const kernelSize = getSize(input.kernelShape);\n        const R = outputDimsSize(D, input.kernelShape, input.pads.slice(0, input.pads.length / 2), input.pads.slice(input.pads.length / 2), new Array(D.length).fill(1), input.strides);\n        let outputShape = [N, C];\n        outputShape = outputShape.concat(R);\n        return this.compute(outputShape, { X: input.X }, {\n            kernelSize,\n            includePad: input.includePad ? 1 : 0,\n            dataRank: D.length,\n            pads: this.copyPad(input.pads, this.maxRank * 2),\n            strides: this.copyPad(input.strides),\n            kernelShape: this.copyPad(input.kernelShape),\n        });\n    }\n    getOutputShape(input) {\n        const N = input.X.shape[0];\n        const C = input.X.shape[1];\n        const D = input.X.shape.slice(2);\n        const R = outputDimsSize(D, input.kernelShape, input.pads.slice(0, input.pads.length / 2), input.pads.slice(input.pads.length / 2), new Array(D.length).fill(1), input.strides);\n        let outputShape = [N, C];\n        outputShape = outputShape.concat(R);\n        return outputShape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            info.dataRank = info.shapeX.length - 2;\n            this.maxRank = info.shapeX.length;\n        }\n        if (info.includePad === true) {\n            info.includePad = 1;\n        }\n        else if (info.includePad === false) {\n            info.includePad = 0;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        const kernelSize = getSize(input.kernelShape);\n        return {\n            shapeX: input.X.shape,\n            widthX: input.X.memory.width,\n            heightX: input.X.memory.height,\n            kernelShape: input.kernelShape,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            pads: input.pads,\n            strides: input.strides,\n            kernelSize: kernelSize,\n            dataRank: input.X.shape.length - 2,\n            includePad: input.includePad ? 1 : 0,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.X.shape}-${input.kernelShape}-${input.pads}-${input.strides}-${input.includePad}`;\n    }\n}\n//# sourceMappingURL=averagePool.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { computeStrides, getSize } from '../../../util/shape';\nimport { poolResultShape } from '../../util/pool';\nimport { Operation } from '../operation';\nexport class PoolOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('mappedInputStrides')} int mappedInputStrides[${this.maxRank}];\n    ${this.getVarModifier('mappedInputStrides')} int sumDims[${this.maxRank}];\n    ${this.getVarModifier('mappedInputStrides')} int sumSize;\n    `;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int inputIx[${this.maxRank}];\n      ${this.initIndex('inputIx')}\n\n      int inputPos = 0;\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (mappedInputStrides[i] == -1 || index[i] == -1) {\n          break;\n        }\n        inputPos += mappedInputStrides[i]*index[i];\n      }\n\n      ${this.posToIndex('stridesX', 'inputIx', 'inputPos')}\n\n      float res = 0.0;\n\n      for (int i = 0; i < ${this.maxIterations}; i++) {\n        if (i >= sumSize) {\n          break;\n        }\n        float curr = _X(inputIx);\n        if (i == 0) {\n          res = ${this.init('curr')};\n        } else {\n          res = ${this.update('curr', 'res')};\n        }\n\n        ${this.incrementConditional('inputIx', 'shapeX', 'sumDims')}\n      }\n\n      ${this.post('res')}\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    post(res) {\n        return '';\n    }\n    init(res) {\n        return res;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'mappedInputStrides', length: this.maxRank },\n            { name: 'sumDims', length: this.maxRank },\n            { name: 'sumSize' },\n        ];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.X });\n        }\n        const [outputShape, ixMap] = poolResultShape(input.X.shape, input.axes, input.keepDims);\n        const inputStrides = computeStrides(input.X.shape);\n        const mappedInputStrides = [];\n        for (const i of ixMap) {\n            mappedInputStrides.push(inputStrides[i]);\n        }\n        let sumSize = 1;\n        const sumDims = new Array(input.X.shape.length).fill(0);\n        for (let i = 0; i < input.axes.length; i++) {\n            sumDims[input.axes[i]] = 1;\n            sumSize *= input.X.shape[input.axes[i]];\n        }\n        return this.compute(outputShape, { X: input.X }, {\n            mappedInputStrides: this.pad(mappedInputStrides),\n            sumDims: this.pad(sumDims),\n            sumSize,\n        });\n    }\n    getOutputShape(input) {\n        // eslint-disable-next-line @typescript-eslint/no-unused-vars\n        const [outputShape, ixMap] = poolResultShape(input.X.shape, input.axes, input.keepDims);\n        return outputShape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined &&\n            info.axes !== undefined &&\n            info.keepDims !== undefined) {\n            const [outputShape, ixMap] = poolResultShape(info.shapeX, info.axes, info.keepDims);\n            const inputStrides = computeStrides(info.shapeX);\n            const mappedInputStrides = [];\n            for (const i of ixMap) {\n                mappedInputStrides.push(inputStrides[i]);\n            }\n            let sumSize = 1;\n            const sumDims = new Array(info.shapeX.length).fill(0);\n            for (let i = 0; i < info.axes.length; i++) {\n                sumDims[info.axes[i]] = 1;\n                sumSize *= info.shapeX[info.axes[i]];\n            }\n            info.sumDims = sumDims;\n            info.shapeOutput = outputShape;\n            info.mappedInputStrides = mappedInputStrides;\n            info.sumSize = sumSize;\n            delete info['keepDims'];\n            delete info['axes'];\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const [outputShape, ixMap] = poolResultShape(input.X.shape, input.axes, input.keepDims);\n        const inputStrides = computeStrides(input.X.shape);\n        const mappedInputStrides = [];\n        for (const i of ixMap) {\n            mappedInputStrides.push(inputStrides[i]);\n        }\n        let sumSize = 1;\n        const sumDims = new Array(input.X.shape.length).fill(0);\n        for (let i = 0; i < input.axes.length; i++) {\n            sumDims[input.axes[i]] = 1;\n            sumSize *= input.X.shape[input.axes[i]];\n        }\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeX: input.X.shape,\n            widthX: input.X.memory.width,\n            heightX: input.X.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            mappedInputStrides,\n            sumDims,\n            sumSize,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.X.shape}-${input.axes}-${input.keepDims}`;\n    }\n}\n//# sourceMappingURL=pool.js.map","import { PoolOperation } from './pool';\nexport class ReduceMeanOperation extends PoolOperation {\n    update(a, b) {\n        return `${a} + ${b}`;\n    }\n    post(res) {\n        return `${res} = ${res}/float(sumSize);`;\n    }\n}\n//# sourceMappingURL=reduceMean.js.map","import { PoolOperation } from './pool';\nexport class ReduceMeanSquareOperation extends PoolOperation {\n    update(a, b) {\n        return `(${a}*${a}) + ${b}`;\n    }\n    post(res) {\n        return `${res} = ${res}/float(sumSize);`;\n    }\n    init(res) {\n        return `${res}*${res}`;\n    }\n}\n//# sourceMappingURL=reduceMeanSquare.js.map","import { PoolOperation } from './pool';\nexport class SumSquareOperation extends PoolOperation {\n    update(a, b) {\n        return `(${a}*${a}) + ${b}`;\n    }\n    init(res) {\n        return `${res}*${res}`;\n    }\n}\n//# sourceMappingURL=sumSquare.js.map","import { PoolOperation } from './pool';\nexport class SumOperation extends PoolOperation {\n    update(a, b) {\n        return `${a} + ${b}`;\n    }\n}\n//# sourceMappingURL=sum.js.map","import { PoolOperation } from './pool';\nexport class ProductOperation extends PoolOperation {\n    update(a, b) {\n        return `${a} * ${b}`;\n    }\n}\n//# sourceMappingURL=product.js.map","import { PoolOperation } from './pool';\nexport class MaxOperation extends PoolOperation {\n    update(a, b) {\n        return `max(${a}, ${b})`;\n    }\n}\n//# sourceMappingURL=max.js.map","import { PoolOperation } from './pool';\nexport class MinOperation extends PoolOperation {\n    update(a, b) {\n        return `min(${a}, ${b})`;\n    }\n}\n//# sourceMappingURL=min.js.map","import { UnaryOperation } from './unaryOperation';\nexport class CeilOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `ceil(${input})`;\n    }\n}\n//# sourceMappingURL=ceil.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { Operation } from '../operation';\nexport class ClipOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    void main() {\n      initVars();\n\n      vec4 maxVec = vec4(maxVal,maxVal,maxVal,maxVal);\n      vec4 minVec = vec4(minVal,minVal,minVal,minVal);\n\n      vec4 res = texture2D(X, uv);\n      if (doMin == 1) {\n        res = max(minVec, res);\n      }\n      if (doMax == 1) {\n        res = min(maxVec, res);\n      }\n\n      gl_FragColor = res;\n    }\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('minVal')} float minVal;\n    ${this.getVarModifier('maxVal')} float maxVal;\n    ${this.getVarModifier('doMin')} int doMin;\n    ${this.getVarModifier('doMax')} int doMax;\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'minVal', type: 'float' },\n            { name: 'maxVal', type: 'float' },\n            { name: 'doMin' },\n            { name: 'doMax' },\n        ];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.input });\n        }\n        return this.compute(input.input.shape, { X: input.input }, {\n            minVal: input.minVal !== undefined ? input.minVal : 0,\n            maxVal: input.maxVal !== undefined ? input.maxVal : 0,\n            doMin: input.minVal !== undefined ? 1 : 0,\n            doMax: input.maxVal !== undefined ? 1 : 0,\n        });\n    }\n    getOutputShape(input) {\n        return input.input.shape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputSize = defaultAllocator.getAllocationDimensions(input.input.size, this.dtype);\n        return {\n            shapeX: input.input.shape,\n            widthX: input.input.memory.width,\n            heightX: input.input.memory.height,\n            shapeOutput: input.input.shape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            minVal: input.minVal !== undefined ? input.minVal : 0,\n            maxVal: input.maxVal !== undefined ? input.maxVal : 0,\n            doMin: input.minVal !== undefined ? 1 : 0,\n            doMax: input.maxVal !== undefined ? 1 : 0,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.input.shape}-${input.minVal}-${input.maxVal}`;\n    }\n}\n//# sourceMappingURL=clip.js.map","import { UnaryOperation } from './unaryOperation';\nexport class FloorOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `floor(${input})`;\n    }\n}\n//# sourceMappingURL=floor.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class ConcatOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('axis')} int axis;\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'axis' }];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      float res = 0.0;\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (i == axis) {\n          if (index[i] >= shapeA[i]) {\n            index[i] = index[i] - shapeA[i];\n            res = _B(index);\n          } else {\n            res = _A(index);\n          }\n          break;\n        }\n      }\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['A', 'B'];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { A: input.A, B: input.B });\n        }\n        const outputShape = this.getOutputShape(input);\n        return this.compute(outputShape, { A: input.A, B: input.B }, { axis: input.axis });\n    }\n    getOutputShape(input) {\n        const outputShape = [...input.A.shape];\n        outputShape[input.axis] += input.B.shape[input.axis];\n        return outputShape;\n    }\n    compile(info) {\n        if (info.shapeA !== undefined) {\n            this.maxRank = info.shapeA.length;\n        }\n        if (info.shapeB !== undefined) {\n            this.maxRank = info.shapeB.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeB: input.B.shape,\n            widthB: input.B.memory.width,\n            heightB: input.B.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            axis: input.axis,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.B.shape}-${input.axis}`;\n    }\n}\n//# sourceMappingURL=concat.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class CopyOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    void main() {\n      initVars();\n\n      gl_FragColor = texture2D(X, uv);\n    }\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    calc(input) {\n        const shape = this.getOutputShape(input);\n        return this.compute(shape, { X: input.input });\n    }\n    getOutputShape(input) {\n        let shape = input.outputShape;\n        if (shape === undefined) {\n            shape = input.input.shape;\n        }\n        return shape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeX: input.input.shape,\n            widthX: input.input.memory.width,\n            heightX: input.input.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.input}-${this.getOutputShape(input)}`;\n    }\n}\n//# sourceMappingURL=copy.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class ExpandOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      return _X(index);\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    calc(input) {\n        return this.compute(input.outputShape, { X: input.input });\n    }\n    getOutputShape(input) {\n        return input.outputShape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeX: input.input.shape,\n            widthX: input.input.memory.width,\n            heightX: input.input.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.input.shape}-${input.outputShape}`;\n    }\n}\n//# sourceMappingURL=expand.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { computeStrides, getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class GatherOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.gatherMaxIxSize = 10;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('axis')} int axis;\n    ${this.getVarModifier('indexValues')} int indexValues[${this.gatherMaxIxSize}];\n    ${this.getVarModifier('mappedIndexStrides')} int mappedIndexStrides[${this.maxRank}];\n    ${this.getVarModifier('mappedInputStrides')} int mappedInputStrides[${this.maxRank}];\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'axis' },\n            { name: 'indexValues', length: this.gatherMaxIxSize },\n            { name: 'mappedInputStrides', length: this.maxRank },\n            { name: 'mappedIndexStrides', length: this.maxRank },\n        ];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int inputPos = 0;\n      int indexPos = 0;\n\n      int strideAxis = 0;\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (index[i] == -1) {\n          break;\n        }\n        if (i == axis) {\n          strideAxis = stridesX[i];\n        }\n        inputPos += mappedInputStrides[i]*index[i];\n        indexPos += mappedIndexStrides[i]*index[i];\n      }\n\n      for (int i = 0; i < ${this.gatherMaxIxSize}; i++) {\n        if (i == indexPos) {\n          inputPos += indexValues[i]*strideAxis;\n          break;\n        }\n      }\n\n      return getValueAtPos(inputPos, widthX, heightX, X);\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.X });\n        }\n        if (input.indices.size > this.gatherMaxIxSize) {\n            throw new Error(`Gather on GPU can deal with at most ${this.gatherMaxIxSize} indices, input had ${input.indices.size}`);\n        }\n        const r = input.X.shape.length;\n        const q = input.indices.shape.length;\n        const inputStrides = computeStrides(input.X.shape);\n        const indexStrides = computeStrides(input.indices.shape);\n        const resultRank = r + q - 1;\n        const resultShape = new Array(resultRank);\n        const mappedInputStrides = new Array(resultRank).fill(0);\n        const mappedIndexStrides = new Array(resultRank).fill(0);\n        for (let i = 0; i < input.axis; i++) {\n            resultShape[i] = input.X.shape[i];\n            mappedInputStrides[i] = inputStrides[i];\n            mappedIndexStrides[i] = 0;\n        }\n        for (let i = 0; i < q; i++) {\n            resultShape[i + input.axis] = input.indices.shape[i];\n            mappedIndexStrides[i + input.axis] = indexStrides[i];\n            mappedInputStrides[i + input.axis] = 0;\n        }\n        for (let i = input.axis + 1; i < r; i++) {\n            resultShape[i + q - 1] = input.X.shape[i];\n            mappedInputStrides[i + q - 1] = inputStrides[i];\n            mappedIndexStrides[i + q - 1] = 0;\n        }\n        return this.compute(resultShape, { X: input.X }, {\n            axis: input.axis,\n            indexValues: this.pad(Array.from(input.indices.values), this.gatherMaxIxSize),\n            mappedInputStrides: this.pad(mappedInputStrides),\n            mappedIndexStrides: this.pad(mappedIndexStrides),\n        });\n    }\n    getOutputShape(input) {\n        const r = input.X.shape.length;\n        const q = input.indices.shape.length;\n        const resultRank = r + q - 1;\n        const resultShape = new Array(resultRank);\n        for (let i = 0; i < input.axis; i++) {\n            resultShape[i] = input.X.shape[i];\n        }\n        for (let i = 0; i < q; i++) {\n            resultShape[i + input.axis] = input.indices.shape[i];\n        }\n        for (let i = input.axis + 1; i < r; i++) {\n            resultShape[i + q - 1] = input.X.shape[i];\n        }\n        return resultShape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n            if (info.indices !== undefined && info.axis !== undefined) {\n                const r = info.shapeX.length;\n                const q = info.indices.shape.length;\n                const inputStrides = computeStrides(info.shapeX);\n                const indexStrides = computeStrides(info.indices.shape);\n                const resultRank = r + q - 1;\n                const resultShape = new Array(resultRank);\n                const mappedInputStrides = new Array(resultRank).fill(0);\n                const mappedIndexStrides = new Array(resultRank).fill(0);\n                for (let i = 0; i < info.axis; i++) {\n                    resultShape[i] = info.shapeX[i];\n                    mappedInputStrides[i] = inputStrides[i];\n                    mappedIndexStrides[i] = 0;\n                }\n                for (let i = 0; i < q; i++) {\n                    resultShape[i + info.axis] = info.indices.shape[i];\n                    mappedIndexStrides[i + info.axis] = indexStrides[i];\n                    mappedInputStrides[i + info.axis] = 0;\n                }\n                for (let i = info.axis + 1; i < r; i++) {\n                    resultShape[i + q - 1] = info.shapeX[i];\n                    mappedInputStrides[i + q - 1] = inputStrides[i];\n                    mappedIndexStrides[i + q - 1] = 0;\n                }\n                info.mappedIndexStrides = mappedIndexStrides;\n                info.mappedInputStrides = mappedInputStrides;\n                info.indexValues = Array.from(info.indices.values);\n                delete info['indices'];\n            }\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        const r = input.X.shape.length;\n        const q = input.indices.shape.length;\n        const inputStrides = computeStrides(input.X.shape);\n        const indexStrides = computeStrides(input.indices.shape);\n        const resultRank = r + q - 1;\n        const resultShape = new Array(resultRank);\n        const mappedInputStrides = new Array(resultRank).fill(0);\n        const mappedIndexStrides = new Array(resultRank).fill(0);\n        for (let i = 0; i < input.axis; i++) {\n            resultShape[i] = input.X.shape[i];\n            mappedInputStrides[i] = inputStrides[i];\n            mappedIndexStrides[i] = 0;\n        }\n        for (let i = 0; i < q; i++) {\n            resultShape[i + input.axis] = input.indices.shape[i];\n            mappedIndexStrides[i + input.axis] = indexStrides[i];\n            mappedInputStrides[i + input.axis] = 0;\n        }\n        for (let i = input.axis + 1; i < r; i++) {\n            resultShape[i + q - 1] = input.X.shape[i];\n            mappedInputStrides[i + q - 1] = inputStrides[i];\n            mappedIndexStrides[i + q - 1] = 0;\n        }\n        return {\n            shapeX: input.X.shape,\n            widthX: input.X.memory.width,\n            heightX: input.X.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            axis: input.axis,\n            indexValues: Array.from(input.indices.values),\n            mappedIndexStrides,\n            mappedInputStrides,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.X.shape}-${input.axis}-${Array.from(input.indices.values)}-${input.indices.shape}`;\n    }\n}\n//# sourceMappingURL=gather.js.map","import { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nimport { defaultAllocator } from '../../../tensor/gpu/gl';\nexport class GemmOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n    }\n    getMainBody() {\n        return `\n      int ixA[${this.maxRank}];\n      ${this.initIndex('ixA')}\n      int ixB[${this.maxRank}];\n      ${this.initIndex('ixA')}\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (i >= rank - 2) {\n          break;\n        }\n        ixA[i] = index[i];\n        ixB[i] = index[i];\n      }\n\n      int m = 0;\n      int o = 0;\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (i == rank-2) {\n          m = index[i];\n          o = index[i+1];\n\n          if (aTranspose == 0) {\n            ixA[i] = m;\n          } else {\n            ixA[i+1] = m;\n          }\n\n          if (bTranspose == 0) {\n            ixB[i+1] = o;\n          } else {\n            ixB[i] = o;\n          }\n\n          break;\n        }\n      }\n\n      float res = 0.0;\n\n      for (int n = 0; n < ${this.maxIterations}; n++) {\n        if (n >= N) {\n          break;\n        }\n        for (int i = 0; i < ${this.maxRank}; i++) {\n          if (i == rank-2) {\n            if (aTranspose == 0) {\n              ixA[i+1] = n;\n            } else {\n              ixA[i] = n;\n            }\n\n            if (bTranspose == 0) {\n              ixB[i] = n;\n            } else {\n              ixB[i+1] = n;\n            }\n\n            break;\n          }\n        }\n        res += _A(ixA) * _B(ixB);\n      }\n\n      res = res*alpha;\n    `;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('M')} int M;\n    ${this.getVarModifier('N')} int N;\n    ${this.getVarModifier('O')} int O;\n    ${this.getVarModifier('rank')} int rank;\n    ${this.getVarModifier('aTranspose')} int aTranspose;\n    ${this.getVarModifier('bTranspose')} int bTranspose;\n    ${this.getVarModifier('alpha')} float alpha;\n    ${this.getVarModifier('beta')} float beta;\n    `;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      ${this.getMainBody()}\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['A', 'B'];\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'M' },\n            { name: 'N' },\n            { name: 'O' },\n            { name: 'rank' },\n            { name: 'aTranspose' },\n            { name: 'bTranspose' },\n            { name: 'alpha', type: 'float' },\n            { name: 'beta', type: 'float' },\n        ];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { A: input.a, B: input.b });\n        }\n        const rank = input.a.shape.length;\n        const M = input.aTranspose\n            ? input.a.shape[rank - 1]\n            : input.a.shape[rank - 2];\n        const N = input.aTranspose\n            ? input.a.shape[rank - 2]\n            : input.a.shape[rank - 1];\n        const O = input.bTranspose\n            ? input.b.shape[rank - 2]\n            : input.b.shape[rank - 1];\n        const batchShape = input.a.shape.slice(0, rank - 2);\n        const resultShape = [...batchShape, M, O];\n        const uniforms = {\n            M,\n            N,\n            O,\n            rank,\n            aTranspose: input.aTranspose ? 1 : 0,\n            bTranspose: input.bTranspose ? 1 : 0,\n            alpha: input.alpha,\n            beta: input.beta,\n        };\n        return this.compute(resultShape, { A: input.a, B: input.b }, uniforms);\n    }\n    getOutputShape(input) {\n        const rank = input.a.shape.length;\n        const M = input.aTranspose\n            ? input.a.shape[rank - 1]\n            : input.a.shape[rank - 2];\n        const O = input.bTranspose\n            ? input.b.shape[rank - 2]\n            : input.b.shape[rank - 1];\n        const batchShape = input.a.shape.slice(0, rank - 2);\n        const resultShape = [...batchShape, M, O];\n        return resultShape;\n    }\n    compile(info) {\n        if (info.shapeA !== undefined) {\n            const rank = info.shapeA.length;\n            info.rank = rank;\n            this.maxRank = rank;\n            if (info.aTranspose !== undefined) {\n                const M = info.aTranspose\n                    ? info.shapeA[rank - 1]\n                    : info.shapeA[rank - 2];\n                const N = info.aTranspose\n                    ? info.shapeA[rank - 2]\n                    : info.shapeA[rank - 1];\n                info.M = M;\n                info.N = N;\n                info.aTranspose = info.aTranspose ? 1 : 0;\n            }\n        }\n        if (info.shapeB !== undefined && info.bTranspose !== undefined) {\n            const rank = info.shapeB.length;\n            const O = info.bTranspose ? info.shapeB[rank - 2] : info.shapeB[rank - 1];\n            info.O = O;\n            info.bTranspose = info.bTranspose ? 1 : 0;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        const rank = input.a.shape.length;\n        const M = input.aTranspose\n            ? input.a.shape[rank - 1]\n            : input.a.shape[rank - 2];\n        const N = input.aTranspose\n            ? input.a.shape[rank - 2]\n            : input.a.shape[rank - 1];\n        const O = input.bTranspose\n            ? input.b.shape[rank - 2]\n            : input.b.shape[rank - 1];\n        const info = {\n            shapeA: input.a.shape,\n            widthA: input.a.memory.width,\n            heightA: input.a.memory.height,\n            shapeB: input.b.shape,\n            widthB: input.b.memory.width,\n            heightB: input.b.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            M,\n            N,\n            O,\n            aTranspose: input.aTranspose ? 1 : 0,\n            bTranspose: input.bTranspose ? 1 : 0,\n            alpha: input.alpha,\n            beta: input.beta,\n            rank,\n        };\n        return info;\n    }\n    getInputInfoString(input) {\n        //TODO: Check precision of alpha and beta\n        return `${input.a.shape}-${input.b.shape}-${input.aTranspose}-${input.bTranspose}-${input.alpha}-${input.beta}`;\n    }\n}\nexport class GemmCOperation extends GemmOperation {\n    getTextureNames() {\n        return ['A', 'B', 'C'];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      ${this.getMainBody()}\n\n      res += beta*_C(index);\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, {\n                A: input.a,\n                B: input.b,\n                C: input.c,\n            });\n        }\n        const rank = input.a.shape.length;\n        const M = input.aTranspose\n            ? input.a.shape[rank - 1]\n            : input.a.shape[rank - 2];\n        const N = input.aTranspose\n            ? input.a.shape[rank - 2]\n            : input.a.shape[rank - 1];\n        const O = input.bTranspose\n            ? input.b.shape[rank - 2]\n            : input.b.shape[rank - 1];\n        const batchShape = input.a.shape.slice(0, rank - 2);\n        const resultShape = [...batchShape, M, O];\n        const uniforms = {\n            M,\n            N,\n            O,\n            rank,\n            aTranspose: input.aTranspose ? 1 : 0,\n            bTranspose: input.bTranspose ? 1 : 0,\n            alpha: input.alpha,\n            beta: input.beta,\n        };\n        return this.compute(resultShape, { A: input.a, B: input.b, C: input.c }, uniforms);\n    }\n    getCompilationInfo(input) {\n        const inf = super.getCompilationInfo(input);\n        const info = Object.assign(Object.assign({}, inf), { shapeC: input.c.shape, widthC: input.c.memory.width, heightC: input.c.memory.height });\n        return info;\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.c.shape}`;\n    }\n}\n//# sourceMappingURL=gemm.js.map","import { BinaryOperation } from './binaryOperation';\nexport class PowerOperation extends BinaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getOp(a, b) {\n        return `pow(${a}, ${b})`;\n    }\n}\n//# sourceMappingURL=power.js.map","import { UnaryOperation } from './unaryOperation';\nexport class SqrtOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `sqrt(${input})`;\n    }\n}\n//# sourceMappingURL=sqrt.js.map","import { UnaryOperation } from './unaryOperation';\nexport class LogOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `log(${input})`;\n    }\n}\n//# sourceMappingURL=log.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { computeStrides, getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class TransposeOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('mappedStrides')} int mappedStrides[${this.maxRank}];\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'mappedStrides', length: this.maxRank }];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      return getValueAt(index, mappedStrides, widthA, heightA, A);\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['A'];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { A: input.A });\n        }\n        const rank = input.A.shape.length;\n        const outputShape = this.getOutputShape(input);\n        const inputStrides = computeStrides(input.A.shape);\n        const mappedStrides = new Array(rank);\n        for (let i = 0; i < rank; i++) {\n            mappedStrides[i] = inputStrides[input.permutation[i]];\n        }\n        return this.compute(outputShape, { A: input.A }, { mappedStrides: this.pad(mappedStrides) });\n    }\n    getOutputShape(input) {\n        const rank = input.A.shape.length;\n        const outputShape = new Array(rank);\n        for (let i = 0; i < rank; i++) {\n            outputShape[i] = input.A.shape[input.permutation[i]];\n        }\n        return outputShape;\n    }\n    compile(info) {\n        if (info.shapeA !== undefined) {\n            this.maxRank = info.shapeA.length;\n            if (info.permutation !== undefined) {\n                const rank = info.shapeA.length;\n                const inputStrides = computeStrides(info.shapeA);\n                const mappedStrides = new Array(rank);\n                for (let i = 0; i < rank; i++) {\n                    mappedStrides[i] = inputStrides[info.permutation[i]];\n                }\n                info.mappedStrides = mappedStrides;\n                delete info['permutation'];\n            }\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        const rank = input.A.shape.length;\n        const inputStrides = computeStrides(input.A.shape);\n        const mappedStrides = new Array(rank);\n        for (let i = 0; i < rank; i++) {\n            mappedStrides[i] = inputStrides[input.permutation[i]];\n        }\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            mappedStrides,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.permutation}`;\n    }\n}\n//# sourceMappingURL=transpose.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class RepeatOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('repeats')} int repeats[${this.maxRank}];\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'repeats', length: this.maxRank }];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int inIndex[${this.maxRank}];\n      ${this.initIndex('inIndex')}\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (repeats[i] == -1) {\n          break;\n        }\n        int d = index[i] / shapeA[i];\n        inIndex[i] = index[i] - d*shapeA[i];\n      }\n\n      return _A(inIndex);\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getOutputShape(input) {\n        const rank = input.A.shape.length;\n        const outputShape = new Array(rank);\n        for (let i = 0; i < rank; i++) {\n            outputShape[i] = input.A.shape[i] * input.repeats[i];\n        }\n        return outputShape;\n    }\n    getTextureNames() {\n        return ['A'];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { A: input.A });\n        }\n        const outputShape = this.getOutputShape(input);\n        return this.compute(outputShape, { A: input.A }, { repeats: this.copyPad(input.repeats) });\n    }\n    compile(info) {\n        if (info.shapeA !== undefined) {\n            this.maxRank = info.shapeA.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            repeats: input.repeats,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.repeats}`;\n    }\n}\n//# sourceMappingURL=repeat.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class PadOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int inputIx[${this.maxRank}];\n      ${this.initIndex('inputIx')}\n      if (mode == 0) {\n        float res = value;\n\n        int outOfBounds = 0;\n        for (int i = 0; i < ${this.maxRank}; i++) {\n          if (index[i] == -1) {\n            break;\n          }\n          inputIx[i] = index[i] - pads[i];\n          if (inputIx[i] < 0 || inputIx[i] >= shapeX[i]) {\n            outOfBounds = 1;\n            break;\n          }\n        }\n\n        if (outOfBounds == 0) {\n          res = _X(inputIx);\n        }\n\n        return res;\n      } else if (mode == 1) {\n        for (int i = 0; i < ${this.maxRank}; i++) {\n          if (index[i] == -1) {\n            break;\n          }\n          inputIx[i] = index[i] - pads[i];\n          if (inputIx[i] < 0) {\n            inputIx[i] = -inputIx[i];\n          } else if (inputIx[i] >= shapeX[i]) {\n            inputIx[i] = 2*shapeX[i] - inputIx[i] - 2;\n          }\n        }\n\n        return _X(inputIx);\n      } else {\n        for (int i = 0; i < ${this.maxRank}; i++) {\n          if (index[i] == -1) {\n            break;\n          }\n          inputIx[i] = index[i] - pads[i];\n          if (inputIx[i] < 0) {\n            inputIx[i] = 0;\n          } else if (inputIx[i] >= shapeX[i]) {\n            inputIx[i] = shapeX[i] - 1;\n          }\n        }\n\n        return _X(inputIx);\n      }\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('pads')} int pads[${this.maxRank * 2}];\n    ${this.getVarModifier('value')} float value;\n    ${this.getVarModifier('mode')} int mode;\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'value', type: 'float' },\n            { name: 'pads', length: this.maxRank * 2 },\n            { name: 'mode' },\n        ];\n    }\n    getModeFlag(mode) {\n        return mode === 'constant' ? 0 : mode === 'reflect' ? 1 : 2;\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.input });\n        }\n        const resultShape = this.getOutputShape(input);\n        return this.compute(resultShape, { X: input.input }, {\n            pads: this.copyPad(input.pads, this.maxRank * 2),\n            value: input.value,\n            mode: this.getModeFlag(input.mode),\n        });\n    }\n    getOutputShape(input) {\n        const rank = input.input.shape.length;\n        const resultShape = [...input.input.shape];\n        for (let i = 0; i < rank; i++) {\n            resultShape[i] += input.pads[i] + input.pads[i + rank];\n        }\n        return resultShape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n        }\n        if (info.mode !== undefined && typeof info.mode === 'string') {\n            info.mode = this.getModeFlag(info.mode);\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeX: input.input.shape,\n            widthX: input.input.memory.width,\n            heightX: input.input.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            pads: input.pads,\n            mode: this.getModeFlag(input.mode),\n            value: input.value,\n        };\n    }\n    getInputInfoString(input) {\n        //TODO: Format value with enough precision?\n        return `${input.input.shape}-${input.pads}-${input.mode}-${input.value}`;\n    }\n}\n//# sourceMappingURL=pad.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class SliceOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int inIx[${this.maxRank}];\n      ${this.initIndex('inIx')}\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (index[i] == -1) {\n          break;\n        }\n\n        inIx[i] = index[i]*steps[i] + offsets[i];\n      }\n\n      return _X(inIx);\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('offsets')} int offsets[${this.maxRank}];\n    ${this.getVarModifier('steps')} int steps[${this.maxRank}];\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'offsets', length: this.maxRank },\n            { name: 'steps', length: this.maxRank },\n        ];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.X });\n        }\n        const rank = input.X.shape.length;\n        const resultShape = [...input.X.shape];\n        const offsets = new Array(rank).fill(0);\n        const steps = new Array(rank).fill(1);\n        let axIx = 0;\n        for (let i = 0; i < rank && axIx < input.axes.length; i++) {\n            if (i === input.axes[axIx]) {\n                resultShape[i] = Math.ceil((input.ends[axIx] - input.starts[axIx]) / input.steps[axIx]);\n                offsets[i] = input.starts[axIx];\n                steps[i] = input.steps[axIx];\n                axIx++;\n            }\n        }\n        return this.compute(resultShape, { X: input.X }, {\n            offsets: this.pad(offsets),\n            steps: this.pad(steps),\n        });\n    }\n    getOutputShape(input) {\n        const rank = input.X.shape.length;\n        const resultShape = [...input.X.shape];\n        let axIx = 0;\n        for (let i = 0; i < rank && axIx < input.axes.length; i++) {\n            if (i === input.axes[axIx]) {\n                resultShape[i] = Math.ceil((input.ends[axIx] - input.starts[axIx]) / input.steps[axIx]);\n                axIx++;\n            }\n        }\n        return resultShape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n            if (info.axes !== undefined &&\n                info.starts !== undefined &&\n                info.ends !== undefined &&\n                info.steps !== undefined) {\n                const rank = info.shapeX.length;\n                const offsets = new Array(rank).fill(0);\n                const steps = new Array(rank).fill(1);\n                let axIx = 0;\n                for (let i = 0; i < rank && axIx < info.axes.length; i++) {\n                    if (i === info.axes[axIx]) {\n                        offsets[i] = info.starts[axIx];\n                        steps[i] = info.steps[axIx];\n                        axIx++;\n                    }\n                }\n                info.offsets = offsets;\n                info.steps = steps;\n                delete info['starts'];\n                delete info['ends'];\n                delete info['axes'];\n            }\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        const rank = input.X.shape.length;\n        const resultShape = [...input.X.shape];\n        const offsets = new Array(rank).fill(0);\n        const steps = new Array(rank).fill(1);\n        let axIx = 0;\n        for (let i = 0; i < rank && axIx < input.axes.length; i++) {\n            if (i === input.axes[axIx]) {\n                resultShape[i] = Math.ceil((input.ends[axIx] - input.starts[axIx]) / input.steps[axIx]);\n                offsets[i] = input.starts[axIx];\n                steps[i] = input.steps[axIx];\n                axIx++;\n            }\n        }\n        return {\n            shapeX: input.X.shape,\n            widthX: input.X.memory.width,\n            heightX: input.X.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            offsets,\n            steps,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.X.shape}-${input.axes}-${input.starts}-${input.ends}-${input.steps}`;\n    }\n}\n//# sourceMappingURL=slice.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class UpsampleOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int inIx[${this.maxRank}];\n      ${this.initIndex('inIx')}\n\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (index[i] == -1) {\n          break;\n        }\n\n        inIx[i] = int(floor(float(index[i]) / scales[i]));\n      }\n\n      return _X(inIx);\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X'];\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('scales')} float scales[${this.maxRank}];\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'scales', length: this.maxRank, type: 'float' }];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.X });\n        }\n        const resultShape = this.getOutputShape(input);\n        return this.compute(resultShape, { X: input.X }, {\n            scales: this.copyPad(input.scales),\n        });\n    }\n    getOutputShape(input) {\n        const rank = input.X.shape.length;\n        const resultShape = [...input.X.shape];\n        for (let i = 0; i < rank; i++) {\n            resultShape[i] = Math.floor(resultShape[i] * input.scales[i]);\n        }\n        return resultShape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeX: input.X.shape,\n            widthX: input.X.memory.width,\n            heightX: input.X.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            scales: input.scales,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.X.shape}-${input.scales}`;\n    }\n}\n//# sourceMappingURL=upsample.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class NormalizeOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('epsilon')} float epsilon;\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'epsilon', type: 'float' }];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int[${this.maxRank}] index) {\n      float result = _X(index) - _Mean(index);\n      result = result / sqrt(_Variance(index) + epsilon);\n      result = result * _Scale(index) + _Bias(index);\n      return result;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getOutputShape(input) {\n        return input.X.shape;\n    }\n    getTextureNames() {\n        return ['X', 'Mean', 'Variance', 'Scale', 'Bias'];\n    }\n    calc(input) {\n        return this.compute(input.X.shape, {\n            X: input.X,\n            Mean: input.Mean,\n            Variance: input.Variance,\n            Scale: input.Scale,\n            Bias: input.Bias,\n        }, { epsilon: input.epsilon });\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeX: input.X.shape,\n            widthX: input.X.memory.width,\n            heightX: input.X.memory.height,\n            shapeBias: input.Bias.shape,\n            widthBias: input.Bias.memory.width,\n            heightBias: input.Bias.memory.height,\n            shapeMean: input.Mean.shape,\n            widthMean: input.Mean.memory.width,\n            heightMean: input.Mean.memory.height,\n            shapeScale: input.Scale.shape,\n            widthScale: input.Scale.memory.width,\n            heightScale: input.Scale.memory.height,\n            shapeVariance: input.Variance.shape,\n            widthVariance: input.Variance.memory.width,\n            heightVariance: input.Variance.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            epsilon: input.epsilon,\n        };\n    }\n    getInputInfoString(input) {\n        // TODO: Format epsilon with enough precision?\n        return `${input.X.shape}-${input.Mean.shape}-${input.Variance.shape}-${input.Scale.shape}-${input.Bias.shape}-${input.epsilon}`;\n    }\n}\n//# sourceMappingURL=normalize.js.map","export class Dispatcher {\n    constructor(getOp, minCallsToCompile = 2) {\n        this.getOp = getOp;\n        this.minCallsToCompile = minCallsToCompile;\n        this.opDict = {};\n    }\n    getDefault(dtype) {\n        const str = `default-${dtype}`;\n        if (this.opDict[str] === undefined) {\n            const op = this.getOp(dtype);\n            op.compile({});\n            this.opDict[str] = {\n                infoString: str,\n                numCalls: 0,\n                operation: op,\n            };\n        }\n        return this.opDict[str];\n    }\n    calc(input, dtype) {\n        const defaultOp = this.getDefault(dtype);\n        //@ts-ignore\n        const compileInfoString = defaultOp.operation.getInputInfoString(input);\n        if (this.opDict[compileInfoString] === undefined) {\n            this.opDict[compileInfoString] = {\n                infoString: compileInfoString,\n                numCalls: 0,\n            };\n        }\n        const opInfo = this.opDict[compileInfoString];\n        opInfo.numCalls++;\n        if (opInfo.numCalls >= this.minCallsToCompile) {\n            if (opInfo.operation === undefined) {\n                opInfo.operation = this.getOp(dtype);\n                //@ts-ignore\n                const compileInfo = defaultOp.operation.getCompilationInfo(input);\n                opInfo.operation.compile(compileInfo);\n            }\n            return opInfo.operation.calc(input);\n        }\n        else {\n            defaultOp.numCalls++;\n            //@ts-ignore\n            return defaultOp.operation.calc(input);\n        }\n    }\n}\n//# sourceMappingURL=dispatcher.js.map","import { UnaryOperation } from './unaryOperation';\nexport class SignOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `sign(${input})`;\n    }\n}\n//# sourceMappingURL=sign.js.map","import { UnaryOperation } from './unaryOperation';\nexport class NegateOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `-${input}`;\n    }\n}\n//# sourceMappingURL=negate.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { Operation } from '../operation';\nexport class ClipBackwardOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      float val = _X(index);\n      if (doMin == 1 && val < minVal) {\n        return 0.0;\n      }\n      if (doMax == 1 && val > maxVal) {\n        return 0.0;\n      }\n      return _Grad(index);\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X', 'Grad'];\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('minVal')} float minVal;\n    ${this.getVarModifier('maxVal')} float maxVal;\n    ${this.getVarModifier('doMin')} int doMin;\n    ${this.getVarModifier('doMax')} int doMax;\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'minVal', type: 'float' },\n            { name: 'maxVal', type: 'float' },\n            { name: 'doMin' },\n            { name: 'doMax' },\n        ];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.input, Grad: input.grad });\n        }\n        return this.compute(input.input.shape, { X: input.input, Grad: input.grad }, {\n            minVal: input.minVal !== undefined ? input.minVal : 0,\n            maxVal: input.maxVal !== undefined ? input.maxVal : 0,\n            doMin: input.minVal !== undefined ? 1 : 0,\n            doMax: input.maxVal !== undefined ? 1 : 0,\n        });\n    }\n    getOutputShape(input) {\n        return input.input.shape;\n    }\n    compile(info) {\n        if (info.shapeX !== undefined) {\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputSize = defaultAllocator.getAllocationDimensions(input.input.size, this.dtype);\n        return {\n            shapeX: input.input.shape,\n            widthX: input.input.memory.width,\n            heightX: input.input.memory.height,\n            shapeGrad: input.grad.shape,\n            widthGrad: input.grad.memory.width,\n            heightGrad: input.grad.memory.height,\n            shapeOutput: input.input.shape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            minVal: input.minVal !== undefined ? input.minVal : 0,\n            maxVal: input.maxVal !== undefined ? input.maxVal : 0,\n            doMin: input.minVal !== undefined ? 1 : 0,\n            doMax: input.maxVal !== undefined ? 1 : 0,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.input.shape}-${input.minVal}-${input.maxVal}`;\n    }\n}\n//# sourceMappingURL=clipBackward.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { outputDimsSize } from '../../util/convTranspose';\nimport { Operation } from '../operation';\nexport class ConvTransposeOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n    }\n    updateInputIx() {\n        return `\n    for (int d = 0; d < ${this.maxRank - 2}; d++) {\n      int stride = strides[d];\n      int pad = pads[d];\n      int dilation = dilations[d];\n      if (stride == -1) {\n        break;\n      }\n\n      int trans_kernel_ix = shapeW[d + 2] - kernelIx[d + 2] - 1;\n\n      inputIx[d+2] = index[d + 2] - pad + trans_kernel_ix * dilation;\n\n      int divS = inputIx[d+2] / stride;\n      int resS = inputIx[d+2] - divS*stride;\n\n      if (resS != 0) {\n        skip = true;\n        break;\n      }\n      inputIx[d+2] = divS;\n\n      if (inputIx[d+2] < 0 || inputIx[d+2] >= shapeX[d+2]) {\n        skip = true;\n        break;\n      }\n    }\n    `;\n    }\n    getMainBody() {\n        return `\n    int n = index[0];\n    int m = index[1];\n\n    int kernelIx[${this.maxRank}];\n    ${this.initIndex('kernelIx')}\n    for (int i = 0; i < ${this.maxRank}; i++) {\n      if (i >= dataRank) {\n        break;\n      }\n      kernelIx[i+2] = 0;\n    }\n    kernelIx[0] = m;\n    int inputIx[${this.maxRank}];\n    ${this.initIndex('inputIx')}\n    inputIx[0] = n;\n\n    for (int cg = 0; cg < ${this.maxIterations}; cg++) {\n      if (cg >= CG) {\n        break;\n      }\n      int c = m * CG + cg;\n      int d = c/C;\n      c = c - d*C;\n      inputIx[1] = c;\n      kernelIx[1] = cg;\n      for (int kIx = 0; kIx < ${this.maxIterations}; kIx++) {\n        if (kIx >= kernelSize) {\n          break;\n        }\n\n        bool skip = false;\n\n        ${this.updateInputIx()}\n\n        if (!skip) {\n          res += _X(inputIx) * _W(kernelIx);\n        }\n\n        ${this.incrementIndex('kernelIx', 'shapeW')}\n      }\n    }\n    `;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('CG')} int CG;\n    ${this.getVarModifier('kernelSize')} int kernelSize;\n    ${this.getVarModifier('dataRank')} int dataRank;\n    ${this.getVarModifier('C')} int C;\n    ${this.getVarModifier('dilations')} int dilations[${this.maxRank}];\n    ${this.getVarModifier('pads')} int pads[${this.maxRank}];\n    ${this.getVarModifier('strides')} int strides[${this.maxRank}];\n    `;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      float res = 0.0;\n\n      ${this.getMainBody()}\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['X', 'W'];\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'CG' },\n            { name: 'kernelSize' },\n            { name: 'C' },\n            { name: 'dataRank' },\n            { name: 'pads', length: this.maxRank * 2 },\n            { name: 'strides', length: this.maxRank },\n            { name: 'dilations', length: this.maxRank },\n        ];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { X: input.X, W: input.W });\n        }\n        const N = input.X.shape[0];\n        const C = input.X.shape[1];\n        const D = input.X.shape.slice(2);\n        const W = input.W.shape.slice(2);\n        const M = input.W.shape[0];\n        const CG = input.W.shape[1];\n        const kernelSize = getSize(W);\n        const R = outputDimsSize(D, W, input.pads.slice(0, input.pads.length / 2), input.pads.slice(input.pads.length / 2), input.dilations, input.strides);\n        let outputShape = [N, M];\n        outputShape = outputShape.concat(R);\n        return this.compute(outputShape, { X: input.X, W: input.W }, {\n            CG,\n            kernelSize,\n            C,\n            dataRank: D.length,\n            pads: this.copyPad(input.pads, this.maxRank * 2),\n            strides: this.copyPad(input.strides),\n            dilations: this.copyPad(input.dilations),\n        });\n    }\n    getOutputShape(input) {\n        const N = input.X.shape[0];\n        const D = input.X.shape.slice(2);\n        const W = input.W.shape.slice(2);\n        const M = input.W.shape[0];\n        const R = outputDimsSize(D, W, input.pads.slice(0, input.pads.length / 2), input.pads.slice(input.pads.length / 2), input.dilations, input.strides);\n        let outputShape = [N, M];\n        outputShape = outputShape.concat(R);\n        return outputShape;\n    }\n    compile(info) {\n        if (info.shapeW !== undefined) {\n            info.CG = info.shapeW[1];\n            info.kernelSize = getSize(info.shapeW.slice(2));\n            info.dataRank = info.shapeW.length - 2;\n            this.maxRank = info.shapeW.length;\n        }\n        if (info.shapeX !== undefined) {\n            info.C = info.shapeX[1];\n            info.dataRank = info.shapeX.length - 2;\n            this.maxRank = info.shapeX.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        const kernelSize = getSize(input.W.shape.slice(2));\n        const C = input.X.shape[1];\n        const D = input.X.shape.slice(2);\n        return {\n            shapeX: input.X.shape,\n            widthX: input.X.memory.width,\n            heightX: input.X.memory.height,\n            shapeW: input.W.shape,\n            widthW: input.W.memory.width,\n            heightW: input.W.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            pads: input.pads,\n            dilations: input.dilations,\n            strides: input.strides,\n            CG: input.W.shape[1],\n            kernelSize: kernelSize,\n            dataRank: D.length,\n            C: C,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.X.shape}-${input.W.shape}-${input.dilations}-${input.pads}-${input.dilations}-${input.strides}`;\n    }\n}\n//# sourceMappingURL=convTranspose.js.map","import { UnaryOperation } from './unaryOperation';\nexport class SigmoidOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `1.0/(1.0 + exp(-${input}))`;\n    }\n}\n//# sourceMappingURL=sigmoid.js.map","import { UnaryOperation } from './unaryOperation';\nexport class AddMultiplyScalarOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `factor*${input} + add`;\n    }\n    calc(input) {\n        return this.compute(input.input.shape, { X: input.input }, { factor: input.factor, add: input.add });\n    }\n    getCompilationInfo(input) {\n        const info = super.getCompilationInfo(input);\n        return Object.assign(Object.assign({}, info), { factor: input.factor, add: input.add });\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.factor}-${input.add}`;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('factor')} float factor;\n    ${this.getVarModifier('add')} float add;\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'factor', type: 'float' },\n            { name: 'add', type: 'float' },\n        ];\n    }\n}\n//# sourceMappingURL=addMultiplyScalar.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { getSize } from '../../../util/shape';\nimport { Operation } from '../operation';\nexport class SetValuesOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('starts')} int starts[${this.maxRank}];\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'starts', length: this.maxRank }];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int valueIx[${this.maxRank}];\n      ${this.initIndex('valueIx')}\n\n      int inValues = 1;\n\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (index[i] == -1) {\n          break;\n        }\n\n        if (index[i] < starts[i] || index[i] >= (starts[i] + shapeValues[i])) {\n          inValues = 0;\n          break;\n        } else {\n          valueIx[i] = index[i] - starts[i];\n        }\n      }\n\n      if (inValues == 1) {\n        return _Values(valueIx);\n      } else {\n        return _A(index);\n      }\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['A', 'Values'];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { A: input.A, Values: input.Values });\n        }\n        const outputShape = this.getOutputShape(input);\n        return this.compute(outputShape, { A: input.A, Values: input.Values }, { starts: this.pad(input.starts) });\n    }\n    getOutputShape(input) {\n        return input.A.shape;\n    }\n    compile(info) {\n        if (info.shapeA !== undefined) {\n            this.maxRank = info.shapeA.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeValues: input.Values.shape,\n            widthValues: input.Values.memory.width,\n            heightValues: input.Values.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            starts: input.starts,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.Values.shape}-${input.starts}`;\n    }\n}\n//# sourceMappingURL=setValues.js.map","import { UnaryOperation } from './unaryOperation';\nexport class SinOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `sin(${input})`;\n    }\n}\nexport class ASinOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `asin(${input})`;\n    }\n}\nexport class SinHOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `(exp(${input}) - exp(-${input}))/2.0`;\n    }\n}\n//# sourceMappingURL=sin.js.map","import { UnaryOperation } from './unaryOperation';\nexport class CosOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `cos(${input})`;\n    }\n}\nexport class ACosOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `acos(${input})`;\n    }\n}\nexport class CosHOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `(1.0 + exp(-2.0*${input}))/(2.0*exp(-${input}))`;\n    }\n}\n//# sourceMappingURL=cos.js.map","import { UnaryOperation } from './unaryOperation';\nexport class TanOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `tan(${input})`;\n    }\n}\nexport class ATanOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `atan(${input})`;\n    }\n}\nexport class TanHOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `(exp(2.0*${input}) - 1.0)/(exp(2.0*${input}) + 1.0)`;\n    }\n}\n//# sourceMappingURL=tan.js.map","import { PoolOperation } from './pool';\nexport class ReduceLogSumOperation extends PoolOperation {\n    update(a, b) {\n        return `${a} + ${b}`;\n    }\n    post(res) {\n        return `${res} = log(${res});`;\n    }\n    init(res) {\n        return `${res}`;\n    }\n}\n//# sourceMappingURL=reduceLogSum.js.map","import { PoolOperation } from './pool';\nexport class ReduceLogSumExpOperation extends PoolOperation {\n    update(a, b) {\n        return `exp(${a}) + ${b}`;\n    }\n    post(res) {\n        return `${res} = log(${res});`;\n    }\n    init(res) {\n        return `exp(${res})`;\n    }\n}\n//# sourceMappingURL=reduceLogSumExp.js.map","import { UnaryOperation } from './unaryOperation';\nexport class HardSigmoidOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `max(vec4(0.0,0.0,0.0,0.0), min(vec4(1.0,1.0,1.0,1.0), alpha*${input} + beta))`;\n    }\n    calc(input) {\n        return this.compute(input.input.shape, { X: input.input }, { alpha: input.alpha, beta: input.beta });\n    }\n    getCompilationInfo(input) {\n        const info = super.getCompilationInfo(input);\n        return Object.assign(Object.assign({}, info), { alpha: input.alpha, beta: input.beta });\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.alpha}-${input.beta}`;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('alpha')} float alpha;\n    ${this.getVarModifier('beta')} float beta;\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'alpha', type: 'float' },\n            { name: 'beta', type: 'float' },\n        ];\n    }\n}\n//# sourceMappingURL=hardSigmoid.js.map","import { UnaryOperation } from './unaryOperation';\nexport class PowerScalarOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        throw new Error('Method not implemented.');\n    }\n    getFragmentShader(info) {\n        return `\n    void main() {\n      initVars();\n\n      if (power < 0.0) {\n        gl_FragColor = vec4(factor,factor,factor,factor) / pow(texture2D(X, uv), vec4(-power,-power,-power,-power));\n      } else {\n        gl_FragColor = pow(texture2D(X, uv), vec4(power,power,power,power)) * factor;\n      }\n    }\n    `;\n    }\n    calc(input) {\n        return this.compute(input.input.shape, { X: input.input }, { factor: input.factor, power: input.power });\n    }\n    getCompilationInfo(input) {\n        const info = super.getCompilationInfo(input);\n        return Object.assign(Object.assign({}, info), { factor: input.factor, power: input.power });\n    }\n    getInputInfoString(input) {\n        return `${super.getInputInfoString(input)}-${input.factor}-${input.power}`;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('factor')} float factor;\n    ${this.getVarModifier('power')} float power;\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'factor', type: 'float' },\n            { name: 'power', type: 'float' },\n        ];\n    }\n}\n//# sourceMappingURL=powerScalar.js.map","import { UnaryOperation } from './unaryOperation';\nexport class RoundOperation extends UnaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    operation(input) {\n        return `floor(${input}+vec4(0.5))`;\n    }\n}\n//# sourceMappingURL=round.js.map","import Tensor, { tensorValuesConstructor, } from '../../types';\nimport { compareShapes, getSize } from '../../util/shape';\nimport { MatMulOperation } from '../../ops/gpu/matMul/matmul';\nimport { defaultAllocator, gl } from './gl';\nimport { ExpOperation } from '../../ops/gpu/unary/exp';\nimport { ConvBiasOperation, ConvOperation } from '../../ops/gpu/conv/conv';\nimport { AbsOperation } from '../../ops/gpu/unary/abs';\nimport { AddOperation } from '../../ops/gpu/binary/add';\nimport { MultiplyOperation } from '../../ops/gpu/binary/multiply';\nimport { SubtractOperation } from '../../ops/gpu/binary/subtract';\nimport { DivideOperation } from '../../ops/gpu/binary/divide';\nimport { AveragePoolOperation } from '../../ops/gpu/conv/averagePool';\nimport { ReduceMeanOperation } from '../../ops/gpu/pool/reduceMean';\nimport { ReduceMeanSquareOperation } from '../../ops/gpu/pool/reduceMeanSquare';\nimport { SumSquareOperation } from '../../ops/gpu/pool/sumSquare';\nimport { SumOperation } from '../../ops/gpu/pool/sum';\nimport { ProductOperation } from '../../ops/gpu/pool/product';\nimport { MaxOperation } from '../../ops/gpu/pool/max';\nimport { MinOperation } from '../../ops/gpu/pool/min';\nimport { CeilOperation } from '../../ops/gpu/unary/ceil';\nimport { ClipOperation } from '../../ops/gpu/unary/clip';\nimport { FloorOperation } from '../../ops/gpu/unary/floor';\nimport { ConcatOperation } from '../../ops/gpu/util/concat';\nimport { CopyOperation } from '../../ops/gpu/util/copy';\nimport { ExpandOperation } from '../../ops/gpu/util/expand';\nimport { GatherOperation } from '../../ops/gpu/util/gather';\nimport { GemmCOperation, GemmOperation } from '../../ops/gpu/matMul/gemm';\nimport { PowerOperation } from '../../ops/gpu/binary/power';\nimport { SqrtOperation } from '../../ops/gpu/unary/sqrt';\nimport { LogOperation } from '../../ops/gpu/unary/log';\nimport { TransposeOperation } from '../../ops/gpu/util/transpose';\nimport { RepeatOperation } from '../../ops/gpu/util/repeat';\nimport { PadOperation } from '../../ops/gpu/conv/pad';\nimport { SliceOperation } from '../../ops/gpu/util/slice';\nimport { UpsampleOperation } from '../../ops/gpu/conv/upsample';\nimport { NormalizeOperation } from '../../ops/gpu/conv/normalize';\nimport { Dispatcher } from '../../ops/gpu/dispatcher';\nimport { SignOperation } from '../../ops/gpu/unary/sign';\nimport { NegateOperation } from '../../ops/gpu/unary/negate';\nimport { ClipBackwardOperation } from '../../ops/gpu/util/clipBackward';\nimport { ConvTransposeOperation } from '../../ops/gpu/conv/convTranspose';\nimport { SigmoidOperation } from '../../ops/gpu/unary/sigmoid';\nimport { AddMultiplyScalarOperation } from '../../ops/gpu/unary/addMultiplyScalar';\nimport { SetValuesOperation } from '../../ops/gpu/util/setValues';\nimport { ASinOperation, SinHOperation, SinOperation, } from '../../ops/gpu/unary/sin';\nimport { ACosOperation, CosHOperation, CosOperation, } from '../../ops/gpu/unary/cos';\nimport { ATanOperation, TanHOperation, TanOperation, } from '../../ops/gpu/unary/tan';\nimport { ReduceLogSumOperation } from '../../ops/gpu/pool/reduceLogSum';\nimport { ReduceLogSumExpOperation } from '../../ops/gpu/pool/reduceLogSumExp';\nimport { HardSigmoidOperation } from '../../ops/gpu/unary/hardSigmoid';\nimport { PowerScalarOperation } from '../../ops/gpu/unary/powerScalar';\nimport { RoundOperation } from '../../ops/gpu/unary/round';\nexport class GPUTensor extends Tensor {\n    constructor(values, shape, dtype) {\n        super(dtype || 'float32');\n        this.shape = shape;\n        this.deleted = false;\n        this.size = getSize(shape);\n        if (values instanceof Array) {\n            this.memory = defaultAllocator.allocateTexture(values, this.dtype);\n        }\n        else {\n            this.memory = values;\n        }\n    }\n    static range(start, limit, delta, dtype) {\n        const size = Math.max(Math.ceil((limit - start) / delta), 0);\n        const values = new Array(size);\n        for (let i = 0; i < size; i++) {\n            values[i] = start + i * delta;\n        }\n        return new GPUTensor(values, [size], dtype);\n    }\n    static fromData(data) {\n        const texture = gl.texture({\n            data: data,\n            format: 'rgba',\n            type: defaultAllocator.getColorType('float32'),\n        });\n        const memory = defaultAllocator.allocateFramebuffer(texture, 'float32');\n        const width = texture.width;\n        const height = texture.height;\n        return new GPUTensor(memory, [height, width, 4], 'float32');\n    }\n    cast(dtype) {\n        if (dtype === 'float64') {\n            throw new Error('The WebGL backend does not support float64 tensors');\n        }\n        return defaultCopyD.calc({ input: this }, dtype);\n    }\n    getValues() {\n        if (this.dtype !== 'float16') {\n            return new Promise(resolve => {\n                gl({ framebuffer: this.memory.frameBuffer })(() => {\n                    let result = new Float32Array(this.memory.size);\n                    result = gl.read(result);\n                    if (this.dtype === 'float32') {\n                        resolve(result.subarray(0, this.size));\n                    }\n                    else {\n                        const arr = new tensorValuesConstructor[this.dtype](this.size);\n                        for (let i = 0; i < this.size; i++) {\n                            arr[i] = result[i];\n                        }\n                        resolve(arr);\n                    }\n                });\n            });\n        }\n        throw new Error('Reading values not supported for data type float16');\n    }\n    getShape() {\n        return this.shape;\n    }\n    constantLike(value) {\n        return new GPUTensor(new Array(this.size).fill(value), this.shape, this.dtype);\n    }\n    singleConstant(value) {\n        return new GPUTensor([value], [1], this.dtype);\n    }\n    delete() {\n        if (!this.deleted) {\n            this.deleted = true;\n            defaultAllocator.deallocate(this.memory);\n            //@ts-ignore\n            this.memory = undefined;\n        }\n    }\n    copy() {\n        return defaultCopyD.calc({ input: this }, this.dtype);\n    }\n    exp() {\n        return defaultExpD.calc({ input: this }, this.dtype);\n    }\n    log() {\n        return defaultLogD.calc({ input: this }, this.dtype);\n    }\n    sqrt() {\n        return defaultSqrtD.calc({ input: this }, this.dtype);\n    }\n    abs() {\n        return defaultAbsD.calc({ input: this }, this.dtype);\n    }\n    sin() {\n        return defaultSinD.calc({ input: this }, this.dtype);\n    }\n    cos() {\n        return defaultCosD.calc({ input: this }, this.dtype);\n    }\n    tan() {\n        return defaultTanD.calc({ input: this }, this.dtype);\n    }\n    asin() {\n        return defaultASinD.calc({ input: this }, this.dtype);\n    }\n    acos() {\n        return defaultACosD.calc({ input: this }, this.dtype);\n    }\n    atan() {\n        return defaultATanD.calc({ input: this }, this.dtype);\n    }\n    sinh() {\n        return defaultSinHD.calc({ input: this }, this.dtype);\n    }\n    cosh() {\n        return defaultCosHD.calc({ input: this }, this.dtype);\n    }\n    tanh() {\n        return defaultTanHD.calc({ input: this }, this.dtype);\n    }\n    asinh() {\n        throw new Error('Method not implemented');\n    }\n    acosh() {\n        throw new Error('Method not implemented');\n    }\n    atanh() {\n        throw new Error('Method not implemented');\n    }\n    sigmoid() {\n        return defaultSigmoidD.calc({ input: this }, this.dtype);\n    }\n    hardSigmoid(alpha, beta) {\n        return defaultHardSigmoidD.calc({ input: this, alpha, beta }, this.dtype);\n    }\n    floor() {\n        return defaultFloorD.calc({ input: this }, this.dtype);\n    }\n    ceil() {\n        return defaultCeilD.calc({ input: this }, this.dtype);\n    }\n    round() {\n        return defaultRoundD.calc({ input: this }, this.dtype);\n    }\n    negate() {\n        return defaultNegateD.calc({ input: this }, this.dtype);\n    }\n    addMultiplyScalar(factor, add) {\n        return defaultAddMultiplyScalarD.calc({ input: this, factor, add }, this.dtype);\n    }\n    powerScalar(power, factor) {\n        return defaultPowerScalarD.calc({ input: this, factor, power }, this.dtype);\n    }\n    sign() {\n        return defaultSignD.calc({ input: this }, this.dtype);\n    }\n    setValues(values, starts) {\n        if (!(values instanceof GPUTensor)) {\n            throw new Error('Can only set GPU values to GPU tensor');\n        }\n        return defaultSetValuesD.calc({ A: this, Values: values, starts }, this.dtype);\n    }\n    add_impl(th, tensor, resultShape, alpha, beta) {\n        if (!(tensor instanceof GPUTensor) || !(th instanceof GPUTensor)) {\n            throw new Error('Can only add GPU tensor to GPU tensor');\n        }\n        return defaultAddD.calc({ A: th, B: tensor, outputShape: resultShape, alpha, beta }, this.dtype);\n    }\n    subtract_impl(th, tensor, resultShape, alpha, beta) {\n        if (!(tensor instanceof GPUTensor) || !(th instanceof GPUTensor)) {\n            throw new Error('Can only subtract GPU tensor from GPU tensor');\n        }\n        return defaultSubtractD.calc({ A: th, B: tensor, outputShape: resultShape, alpha, beta }, this.dtype);\n    }\n    multiply_impl(th, tensor, resultShape, alpha) {\n        if (!(tensor instanceof GPUTensor) || !(th instanceof GPUTensor)) {\n            throw new Error('Can only multiply GPU tensor with GPU tensor');\n        }\n        return defaultMultiplyD.calc({ A: th, B: tensor, outputShape: resultShape, alpha }, this.dtype);\n    }\n    divide_impl(th, tensor, resultShape, alpha) {\n        if (!(tensor instanceof GPUTensor) || !(th instanceof GPUTensor)) {\n            throw new Error('Can only divide GPU tensor by GPU tensor');\n        }\n        return defaultDivideD.calc({ A: th, B: tensor, outputShape: resultShape, alpha }, this.dtype);\n    }\n    power_impl(th, tensor, resultShape) {\n        if (!(tensor instanceof GPUTensor) || !(th instanceof GPUTensor)) {\n            throw new Error('Can only take GPU tensor to power of GPU tensor');\n        }\n        return defaultPowerD.calc({ A: th, B: tensor, outputShape: resultShape }, this.dtype);\n    }\n    matMul(tensor) {\n        if (!(tensor instanceof GPUTensor)) {\n            throw new Error('Can only matrix multiply GPU tensor to GPU tensor');\n        }\n        return defaultMatMulD.calc({ A: this, B: tensor }, this.dtype);\n    }\n    gemm_impl(b, aTranspose, bTranspose, alpha, beta, c) {\n        if (!(b instanceof GPUTensor && (c === undefined || c instanceof GPUTensor))) {\n            throw new Error('Can only do gemm with CPU tensors');\n        }\n        if (c === undefined) {\n            return defaultGemmD.calc({ a: this, b, aTranspose, bTranspose, alpha, beta }, this.dtype);\n        }\n        else {\n            return defaultGemmCD.calc({\n                a: this,\n                b,\n                c: c,\n                aTranspose,\n                bTranspose,\n                alpha,\n                beta,\n            }, this.dtype);\n        }\n    }\n    sum_impl(axes, keepDims) {\n        return defaultSumD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    sumSquare_impl(axes, keepDims) {\n        return defaultSumSquareD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    reduceMean_impl(axes, keepDims) {\n        return defaultMeanD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    reduceMeanSquare_impl(axes, keepDims) {\n        return defaultMeanSquareD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    reduceLogSum_impl(axes, keepDims) {\n        return defaultLogSumD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    reduceLogSumExp_impl(axes, keepDims) {\n        return defaultLogSumExpD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    product_impl(axes, keepDims) {\n        return defaultProductD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    max_impl(axes, keepDims) {\n        return defaultMaxD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    min_impl(axes, keepDims) {\n        return defaultMinD.calc({ X: this, axes, keepDims }, this.dtype);\n    }\n    conv_impl(kernel, dilations, group, pads, strides, activation, bias) {\n        if (!(kernel instanceof GPUTensor) ||\n            (bias !== undefined && !(bias instanceof GPUTensor))) {\n            throw new Error('Can only do convolution of GPU tensor with GPU tensor');\n        }\n        if (bias === undefined) {\n            return defaultConvD.calc({\n                X: this,\n                W: kernel,\n                pads,\n                dilations,\n                strides,\n                activation,\n            }, this.dtype);\n        }\n        else {\n            return defaultConvBiasD.calc({\n                X: this,\n                W: kernel,\n                B: bias,\n                pads,\n                dilations,\n                strides,\n                activation,\n            }, this.dtype);\n        }\n    }\n    convTranspose_impl(kernel, dilations, group, pads, strides) {\n        if (!(kernel instanceof GPUTensor)) {\n            throw new Error('Can only do transpose convolution of GPU tensor with GPU tensor');\n        }\n        return defaultConvTransposeD.calc({\n            X: this,\n            W: kernel,\n            pads,\n            dilations,\n            strides,\n        }, this.dtype);\n    }\n    averagePool_impl(kernelShape, pads, strides, includePad) {\n        return defaultAveragePoolD.calc({\n            X: this,\n            includePad,\n            kernelShape,\n            pads,\n            strides,\n        }, this.dtype);\n    }\n    reshape_impl(shape, _copy) {\n        if (_copy) {\n            return defaultCopyD.calc({ input: this, outputShape: shape }, this.dtype);\n        }\n        else {\n            return new GPUTensor(this.memory, shape, this.dtype);\n        }\n    }\n    concat(tensor, axis) {\n        if (!(tensor instanceof GPUTensor)) {\n            throw new Error('Can only concat GPU tensor to GPU tensor');\n        }\n        if (axis < 0) {\n            axis += this.shape.length;\n        }\n        return defaultConcatD.calc({ A: this, B: tensor, axis }, this.dtype);\n    }\n    transpose_impl(permutation) {\n        return defaultTransposeD.calc({ A: this, permutation }, this.dtype);\n    }\n    clip(min, max) {\n        return defaultClipD.calc({ input: this, minVal: min, maxVal: max }, this.dtype);\n    }\n    clipBackward(grad, min, max) {\n        return defaultClipBackwardD.calc({ input: this, minVal: min, maxVal: max, grad }, this.dtype);\n    }\n    repeat(repeats) {\n        return defaultRepeatD.calc({ A: this, repeats }, this.dtype);\n    }\n    expand(shape) {\n        // eslint-disable-next-line @typescript-eslint/no-unused-vars\n        const [_shape, _o, resultShape] = this.alignShapes(this.shape, shape);\n        if (compareShapes(this.shape, resultShape)) {\n            return this.copy();\n        }\n        return defaultExpandD.calc({\n            input: this.reshape(_shape, false),\n            outputShape: resultShape,\n        }, this.dtype);\n    }\n    pad_impl(pads, mode, value) {\n        return defaultPadD.calc({ input: this, pads, mode, value }, this.dtype);\n    }\n    gather(axis, indices) {\n        return defaultGatherD.calc({ X: this, axis, indices }, this.dtype);\n    }\n    slice_impl(starts, ends, axes, steps) {\n        return defaultSliceD.calc({ X: this, starts, ends, axes, steps }, this.dtype);\n    }\n    upsample(scales) {\n        return defaultUpsampleD.calc({ X: this, scales }, this.dtype);\n    }\n    normalize(mean, variance, epsilon, scale, bias) {\n        if (!(mean instanceof GPUTensor) ||\n            !(variance instanceof GPUTensor) ||\n            !(scale instanceof GPUTensor) ||\n            !(bias instanceof GPUTensor)) {\n            throw new Error('Can only normalize with CPU tensors');\n        }\n        return defaultNormalizeD.calc({\n            X: this,\n            Mean: mean,\n            Variance: variance,\n            Scale: scale,\n            Bias: bias,\n            epsilon,\n        }, this.dtype);\n    }\n}\nexport function gpuConstructor(a, b, dtype) {\n    return new GPUTensor(a, b, dtype);\n}\nconst defaultMatMulD = new Dispatcher((dtype) => new MatMulOperation(gpuConstructor, dtype));\nconst defaultGemmD = new Dispatcher((dtype) => new GemmOperation(gpuConstructor, dtype));\nconst defaultGemmCD = new Dispatcher((dtype) => new GemmCOperation(gpuConstructor, dtype));\n//Unary operations\nconst defaultExpD = new Dispatcher((dtype) => new ExpOperation(gpuConstructor, dtype));\nconst defaultAbsD = new Dispatcher((dtype) => new AbsOperation(gpuConstructor, dtype));\nconst defaultSinD = new Dispatcher((dtype) => new SinOperation(gpuConstructor, dtype));\nconst defaultCosD = new Dispatcher((dtype) => new CosOperation(gpuConstructor, dtype));\nconst defaultTanD = new Dispatcher((dtype) => new TanOperation(gpuConstructor, dtype));\nconst defaultASinD = new Dispatcher((dtype) => new ASinOperation(gpuConstructor, dtype));\nconst defaultACosD = new Dispatcher((dtype) => new ACosOperation(gpuConstructor, dtype));\nconst defaultATanD = new Dispatcher((dtype) => new ATanOperation(gpuConstructor, dtype));\nconst defaultSinHD = new Dispatcher((dtype) => new SinHOperation(gpuConstructor, dtype));\nconst defaultCosHD = new Dispatcher((dtype) => new CosHOperation(gpuConstructor, dtype));\nconst defaultTanHD = new Dispatcher((dtype) => new TanHOperation(gpuConstructor, dtype));\nconst defaultSigmoidD = new Dispatcher((dtype) => new SigmoidOperation(gpuConstructor, dtype));\nconst defaultHardSigmoidD = new Dispatcher((dtype) => new HardSigmoidOperation(gpuConstructor, dtype));\nconst defaultCeilD = new Dispatcher((dtype) => new CeilOperation(gpuConstructor, dtype));\nconst defaultFloorD = new Dispatcher((dtype) => new FloorOperation(gpuConstructor, dtype));\nconst defaultRoundD = new Dispatcher((dtype) => new RoundOperation(gpuConstructor, dtype));\nconst defaultClipD = new Dispatcher((dtype) => new ClipOperation(gpuConstructor, dtype));\nconst defaultClipBackwardD = new Dispatcher((dtype) => new ClipBackwardOperation(gpuConstructor, dtype));\nconst defaultSqrtD = new Dispatcher((dtype) => new SqrtOperation(gpuConstructor, dtype));\nconst defaultLogD = new Dispatcher((dtype) => new LogOperation(gpuConstructor, dtype));\nconst defaultNegateD = new Dispatcher((dtype) => new NegateOperation(gpuConstructor, dtype));\nconst defaultAddMultiplyScalarD = new Dispatcher((dtype) => new AddMultiplyScalarOperation(gpuConstructor, dtype));\nconst defaultPowerScalarD = new Dispatcher((dtype) => new PowerScalarOperation(gpuConstructor, dtype));\nconst defaultSignD = new Dispatcher((dtype) => new SignOperation(gpuConstructor, dtype));\n//Convolutions\nconst defaultConvD = new Dispatcher((dtype) => new ConvOperation(gpuConstructor, dtype));\nconst defaultAveragePoolD = new Dispatcher((dtype) => new AveragePoolOperation(gpuConstructor, dtype));\nconst defaultConvBiasD = new Dispatcher((dtype) => new ConvBiasOperation(gpuConstructor, dtype));\nconst defaultConvTransposeD = new Dispatcher((dtype) => new ConvTransposeOperation(gpuConstructor, dtype));\nconst defaultPadD = new Dispatcher((dtype) => new PadOperation(gpuConstructor, dtype));\nconst defaultUpsampleD = new Dispatcher((dtype) => new UpsampleOperation(gpuConstructor, dtype));\n//Binary operations\nconst defaultAddD = new Dispatcher((dtype) => new AddOperation(gpuConstructor, dtype));\nconst defaultSubtractD = new Dispatcher((dtype) => new SubtractOperation(gpuConstructor, dtype));\nconst defaultMultiplyD = new Dispatcher((dtype) => new MultiplyOperation(gpuConstructor, dtype));\nconst defaultDivideD = new Dispatcher((dtype) => new DivideOperation(gpuConstructor, dtype));\nconst defaultPowerD = new Dispatcher((dtype) => new PowerOperation(gpuConstructor, dtype));\n//Reductions\nconst defaultMeanD = new Dispatcher((dtype) => new ReduceMeanOperation(gpuConstructor, dtype));\nconst defaultMeanSquareD = new Dispatcher((dtype) => new ReduceMeanSquareOperation(gpuConstructor, dtype));\nconst defaultSumSquareD = new Dispatcher((dtype) => new SumSquareOperation(gpuConstructor, dtype));\nconst defaultSumD = new Dispatcher((dtype) => new SumOperation(gpuConstructor, dtype));\nconst defaultProductD = new Dispatcher((dtype) => new ProductOperation(gpuConstructor, dtype));\nconst defaultMaxD = new Dispatcher((dtype) => new MaxOperation(gpuConstructor, dtype));\nconst defaultMinD = new Dispatcher((dtype) => new MinOperation(gpuConstructor, dtype));\nconst defaultLogSumD = new Dispatcher((dtype) => new ReduceLogSumOperation(gpuConstructor, dtype));\nconst defaultLogSumExpD = new Dispatcher((dtype) => new ReduceLogSumExpOperation(gpuConstructor, dtype));\n//Util\nconst defaultConcatD = new Dispatcher((dtype) => new ConcatOperation(gpuConstructor, dtype));\nconst defaultSetValuesD = new Dispatcher((dtype) => new SetValuesOperation(gpuConstructor, dtype));\nconst defaultCopyD = new Dispatcher((dtype) => new CopyOperation(gpuConstructor, dtype));\nconst defaultExpandD = new Dispatcher((dtype) => new ExpandOperation(gpuConstructor, dtype));\nconst defaultGatherD = new Dispatcher((dtype) => new GatherOperation(gpuConstructor, dtype));\nconst defaultTransposeD = new Dispatcher((dtype) => new TransposeOperation(gpuConstructor, dtype));\nconst defaultRepeatD = new Dispatcher((dtype) => new RepeatOperation(gpuConstructor, dtype));\nconst defaultSliceD = new Dispatcher((dtype) => new SliceOperation(gpuConstructor, dtype));\nconst defaultNormalizeD = new Dispatcher((dtype) => new NormalizeOperation(gpuConstructor, dtype));\n//# sourceMappingURL=tensor.js.map","import Tensor, { tensorValuesConstructor, } from '../../types';\nimport { compareShapes } from '../../util/shape';\nlet WASMTF32;\nlet WASMTF64;\nlet WASMTI32;\nlet WASMTI16;\nlet WASMTI8;\nlet WASMTU32;\nlet WASMTU16;\nlet WASMTU8;\nexport let tensorConstructor;\nexport const wasmLoaded = new Promise(resolve => {\n    import('../../wasm/rust_wasm_tensor').then(x => {\n        WASMTF32 = x.TensorF32;\n        WASMTF64 = x.TensorF64;\n        WASMTI32 = x.TensorI32;\n        WASMTI16 = x.TensorI16;\n        WASMTI8 = x.TensorI8;\n        WASMTU32 = x.TensorU32;\n        WASMTU16 = x.TensorU16;\n        WASMTU8 = x.TensorU8;\n        tensorConstructor = {\n            float64: WASMTF64,\n            float32: WASMTF32,\n            int32: WASMTI32,\n            int16: WASMTI16,\n            int8: WASMTI8,\n            uint32: WASMTU32,\n            uint16: WASMTU16,\n            uint8: WASMTU8,\n        };\n        resolve();\n    });\n});\nexport class WASMTensor extends Tensor {\n    constructor(values, shape, dtype) {\n        super(dtype || 'float32');\n        if (values instanceof Array) {\n            if (shape === undefined) {\n                throw new Error('Need the shape when creating a Wasm tensor from values');\n            }\n            const array = new tensorValuesConstructor[this.dtype](values);\n            this.wasmTensor = tensorConstructor[this.dtype].create(shape, array);\n        }\n        else {\n            this.wasmTensor = values;\n        }\n    }\n    static range(start, limit, delta) {\n        const size = Math.max(Math.ceil((limit - start) / delta), 0);\n        const values = new Array(size);\n        for (let i = 0; i < size; i++) {\n            values[i] = start + i * delta;\n        }\n        return new WASMTensor(values, new Uint32Array([size]));\n    }\n    cast(dtype) {\n        throw new Error('Method not implemented.');\n    }\n    getValues() {\n        return Promise.resolve(this.wasmTensor.get_vals());\n    }\n    getShape() {\n        return Array.from(this.wasmTensor.get_shape());\n    }\n    constantLike(value) {\n        // TODO: Maybe more efficient in WASM?\n        return new WASMTensor([value], this.wasmTensor.get_shape(), this.dtype);\n    }\n    singleConstant(value) {\n        return new WASMTensor([value], new Uint32Array([1]), this.dtype);\n    }\n    delete() {\n        if (this.wasmTensor !== undefined) {\n            this.wasmTensor.free();\n            //@ts-ignore\n            this.wasmTensor = undefined;\n        }\n    }\n    copy() {\n        return new WASMTensor(this.wasmTensor.copy(), undefined, this.dtype);\n    }\n    exp() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Exp can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.exp());\n    }\n    log() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Log can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.log());\n    }\n    sqrt() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Sqrt can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.sqrt());\n    }\n    abs() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32) &&\n            !(this.wasmTensor instanceof WASMTI32) &&\n            !(this.wasmTensor instanceof WASMTI16) &&\n            !(this.wasmTensor instanceof WASMTI8)) {\n            throw new Error('Abs can only be called on signed tensors');\n        }\n        return new WASMTensor(this.wasmTensor.abs());\n    }\n    sin() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Sin can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.sin());\n    }\n    cos() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Cos can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.cos());\n    }\n    tan() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Tan can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.tan());\n    }\n    asin() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Asin can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.asin());\n    }\n    acos() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Acos can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.acos());\n    }\n    atan() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Atan can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.atan());\n    }\n    sinh() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Sinh can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.sinh());\n    }\n    cosh() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Cosh can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.cosh());\n    }\n    tanh() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Tanh can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.tanh());\n    }\n    asinh() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Asinh can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.asinh());\n    }\n    acosh() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Acosh can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.acosh());\n    }\n    atanh() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Atanh can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.atanh());\n    }\n    sigmoid() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Sigmoid can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.sigmoid());\n    }\n    hardSigmoid(alpha, beta) {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('HardSigmoid can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.hard_sigmoid(alpha, beta));\n    }\n    negate() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32) &&\n            !(this.wasmTensor instanceof WASMTI32) &&\n            !(this.wasmTensor instanceof WASMTI16) &&\n            !(this.wasmTensor instanceof WASMTI8)) {\n            throw new Error('Negate can only be called on signed tensors');\n        }\n        return new WASMTensor(this.wasmTensor.negate());\n    }\n    powerScalar(power, factor) {\n        return new WASMTensor(this.wasmTensor.power_scalar(power, factor));\n    }\n    addMultiplyScalar(factor, add) {\n        return new WASMTensor(this.wasmTensor.add_multiply_scalar(factor, add));\n    }\n    sign() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32) &&\n            !(this.wasmTensor instanceof WASMTI32) &&\n            !(this.wasmTensor instanceof WASMTI16) &&\n            !(this.wasmTensor instanceof WASMTI8)) {\n            throw new Error('Sign can only be called on signed tensors');\n        }\n        return new WASMTensor(this.wasmTensor.sign());\n    }\n    setValues(values, starts) {\n        if (!(values instanceof WASMTensor)) {\n            throw new Error('Can only set WASM values to WASM values');\n        }\n        return new WASMTensor(this.wasmTensor.set_values(values.wasmTensor, new Uint32Array(starts)));\n    }\n    add_impl(th, tensor, \n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    _resultShape, alpha, beta) {\n        if (!(tensor instanceof WASMTensor) || !(th instanceof WASMTensor)) {\n            throw new Error('Can only add WASM tensor to WASM tensor');\n        }\n        return new WASMTensor(th.wasmTensor.addition(tensor.wasmTensor, alpha, beta));\n    }\n    subtract_impl(th, tensor, \n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    resultShape, alpha, beta) {\n        if (!(tensor instanceof WASMTensor) || !(th instanceof WASMTensor)) {\n            throw new Error('Can only subtract WASM tensor from WASM tensor');\n        }\n        return new WASMTensor(th.wasmTensor.subtraction(tensor.wasmTensor, alpha, beta));\n    }\n    multiply_impl(th, tensor, \n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    resultShape, alpha) {\n        if (!(tensor instanceof WASMTensor) || !(th instanceof WASMTensor)) {\n            throw new Error('Can only multiply WASM tensor with WASM tensor');\n        }\n        return new WASMTensor(th.wasmTensor.multiply(tensor.wasmTensor, alpha));\n    }\n    divide_impl(th, tensor, \n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    resultShape, alpha) {\n        if (!(tensor instanceof WASMTensor) || !(th instanceof WASMTensor)) {\n            throw new Error('Can only divide WASM tensor by WASM tensor');\n        }\n        return new WASMTensor(th.wasmTensor.divide(tensor.wasmTensor, alpha));\n    }\n    power_impl(th, tensor, \n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    resultShape) {\n        if (!(tensor instanceof WASMTensor) || !(th instanceof WASMTensor)) {\n            throw new Error('Can only take WASM tensor to power of WASM tensor');\n        }\n        return new WASMTensor(th.wasmTensor.power(tensor.wasmTensor));\n    }\n    matMul(tensor) {\n        if (!(tensor instanceof WASMTensor)) {\n            throw new Error('Can only add WASM tensor to WASM tensor');\n        }\n        return new WASMTensor(this.wasmTensor.matmul(tensor.wasmTensor));\n    }\n    gemm_impl(b, aTranspose, bTranspose, alpha, beta, c) {\n        if (!(b instanceof WASMTensor && (c === undefined || c instanceof WASMTensor))) {\n            throw new Error('Can only do gemm with CPU tensors');\n        }\n        if (c !== undefined) {\n            return new WASMTensor(this.wasmTensor.gemm_with_c(b.wasmTensor, aTranspose, bTranspose, alpha, c.wasmTensor, beta));\n        }\n        else {\n            return new WASMTensor(this.wasmTensor.gemm(b.wasmTensor, aTranspose, bTranspose, alpha));\n        }\n    }\n    sum_impl(axes, keepDims) {\n        return new WASMTensor(this.wasmTensor.sum(new Uint32Array(axes), keepDims));\n    }\n    sumSquare_impl(axes, keepDims) {\n        return new WASMTensor(this.wasmTensor.sum_square(new Uint32Array(axes), keepDims));\n    }\n    product_impl(axes, keepDims) {\n        return new WASMTensor(this.wasmTensor.product(new Uint32Array(axes), keepDims));\n    }\n    max_impl(axes, keepDims) {\n        return new WASMTensor(this.wasmTensor.max(new Uint32Array(axes), keepDims));\n    }\n    min_impl(axes, keepDims) {\n        return new WASMTensor(this.wasmTensor.min(new Uint32Array(axes), keepDims));\n    }\n    reduceMean_impl(axes, keepDims) {\n        return new WASMTensor(this.wasmTensor.reduce_mean(new Uint32Array(axes), keepDims));\n    }\n    reduceMeanSquare_impl(axes, keepDims) {\n        return new WASMTensor(this.wasmTensor.reduce_mean_square(new Uint32Array(axes), keepDims));\n    }\n    reduceLogSum_impl(axes, keepDims) {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('ReduceLogSum can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.reduce_log_sum(new Uint32Array(axes), keepDims));\n    }\n    reduceLogSumExp_impl(axes, keepDims) {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('ReduceLogSumExp can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.reduce_log_sum_exp(new Uint32Array(axes), keepDims));\n    }\n    getActivationFlag(activation) {\n        if (activation === 'id') {\n            return 0;\n        }\n        else if (activation === 'relu') {\n            return 1;\n        }\n        else {\n            return 2;\n        }\n    }\n    conv_impl(kernel, dilations, group, pads, strides, activation, bias) {\n        if (!(kernel instanceof WASMTensor) ||\n            (bias !== undefined && !(bias instanceof WASMTensor))) {\n            throw new Error('Can only do convolution of WASM tensor with WASM tensor');\n        }\n        const activationFlag = this.getActivationFlag(activation);\n        if (bias !== undefined) {\n            return new WASMTensor(this.wasmTensor.conv_with_bias(kernel.wasmTensor, bias.wasmTensor, new Uint32Array(dilations), group, new Uint32Array(pads), new Uint32Array(strides), activationFlag));\n        }\n        else {\n            return new WASMTensor(this.wasmTensor.conv(kernel.wasmTensor, new Uint32Array(dilations), group, new Uint32Array(pads), new Uint32Array(strides), activationFlag));\n        }\n    }\n    convTranspose_impl(kernel, dilations, group, pads, strides) {\n        if (!(kernel instanceof WASMTensor)) {\n            throw new Error('Can only do transpose convolution of WASM tensor with WASM tensor');\n        }\n        return new WASMTensor(this.wasmTensor.conv_transpose(kernel.wasmTensor, new Uint32Array(dilations), group, new Uint32Array(pads), new Uint32Array(strides)));\n    }\n    averagePool_impl(kernelShape, pads, strides, includePad) {\n        return new WASMTensor(this.wasmTensor.average_pool(new Uint32Array(kernelShape), new Uint32Array(pads), new Uint32Array(strides), includePad));\n    }\n    reshape_impl(shape) {\n        const sh = new Uint32Array(shape);\n        return new WASMTensor(this.wasmTensor.reshape(sh), sh);\n    }\n    concat(tensor, axis) {\n        if (!(tensor instanceof WASMTensor)) {\n            throw new Error('Can only concat WASM tensor to WASM tensor');\n        }\n        if (axis < 0) {\n            axis += this.getShape().length;\n        }\n        return new WASMTensor(this.wasmTensor.concat(tensor.wasmTensor, axis));\n    }\n    transpose_impl(permutation) {\n        return new WASMTensor(this.wasmTensor.transpose(new Uint32Array(permutation)));\n    }\n    clip(min, max) {\n        if (min !== undefined && max !== undefined) {\n            return new WASMTensor(this.wasmTensor.clip(min, max));\n        }\n        else if (min !== undefined) {\n            return new WASMTensor(this.wasmTensor.clip_min(min));\n        }\n        else if (max !== undefined) {\n            return new WASMTensor(this.wasmTensor.clip_max(max));\n        }\n        return this.copy();\n    }\n    clipBackward(grad, min, max) {\n        if (!(grad instanceof WASMTensor)) {\n            throw new Error('Can only do grad backward with Wasm tensor');\n        }\n        if (min !== undefined && max !== undefined) {\n            return new WASMTensor(this.wasmTensor.clip_backward(min, max, grad.wasmTensor));\n        }\n        else if (min !== undefined) {\n            return new WASMTensor(this.wasmTensor.clip_min_backward(min, grad.wasmTensor));\n        }\n        else if (max !== undefined) {\n            return new WASMTensor(this.wasmTensor.clip_max_backward(max, grad.wasmTensor));\n        }\n        return this.copy();\n    }\n    repeat(repeats) {\n        return new WASMTensor(this.wasmTensor.repeat(new Uint32Array(repeats)));\n    }\n    expand(shape) {\n        const thisShape = this.getShape();\n        // eslint-disable-next-line @typescript-eslint/no-unused-vars\n        const [_shape, goal, resultShape] = this.alignShapes(thisShape, shape);\n        if (compareShapes(thisShape, resultShape)) {\n            return this.copy();\n        }\n        const reshaped = this.reshape(_shape, false);\n        return new WASMTensor(reshaped.wasmTensor.expand(new Uint32Array(resultShape)));\n    }\n    pad_impl(pads, mode, value) {\n        return new WASMTensor(this.wasmTensor.pad(new Uint32Array(pads), WASMTensor.padModeToInt[mode], value));\n    }\n    gather(axis, indices) {\n        return new WASMTensor(this.wasmTensor.gather(axis, indices.values, new Uint32Array(indices.shape)));\n    }\n    floor() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Floor can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.floor());\n    }\n    ceil() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Ceil can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.ceil());\n    }\n    round() {\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Round can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.round());\n    }\n    slice_impl(starts, ends, axes, steps) {\n        return new WASMTensor(this.wasmTensor.slice(new Uint32Array(starts), new Uint32Array(ends), new Uint32Array(axes), new Int32Array(steps)));\n    }\n    upsample(scales) {\n        return new WASMTensor(this.wasmTensor.upsample(new Float32Array(scales)));\n    }\n    normalize(mean, variance, epsilon, scale, bias) {\n        if (!(mean instanceof WASMTensor) ||\n            !(variance instanceof WASMTensor) ||\n            !(scale instanceof WASMTensor) ||\n            !(bias instanceof WASMTensor)) {\n            throw new Error('Can only normalize with WASM tensors');\n        }\n        if (!(this.wasmTensor instanceof WASMTF64) &&\n            !(this.wasmTensor instanceof WASMTF32)) {\n            throw new Error('Normalize can only be called on float tensors');\n        }\n        return new WASMTensor(this.wasmTensor.normalize(mean.wasmTensor, variance.wasmTensor, epsilon, scale.wasmTensor, bias.wasmTensor));\n    }\n}\nWASMTensor.padModeToInt = {\n    constant: 0,\n    reflect: 1,\n    edge: 2,\n};\n//# sourceMappingURL=tensor.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { getSize, incrementIndex, indexToPos } from '../../../util/shape';\nimport { poolResultShape } from '../../util/pool';\nexport function aggregateSparseCPU(tensor, axes, keepDims, op, init, postProcess) {\n    const [resultShape, ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n    const result = new CPUTensor(resultShape, undefined, tensor.values.dtype);\n    const denseShape = tensor.getDenseShape();\n    const denseSize = getSize(denseShape, 1);\n    let count;\n    if (init !== undefined || postProcess !== undefined) {\n        count = new Array(result.size).fill(0);\n    }\n    const aIndices = tensor.indices;\n    const aValues = tensor.values;\n    for (let i = 0; i < tensor.nnz; i++) {\n        const sparseIx = new Array(tensor.sparseDims);\n        for (let j = 0; j < tensor.sparseDims; j++) {\n            sparseIx[j] = aIndices.get(i * tensor.sparseDims + j);\n        }\n        const denseIx = new Array(tensor.denseDims).fill(0);\n        for (let j = 0; j < denseSize; j++) {\n            const resultIx = [...sparseIx, ...denseIx];\n            const mappedResultIx = new Array(ixMap.length);\n            for (let k = 0; k < ixMap.length; k++) {\n                mappedResultIx[k] = resultIx[ixMap[k]];\n            }\n            const pos = indexToPos(mappedResultIx, result.strides);\n            if (init !== undefined && count !== undefined) {\n                if (count[pos] === 0) {\n                    result.set(pos, init(aValues.get(i * denseSize + j)));\n                }\n                else {\n                    result.set(pos, op(result.get(pos), aValues.get(i * denseSize + j)));\n                }\n            }\n            else {\n                result.set(pos, op(result.get(pos), aValues.get(i * denseSize + j)));\n            }\n            if (count !== undefined) {\n                count[pos]++;\n            }\n            incrementIndex(denseIx, denseShape);\n        }\n    }\n    if (postProcess !== undefined && count !== undefined) {\n        for (let i = 0; i < result.size; i++) {\n            result.set(i, postProcess(result.get(i), count[i]));\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=cpu.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { maxSparseCPU } from './cpu';\nimport { maxSparseWASM } from './wasm';\nexport function max(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return maxSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.max(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction maxSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return maxSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return maxSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Maximum over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=max.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function maxSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => Math.max(a, b), e => e);\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function maxSparseWASM(tensor, axes, keepDims) {\n    return new WASMTensor(tensor.values.wasmTensor.max_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { minSparseCPU } from './cpu';\nimport { minSparseWASM } from './wasm';\nexport function min(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return minSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.min(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction minSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return minSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return minSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Minimum over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=min.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function minSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => Math.min(a, b), e => e);\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function minSparseWASM(tensor, axes, keepDims) {\n    return new WASMTensor(tensor.values.wasmTensor.min_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { productSparseCPU } from './cpu';\nimport { productSparseWASM } from './wasm';\nexport function product(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return productSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.product(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction productSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return productSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return productSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Product over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=product.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function productSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => a * b, e => e);\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function productSparseWASM(tensor, axes, keepDims) {\n    return new WASMTensor(tensor.values.wasmTensor.product_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { reduceLogSumSparseCPU } from './cpu';\nimport { reduceLogSumSparseWASM } from './wasm';\nexport function reduceLogSum(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return reduceLogSumSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.reduceLogSum(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction reduceLogSumSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return reduceLogSumSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return reduceLogSumSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Reduce log sum over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=reduceLogSum.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function reduceLogSumSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => a + b, undefined, (e, c) => Math.log(e));\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function reduceLogSumSparseWASM(tensor, axes, keepDims) {\n    if (tensor.dtype !== 'float32' && tensor.dtype === 'float64') {\n        throw new Error('Reduce log sum expects tensor datatype to be float, but found ' +\n            tensor.dtype);\n    }\n    return new WASMTensor(tensor.values\n        .wasmTensor.reduce_log_sum_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { reduceLogSumExpSparseCPU } from './cpu';\nimport { reduceLogSumExpSparseWASM } from './wasm';\nexport function reduceLogSumExp(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return reduceLogSumExpSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.reduceLogSumExp(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction reduceLogSumExpSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return reduceLogSumExpSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return reduceLogSumExpSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Reduce log sum exp over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=reduceLogSumExp.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function reduceLogSumExpSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => a + Math.exp(b), e => Math.exp(e), (e, c) => Math.log(e));\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function reduceLogSumExpSparseWASM(tensor, axes, keepDims) {\n    if (tensor.dtype !== 'float32' && tensor.dtype === 'float64') {\n        throw new Error('Reduce log sum exp expects tensor datatype to be float, but found ' +\n            tensor.dtype);\n    }\n    return new WASMTensor(tensor.values\n        .wasmTensor.reduce_log_sum_exp_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { reduceMeanSparseCPU } from './cpu';\nimport { reduceMeanSparseWASM } from './wasm';\nexport function reduceMean(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return reduceMeanSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.reduceMean(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction reduceMeanSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return reduceMeanSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return reduceMeanSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Reduce mean over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=reduceMean.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function reduceMeanSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => a + b, undefined, (e, c) => e / c);\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function reduceMeanSparseWASM(tensor, axes, keepDims) {\n    return new WASMTensor(tensor.values.wasmTensor.reduce_mean_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { reduceMeanSquareSparseCPU } from './cpu';\nimport { reduceMeanSquareSparseWASM } from './wasm';\nexport function reduceMeanSquare(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return reduceMeanSquareSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.reduceMeanSquare(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction reduceMeanSquareSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return reduceMeanSquareSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return reduceMeanSquareSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Reduce mean squared over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=reduceMeanSquare.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function reduceMeanSquareSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => a + b * b, e => e * e, (e, c) => e / c);\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function reduceMeanSquareSparseWASM(tensor, axes, keepDims) {\n    return new WASMTensor(tensor.values.wasmTensor.reduce_mean_squared_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { sumSparseCPU } from './cpu';\nimport { sumSparseWASM } from './wasm';\nexport function sum(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return sumSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.sum(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction sumSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return sumSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return sumSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Sum over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=sum.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function sumSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => a + b);\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function sumSparseWASM(tensor, axes, keepDims) {\n    return new WASMTensor(tensor.values.wasmTensor.sum_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { poolResultShape } from '../../../util/pool';\nimport { sumSquareSparseCPU } from './cpu';\nimport { sumSquareSparseWASM } from './wasm';\nexport function sumSquare(tensor, axes, keepDims) {\n    if (axes.find(ax => ax < tensor.sparseDims) !== undefined) {\n        return sumSquareSparse(tensor, axes, keepDims);\n    }\n    else {\n        const [resultShape, _ixMap] = poolResultShape(tensor.shape, axes, keepDims);\n        return new SparseTensor(tensor.values.sumSquare(axes.map(ax => ax - tensor.sparseDims + 1), keepDims), tensor.indices.copy(), resultShape, keepDims ? tensor.denseDims : tensor.denseDims - axes.length);\n    }\n}\nfunction sumSquareSparse(tensor, axes, keepDims) {\n    if (tensor.values instanceof CPUTensor) {\n        return sumSquareSparseCPU(tensor, axes, keepDims);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return sumSquareSparseWASM(tensor, axes, keepDims);\n    }\n    throw new Error('Squared sum over sparse dimensions not implemented in WebGL yet');\n}\n//# sourceMappingURL=sumSquare.js.map","import { aggregateSparseCPU } from '../cpu';\nexport function sumSquareSparseCPU(tensor, axes, keepDims) {\n    return aggregateSparseCPU(tensor, axes, keepDims, (a, b) => a + b * b, e => e * e);\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function sumSquareSparseWASM(tensor, axes, keepDims) {\n    return new WASMTensor(tensor.values.wasmTensor.sum_square_sparse(new Uint32Array(tensor.getShape()), tensor.indices.wasmTensor, new Uint32Array(axes), keepDims), undefined, tensor.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../tensor/sparse/tensor';\nimport { computeStrides, getSize, incrementIndex } from '../../../util/shape';\nexport function binaryDenseCPU(a, b, resultShape, op) {\n    const S = a.sparseDims;\n    const sparseResultShape = resultShape.slice(0, S);\n    const denseResultShape = resultShape.slice(S);\n    const denseSize = getSize(denseResultShape, 1);\n    const valsA = a.values;\n    const indicesA = a.indices;\n    const values = new CPUTensor([a.nnz, ...denseResultShape], undefined, a.dtype);\n    const indices = a.indices.copy();\n    for (let i = 0; i < a.nnz; i++) {\n        const sparseIx = new Array(sparseResultShape.length);\n        for (let j = 0; j < sparseResultShape.length; j++) {\n            sparseIx[j] = indicesA.get(i * S + j);\n        }\n        const denseIx = new Array(denseResultShape.length).fill(0);\n        for (let j = 0; j < denseSize; j++) {\n            const vA = valsA.get([i, ...denseIx]);\n            const vB = b.get([...sparseIx, ...denseIx]);\n            values.set(i * denseSize + j, op(vA, vB));\n            incrementIndex(denseIx, denseResultShape);\n        }\n    }\n    return new SparseTensor(values, indices, resultShape, a.denseDims);\n}\nexport function binarySparseCPU(a, b, resultShape, op) {\n    const S = a.sparseDims;\n    const sparseResultShape = resultShape.slice(0, S);\n    const denseResultShape = resultShape.slice(S);\n    const denseSize = getSize(denseResultShape, 1);\n    const valsA = a.values;\n    const indicesA = a.indices;\n    const valsB = b.values;\n    const indicesB = b.indices;\n    const sparseStrides = computeStrides(sparseResultShape);\n    const bIxPositionMap = {};\n    for (let i = 0; i < b.nnz; i++) {\n        let pos = 0;\n        for (let j = 0; j < S; j++) {\n            pos += indicesB.get(i * S + j) * sparseStrides[j];\n        }\n        bIxPositionMap[pos] = i;\n    }\n    const values = new CPUTensor([a.nnz, ...denseResultShape], undefined, a.dtype);\n    const indices = a.indices.copy();\n    for (let i = 0; i < a.nnz; i++) {\n        let pos = 0;\n        for (let j = 0; j < S; j++) {\n            pos += indicesA.get(i * S + j) * sparseStrides[j];\n        }\n        const iB = bIxPositionMap[pos];\n        const denseIx = new Array(denseResultShape.length).fill(0);\n        for (let j = 0; j < denseSize; j++) {\n            const vA = valsA.get([i, ...denseIx]);\n            const vB = valsB.get([iB, ...denseIx]);\n            values.set(i * denseSize + j, op(vA, vB));\n            incrementIndex(denseIx, denseResultShape);\n        }\n    }\n    return new SparseTensor(values, indices, resultShape, a.denseDims);\n}\n//# sourceMappingURL=cpu.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { addDenseCPU, addSparseCPU } from './cpu';\nimport { addDenseWASM, addSparseWASM } from './wasm';\nexport function add(a, b, resultShape, alpha, beta) {\n    if (b instanceof SparseTensor) {\n        return addSparse(a, b, resultShape, alpha, beta);\n    }\n    else {\n        return addDense(a, b, resultShape, alpha, beta);\n    }\n}\nfunction addSparse(a, b, resultShape, alpha, beta) {\n    if (a.nnz !== b.nnz) {\n        throw new Error('Addition with two sparse tensors expects the same sparsity pattern, and thus the same number of nonzero entries in both tensors');\n    }\n    else if (a.denseDims !== b.denseDims) {\n        throw new Error('Addition with two sparse tensors expects the same number of sparse and dense dimensions in both tensors');\n    }\n    if (a.values instanceof CPUTensor) {\n        return addSparseCPU(a, b, resultShape, alpha, beta);\n    }\n    else if (a.values instanceof WASMTensor) {\n        return addSparseWASM(a, b, resultShape, alpha, beta);\n    }\n    throw new Error('Sparse-sparse matrix addition not supported on WebGL backend');\n}\nfunction addDense(a, b, resultShape, alpha, beta) {\n    if (b instanceof CPUTensor) {\n        return addDenseCPU(a, b, resultShape, alpha, beta);\n    }\n    else if (b instanceof WASMTensor) {\n        return addDenseWASM(a, b, resultShape, alpha, beta);\n    }\n    throw new Error('Sparse-dense matrix addition not supported on WebGL backend');\n}\n//# sourceMappingURL=add.js.map","import { binaryDenseCPU, binarySparseCPU } from '../cpu';\nexport function addDenseCPU(a, b, resultShape, alpha, beta) {\n    return binaryDenseCPU(a, b, resultShape, (a, b) => alpha * a + beta * b);\n}\nexport function addSparseCPU(a, b, resultShape, alpha, beta) {\n    return binarySparseCPU(a, b, resultShape, (a, b) => alpha * a + beta * b);\n}\n//# sourceMappingURL=cpu.js.map","import { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function addDenseWASM(a, b, resultShape, alpha, beta) {\n    const vals = new WASMTensor(a.values.wasmTensor.add_sparse_dense(a.indices.wasmTensor, b.wasmTensor, new Uint32Array(resultShape), alpha, beta), undefined, a.dtype);\n    return new SparseTensor(vals, a.indices.copy(), resultShape, a.denseDims);\n}\nexport function addSparseWASM(a, b, resultShape, alpha, beta) {\n    const vals = new WASMTensor(a.values.wasmTensor.add_sparse_sparse(a.indices.wasmTensor, b.indices.wasmTensor, b.values.wasmTensor, new Uint32Array(resultShape), alpha, beta), undefined, a.dtype);\n    return new SparseTensor(vals, a.indices.copy(), resultShape, a.denseDims);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { divideDenseCPU, divideSparseCPU } from './cpu';\nimport { divideDenseWASM, divideSparseWASM } from './wasm';\nexport function divide(a, b, resultShape, alpha) {\n    if (b instanceof SparseTensor) {\n        return divideSparse(a, b, resultShape, alpha);\n    }\n    else {\n        return divideDense(a, b, resultShape, alpha);\n    }\n}\nfunction divideSparse(a, b, resultShape, alpha) {\n    if (a.nnz !== b.nnz) {\n        throw new Error('Element wise division with two sparse tensors expects the same sparsity pattern, and thus the same number of nonzero entries in both tensors');\n    }\n    else if (a.denseDims !== b.denseDims) {\n        throw new Error('Element wise division with two sparse tensors expects the same number of sparse and dense dimensions in both tensors');\n    }\n    if (a.values instanceof CPUTensor) {\n        return divideSparseCPU(a, b, resultShape, alpha);\n    }\n    else if (a.values instanceof WASMTensor) {\n        return divideSparseWASM(a, b, resultShape, alpha);\n    }\n    throw new Error('Sparse-sparse matrix division not supported on WebGL backend');\n}\nfunction divideDense(a, b, resultShape, alpha) {\n    if (b instanceof CPUTensor) {\n        return divideDenseCPU(a, b, resultShape, alpha);\n    }\n    else if (b instanceof WASMTensor) {\n        return divideDenseWASM(a, b, resultShape, alpha);\n    }\n    throw new Error('Sparse-dense matrix element wise division not supported on WebGL backend');\n}\n//# sourceMappingURL=divide.js.map","import { binaryDenseCPU, binarySparseCPU } from '../cpu';\nexport function divideDenseCPU(a, b, resultShape, alpha) {\n    return binaryDenseCPU(a, b, resultShape, (a, b) => (alpha * a) / b);\n}\nexport function divideSparseCPU(a, b, resultShape, alpha) {\n    return binarySparseCPU(a, b, resultShape, (a, b) => (alpha * a) / b);\n}\n//# sourceMappingURL=cpu.js.map","import { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function divideDenseWASM(a, b, resultShape, alpha) {\n    const vals = new WASMTensor(a.values.wasmTensor.divide_sparse_dense(a.indices.wasmTensor, b.wasmTensor, new Uint32Array(resultShape), alpha), undefined, a.dtype);\n    return new SparseTensor(vals, a.indices.copy(), resultShape, a.denseDims);\n}\nexport function divideSparseWASM(a, b, resultShape, alpha) {\n    const vals = new WASMTensor(a.values.wasmTensor.divide_sparse_sparse(a.indices.wasmTensor, b.indices.wasmTensor, b.values.wasmTensor, new Uint32Array(resultShape), alpha), undefined, a.dtype);\n    return new SparseTensor(vals, a.indices.copy(), resultShape, a.denseDims);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { multiplyDenseCPU, multiplySparseCPU } from './cpu';\nimport { multiplyDenseWASM, multiplySparseWASM } from './wasm';\nexport function multiply(a, b, resultShape, alpha) {\n    if (b instanceof SparseTensor) {\n        return multiplySparse(a, b, resultShape, alpha);\n    }\n    else {\n        return multiplyDense(a, b, resultShape, alpha);\n    }\n}\nfunction multiplySparse(a, b, resultShape, alpha) {\n    if (a.nnz !== b.nnz) {\n        throw new Error('Element wise multiplication with two sparse tensors expects the same sparsity pattern, and thus the same number of nonzero entries in both tensors');\n    }\n    else if (a.denseDims !== b.denseDims) {\n        throw new Error('Element wise multiplication with two sparse tensors expects the same number of sparse and dense dimensions in both tensors');\n    }\n    if (a.values instanceof CPUTensor) {\n        return multiplySparseCPU(a, b, resultShape, alpha);\n    }\n    else if (a.values instanceof WASMTensor) {\n        return multiplySparseWASM(a, b, resultShape, alpha);\n    }\n    throw new Error('Sparse-sparse matrix addition not supported on WebGL backend');\n}\nfunction multiplyDense(a, b, resultShape, alpha) {\n    if (b instanceof CPUTensor) {\n        return multiplyDenseCPU(a, b, resultShape, alpha);\n    }\n    else if (b instanceof WASMTensor) {\n        return multiplyDenseWASM(a, b, resultShape, alpha);\n    }\n    throw new Error('Sparse-dense matrix element wise multiplication not supported on WASM/WebGL backend');\n}\n//# sourceMappingURL=multiply.js.map","import { binaryDenseCPU, binarySparseCPU } from '../cpu';\nexport function multiplyDenseCPU(a, b, resultShape, alpha) {\n    return binaryDenseCPU(a, b, resultShape, (a, b) => alpha * a * b);\n}\nexport function multiplySparseCPU(a, b, resultShape, alpha) {\n    return binarySparseCPU(a, b, resultShape, (a, b) => alpha * a * b);\n}\n//# sourceMappingURL=cpu.js.map","import { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function multiplyDenseWASM(a, b, resultShape, alpha) {\n    const vals = new WASMTensor(a.values.wasmTensor.multiply_sparse_dense(a.indices.wasmTensor, b.wasmTensor, new Uint32Array(resultShape), alpha), undefined, a.dtype);\n    return new SparseTensor(vals, a.indices.copy(), resultShape, a.denseDims);\n}\nexport function multiplySparseWASM(a, b, resultShape, alpha) {\n    const vals = new WASMTensor(a.values.wasmTensor.multiply_sparse_sparse(a.indices.wasmTensor, b.indices.wasmTensor, b.values.wasmTensor, new Uint32Array(resultShape), alpha), undefined, a.dtype);\n    return new SparseTensor(vals, a.indices.copy(), resultShape, a.denseDims);\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { subtractDenseCPU, subtractSparseCPU } from './cpu';\nimport { subtractDenseWASM, subtractSparseWASM } from './wasm';\nexport function subtract(a, b, resultShape, alpha, beta) {\n    if (b instanceof SparseTensor) {\n        return subtractSparse(a, b, resultShape, alpha, beta);\n    }\n    else {\n        return subtractDense(a, b, resultShape, alpha, beta);\n    }\n}\nfunction subtractSparse(a, b, resultShape, alpha, beta) {\n    if (a.nnz !== b.nnz) {\n        throw new Error('Subtraction with two sparse tensors expects the same sparsity pattern, and thus the same number of nonzero entries in both tensors');\n    }\n    else if (a.denseDims !== b.denseDims) {\n        throw new Error('Subtraction with two sparse tensors expects the same number of sparse and dense dimensions in both tensors');\n    }\n    if (a.values instanceof CPUTensor) {\n        return subtractSparseCPU(a, b, resultShape, alpha, beta);\n    }\n    else if (a.values instanceof WASMTensor) {\n        return subtractSparseWASM(a, b, resultShape, alpha, beta);\n    }\n    throw new Error('Sparse-sparse matrix subtraction not supported on WebGL backend');\n}\nfunction subtractDense(a, b, resultShape, alpha, beta) {\n    if (b instanceof CPUTensor) {\n        return subtractDenseCPU(a, b, resultShape, alpha, beta);\n    }\n    else if (b instanceof WASMTensor) {\n        return subtractDenseWASM(a, b, resultShape, alpha, beta);\n    }\n    throw new Error('Sparse-dense matrix addition not supported on WASM/WebGL backend');\n}\n//# sourceMappingURL=subtract.js.map","import { binaryDenseCPU, binarySparseCPU } from '../cpu';\nexport function subtractDenseCPU(a, b, resultShape, alpha, beta) {\n    return binaryDenseCPU(a, b, resultShape, (a, b) => alpha * a - beta * b);\n}\nexport function subtractSparseCPU(a, b, resultShape, alpha, beta) {\n    return binarySparseCPU(a, b, resultShape, (a, b) => alpha * a - beta * b);\n}\n//# sourceMappingURL=cpu.js.map","import { SparseTensor } from '../../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nexport function subtractDenseWASM(a, b, resultShape, alpha, beta) {\n    const vals = new WASMTensor(a.values.wasmTensor.subtract_sparse_dense(a.indices.wasmTensor, b.wasmTensor, new Uint32Array(resultShape), alpha, beta), undefined, a.dtype);\n    return new SparseTensor(vals, a.indices.copy(), resultShape, a.denseDims);\n}\nexport function subtractSparseWASM(a, b, resultShape, alpha, beta) {\n    const vals = new WASMTensor(a.values.wasmTensor.subtract_sparse_sparse(a.indices.wasmTensor, b.indices.wasmTensor, b.values.wasmTensor, new Uint32Array(resultShape), alpha, beta), undefined, a.dtype);\n    return new SparseTensor(vals, a.indices.copy(), resultShape, a.denseDims);\n}\n//# sourceMappingURL=wasm.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { gpuConstructor } from '../../../tensor/gpu/tensor';\nimport { getSize } from '../../../util/shape';\nimport { Dispatcher } from '../../gpu/dispatcher';\nimport { Operation } from '../../gpu/operation';\nexport class AddIndexOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('axis')} int axis;\n    ${this.getVarModifier('count')} int count;\n    `;\n    }\n    getUniformAttrs() {\n        return [{ name: 'axis' }, { name: 'count' }];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      float res = _A(index);\n\n      if (index[1] == axis) {\n        res += float(count);\n      }\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['A'];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { A: input.A });\n        }\n        const outputShape = this.getOutputShape(input);\n        const info = this.getCompilationInfo(input);\n        return this.compute(outputShape, { A: input.A }, {\n            axis: info.axis,\n            count: info.count,\n        });\n    }\n    getOutputShape(input) {\n        return [...input.A.shape];\n    }\n    compile(info) {\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            axis: input.axis,\n            count: input.count,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.axis}-${input.count}`;\n    }\n}\nexport const defaultAddIndexD = new Dispatcher((dtype) => new AddIndexOperation(gpuConstructor, dtype));\nexport function addIndexGPU(indices, axis, count) {\n    return defaultAddIndexD.calc({\n        A: indices,\n        axis,\n        count,\n    }, 'uint32');\n}\n//# sourceMappingURL=gpu.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../tensor/wasm/tensor';\nimport { compareShapes } from '../../../util/shape';\nimport { addIndexCPU } from './cpu';\nimport { addIndexGPU } from './gpu';\nimport { addIndexWASM } from './wasm';\nexport function concat(a, b, axis) {\n    if (!compareShapes(a.shape, b.shape) || a.sparseDims !== b.sparseDims) {\n        throw new Error('Sparse tensors can only be concatenated with the same shape and number of sparse dims');\n    }\n    if (axis > a.sparseDims) {\n        throw new Error('Concatenation along dense axis of sparse tensor not supported yet');\n    }\n    else {\n        const values = a.values.concat(b.values, 0);\n        const indexAdded = addIndex(b.indices, axis, a.shape[axis]);\n        const indices = a.indices.concat(indexAdded, 0);\n        indexAdded.delete();\n        const resultShape = [...a.shape];\n        resultShape[axis] += b.shape[axis];\n        return new SparseTensor(values, indices, resultShape, a.denseDims);\n    }\n}\nfunction addIndex(indices, axis, count) {\n    if (indices instanceof CPUTensor) {\n        return addIndexCPU(indices, axis, count);\n    }\n    else if (indices instanceof WASMTensor) {\n        return addIndexWASM(indices, axis, count);\n    }\n    else {\n        return addIndexGPU(indices, axis, count);\n    }\n}\n//# sourceMappingURL=concat.js.map","export function addIndexCPU(indices, axis, count) {\n    const result = indices.copy();\n    for (let i = axis; i < result.size; i += indices.shape[1]) {\n        result.set(i, result.get(i) + count);\n    }\n    return result;\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../tensor/wasm/tensor';\nexport function addIndexWASM(indices, axis, count) {\n    return new WASMTensor(indices.wasmTensor.add_index(axis, count));\n}\n//# sourceMappingURL=wasm.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../tensor/wasm/tensor';\nimport { sparseDenseMatMulCPU } from './cpu';\nimport { sparseDenseMatMulWASM } from './wasm';\nexport function matMul(a, b) {\n    if (b instanceof SparseTensor) {\n        throw new Error('Sparse-sparse matrix multiplication not yet implemented');\n    }\n    else {\n        return sparseDenseMatMul(a, b);\n    }\n}\nfunction sparseDenseMatMul(a, b) {\n    if (a.denseDims === 1) {\n        return new SparseTensor(a.values.matMul(b), a.indices.copy(), [a.shape[0], b.getShape()[1]], 1);\n    }\n    if (b instanceof CPUTensor) {\n        return sparseDenseMatMulCPU(a, b);\n    }\n    else if (b instanceof WASMTensor) {\n        return sparseDenseMatMulWASM(a, b);\n    }\n    throw new Error('Sparse-dense matrix multiplication not yet supported on WebGL');\n}\n//# sourceMappingURL=matMul.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\n/**\n * Calculates the sparse-dense matrix product, assuming that a\n * has zero dense dimensions.\n *\n * The result is a dense CPU tensor\n */\nexport function sparseDenseMatMulCPU(a, b) {\n    const M = a.shape[0];\n    const O = b.shape[1];\n    const result = new CPUTensor([M, O], undefined, b.dtype);\n    const indices = a.indices;\n    const values = a.values;\n    for (let i = 0; i < a.nnz; i++) {\n        const m = indices.get(i * 2);\n        const n = indices.get(i * 2 + 1);\n        const v = values.get(i);\n        for (let o = 0; o < O; o++) {\n            result.set(m * O + o, result.get(m * O + o) + v * b.get(n * O + o));\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../tensor/wasm/tensor';\n/**\n * Calculates the sparse-dense matrix product, assuming that a\n * has zero dense dimensions.\n *\n * The result is a dense CPU tensor\n */\nexport function sparseDenseMatMulWASM(a, b) {\n    return new WASMTensor(a.values.wasmTensor.matmul_sparse_dense(a.indices.wasmTensor, b.wasmTensor, a.shape[0]), undefined, a.dtype);\n}\n//# sourceMappingURL=wasm.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { gpuConstructor } from '../../../tensor/gpu/tensor';\nimport { computeStrides, getSize } from '../../../util/shape';\nimport { Dispatcher } from '../../gpu/dispatcher';\nimport { Operation } from '../../gpu/operation';\nexport class RepeatIndexOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('sparseShape')} int sparseShape[${this.maxRank}];\n    ${this.getVarModifier('repeatStrides')} int repeatStrides[${this.maxRank}];\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'sparseShape', length: this.maxRank },\n            { name: 'repeatStrides', length: this.maxRank },\n        ];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      int newPos = index[0];\n      int nnz = shapeA[0];\n\n      int repeatPos = newPos / nnz;\n      int oldPos = newPos - repeatPos*nnz;\n\n      int oldIx[${this.maxRank}];\n      ${this.initIndex('oldIx')}\n      oldIx[0] = oldPos;\n      oldIx[1] = index[1];\n\n      int repeatIx[${this.maxRank}];\n      ${this.initIndex('repeatIx')}\n      ${this.posToIndex('repeatStrides', 'repeatIx', 'repeatPos')}\n\n      float res = _A(oldIx);\n      for (int i = 0; i < ${this.maxRank}; i++) {\n        if (i == index[1]) {\n          res += float(repeatIx[i]*sparseShape[i]);\n          break;\n        }\n      }\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['A'];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { A: input.A });\n        }\n        const outputShape = this.getOutputShape(input);\n        const info = this.getCompilationInfo(input);\n        return this.compute(outputShape, { A: input.A }, {\n            sparseShape: this.pad(info.sparseShape),\n            repeatStrides: this.pad(info.repeatStrides),\n        });\n    }\n    getOutputShape(input) {\n        return [input.A.shape[0] * input.repeatsProd, input.A.shape[1]];\n    }\n    compile(info) {\n        if (info.sparseShape !== undefined) {\n            this.maxRank = info.sparseShape.length;\n        }\n        if (info.repeatStrides !== undefined) {\n            this.maxRank = info.repeatStrides.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            sparseShape: input.shape,\n            repeatStrides: computeStrides(input.repeats),\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.repeats}-${input.shape}`;\n    }\n}\nexport const defaultRepeatIndexD = new Dispatcher((dtype) => new RepeatIndexOperation(gpuConstructor, dtype));\nexport function repeatIndexGPU(indices, repeats, shape, repeatsProd) {\n    return defaultRepeatIndexD.calc({\n        A: indices,\n        repeats,\n        shape,\n        repeatsProd,\n    }, 'uint32');\n}\n//# sourceMappingURL=gpu.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../tensor/wasm/tensor';\nimport { repeatIndicesCPU } from './cpu';\nimport { repeatIndexGPU } from './gpu';\nimport { repeatIndicesWASM } from './wasm';\nexport function repeat(tensor, repeats) {\n    const sparseRepeats = repeats.slice(0, tensor.sparseDims);\n    const denseRepeats = repeats.slice(tensor.sparseDims);\n    const sparseRepeatsProd = sparseRepeats.reduce((a, b) => a * b, 1);\n    const values = tensor.values.repeat([sparseRepeatsProd, ...denseRepeats]);\n    let indices;\n    if (sparseRepeatsProd > 1) {\n        indices = repeatIndices(tensor.indices, sparseRepeats, tensor.getSparseShape(), sparseRepeatsProd);\n    }\n    else {\n        indices = tensor.indices.copy();\n    }\n    const newShape = tensor.shape.map((v, i) => v * repeats[i]);\n    return new SparseTensor(values, indices, newShape, tensor.denseDims);\n}\nfunction repeatIndices(indices, repeats, shape, repeatsProd) {\n    if (indices instanceof CPUTensor) {\n        return repeatIndicesCPU(indices, repeats, shape, repeatsProd);\n    }\n    else if (indices instanceof WASMTensor) {\n        return repeatIndicesWASM(indices, repeats, shape, repeatsProd);\n    }\n    else {\n        return repeatIndexGPU(indices, repeats, shape, repeatsProd);\n    }\n}\n//# sourceMappingURL=repeat.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { computeStrides, posToIndex } from '../../../util/shape';\nexport function repeatIndicesCPU(indices, repeats, shape, repeatsProd) {\n    const nnz = indices.shape[0];\n    const nnzNew = nnz * repeatsProd;\n    const S = indices.shape[1];\n    const result = new CPUTensor([nnz * repeatsProd, S], undefined, 'uint32');\n    const repeatStrides = computeStrides(repeats);\n    for (let i = 0; i < nnzNew; i++) {\n        const oldI = i % nnz;\n        const repeatPos = Math.floor(i / nnz);\n        const repeatIx = posToIndex(repeatPos, repeatStrides);\n        const ix = repeatIx.map((v, i) => v * shape[i]);\n        for (let j = 0; j < S; j++) {\n            result.set(i * S + j, ix[j] + indices.get(oldI * S + j));\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=cpu.js.map","import { WASMTensor } from '../../../tensor/wasm/tensor';\nexport function repeatIndicesWASM(indices, repeats, shape, repeatsProd) {\n    return new WASMTensor(indices.wasmTensor.repeat_sparse_indices(new Uint32Array(repeats), new Uint32Array(shape), repeatsProd));\n}\n//# sourceMappingURL=wasm.js.map","import { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { gpuConstructor } from '../../../tensor/gpu/tensor';\nimport { SparseTensor } from '../../../tensor/sparse/tensor';\nimport { compareShapes, computeStrides, getSize } from '../../../util/shape';\nimport { Dispatcher } from '../../gpu/dispatcher';\nimport { Operation } from '../../gpu/operation';\nexport class ReshapeIndicesOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('nnzFraction')} int nnzFraction;\n    ${this.getVarModifier('sparseDims')} int sparseDims;\n    ${this.getVarModifier('oldSparseStrides')} int oldSparseStrides[${this.maxRank}];\n    ${this.getVarModifier('newSparseStrides')} int newSparseStrides[${this.maxRank}];\n    `;\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'nnzFraction' },\n            { name: 'sparseDims' },\n            { name: 'oldSparseStrides', length: this.maxRank },\n            { name: 'newSparseStrides', length: this.maxRank },\n        ];\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float process(int index[${this.maxRank}]) {\n      float res = 0.0;\n\n      int newNNZ = int(index[0]);\n      int oldNnzIx = newNNZ / nnzFraction;\n      int residualNNZ = newNNZ - oldNnzIx*nnzFraction;\n\n      int oldSparseIx[${this.maxRank}];\n      ${this.initIndex('oldSparseIx')}\n      for (int j = 0; j < ${this.maxRank}; j++) {\n        if (j >= sparseDims) {\n          break;\n        }\n        oldSparseIx[j] = int(getValueAtPos(oldNnzIx * sparseDims + j, widthA, heightA, A));\n      }\n\n      int oldSparsePos = indexToPos(oldSparseIx, oldSparseStrides);\n      int newSparsePos = oldSparsePos * nnzFraction + residualNNZ;\n      int newSparseIx[${this.maxRank}];\n      ${this.initIndex('newSparseIx')}\n      ${this.posToIndex('newSparseStrides', 'newSparseIx', 'newSparsePos')}\n\n      int ax = int(index[1]);\n\n      for (int j = 0; j < ${this.maxRank}; j++) {\n        if (j == ax) {\n          res = float(newSparseIx[j]);\n          break;\n        }\n      }\n\n      return res;\n    }\n\n    ${this.getDefaultMain()}\n    `;\n    }\n    getTextureNames() {\n        return ['A'];\n    }\n    calc(input) {\n        if (this.fullyStatic && this.outputShape !== undefined) {\n            return this.compute(this.outputShape, { A: input.A });\n        }\n        const outputShape = this.getOutputShape(input);\n        const info = this.getCompilationInfo(input);\n        return this.compute(outputShape, { A: input.A }, {\n            nnzFraction: info.nnzFraction,\n            sparseDims: info.sparseDims,\n            oldSparseStrides: this.pad(info.oldSparseStrides),\n            newSparseStrides: this.pad(info.newSparseStrides),\n        });\n    }\n    getOutputShape(input) {\n        const oldSparseSize = getSize(input.sparseShape);\n        const sparseShape = [];\n        let sparseSize = 1;\n        for (let i = 0; i < input.shape.length; i++) {\n            if (sparseSize < oldSparseSize) {\n                sparseSize *= input.shape[i];\n                sparseShape.push(input.shape[i]);\n            }\n            else {\n                break;\n            }\n        }\n        const nnzFraction = sparseSize / oldSparseSize;\n        const nnz = input.nnz * nnzFraction;\n        return [nnz, sparseShape.length];\n    }\n    compile(info) {\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        const oldSparseSize = getSize(input.sparseShape);\n        const sparseShape = [];\n        const denseShape = [];\n        let sparseSize = 1;\n        for (let i = 0; i < input.shape.length; i++) {\n            if (sparseSize < oldSparseSize) {\n                sparseSize *= input.shape[i];\n                sparseShape.push(input.shape[i]);\n            }\n            else {\n                denseShape.push(input.shape[i]);\n            }\n        }\n        const oldSparseStrides = computeStrides(input.sparseShape);\n        const newSparseStrides = computeStrides(sparseShape);\n        const nnzFraction = sparseSize / oldSparseSize;\n        return {\n            shapeA: input.A.shape,\n            widthA: input.A.memory.width,\n            heightA: input.A.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            sparseDims: input.sparseShape.length,\n            newSparseStrides,\n            oldSparseStrides,\n            nnzFraction,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.A.shape}-${input.nnz}-${input.shape}-${input.sparseShape}`;\n    }\n}\nexport const defaultReshapeIndicesD = new Dispatcher((dtype) => new ReshapeIndicesOperation(gpuConstructor, dtype));\nexport function reshapeGPU(tensor, values, indices, shape, copy) {\n    const oldSparseSize = getSize(tensor.getSparseShape());\n    const sparseShape = [];\n    const denseShape = [];\n    let sparseSize = 1;\n    for (let i = 0; i < shape.length; i++) {\n        if (sparseSize < oldSparseSize) {\n            sparseSize *= shape[i];\n            sparseShape.push(shape[i]);\n        }\n        else {\n            denseShape.push(shape[i]);\n        }\n    }\n    const nnzFraction = sparseSize / oldSparseSize;\n    const nnz = tensor.nnz * nnzFraction;\n    const newValues = values.reshape([nnz, ...denseShape], copy);\n    let newIndices;\n    if (!copy && compareShapes(sparseShape, tensor.getSparseShape())) {\n        newIndices = indices;\n    }\n    else {\n        newIndices = defaultReshapeIndicesD.calc({\n            A: indices,\n            sparseShape: tensor.getSparseShape(),\n            shape: shape,\n            nnz: tensor.nnz,\n        }, 'uint32');\n    }\n    return new SparseTensor(newValues, newIndices, shape, denseShape.length);\n}\n//# sourceMappingURL=gpu.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { WASMTensor } from '../../../tensor/wasm/tensor';\nimport { reshapeCPU } from './cpu';\nimport { reshapeGPU } from './gpu';\nimport { reshapeWasm } from './wasm';\nexport function reshape(tensor, shape, copy) {\n    if (tensor.values instanceof CPUTensor) {\n        return reshapeCPU(tensor, tensor.values, tensor.indices, shape, copy);\n    }\n    else if (tensor.values instanceof WASMTensor) {\n        return reshapeWasm(tensor, tensor.values, tensor.indices, shape, copy);\n    }\n    else {\n        return reshapeGPU(tensor, tensor.values, tensor.indices, shape, copy);\n    }\n}\n//# sourceMappingURL=reshape.js.map","import { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { SparseTensor } from '../../../tensor/sparse/tensor';\nimport { compareShapes, computeStrides, getSize, indexToPos, posToIndex, } from '../../../util/shape';\nexport function reshapeCPU(tensor, values, indices, shape, copy) {\n    const oldSparseSize = getSize(tensor.getSparseShape());\n    const sparseShape = [];\n    const denseShape = [];\n    let sparseSize = 1;\n    for (let i = 0; i < shape.length; i++) {\n        if (sparseSize < oldSparseSize) {\n            sparseSize *= shape[i];\n            sparseShape.push(shape[i]);\n        }\n        else {\n            denseShape.push(shape[i]);\n        }\n    }\n    const oldSparseStrides = computeStrides(tensor.getSparseShape());\n    const newSparseStrides = computeStrides(sparseShape);\n    const nnzFraction = sparseSize / oldSparseSize;\n    const nnz = tensor.nnz * nnzFraction;\n    const newValues = values.reshape([nnz, ...denseShape], copy);\n    let newIndices;\n    if (!copy && compareShapes(sparseShape, tensor.getSparseShape())) {\n        newIndices = indices;\n    }\n    else {\n        newIndices = new CPUTensor([nnz, sparseShape.length], undefined, 'uint32');\n        for (let i = 0; i < nnz; i++) {\n            const oldNnzIx = Math.floor(i / nnzFraction);\n            const oldSparseIx = [];\n            for (let j = 0; j < tensor.sparseDims; j++) {\n                oldSparseIx.push(indices.get(oldNnzIx * tensor.sparseDims + j));\n            }\n            const oldSparsePos = indexToPos(oldSparseIx, oldSparseStrides);\n            const newSparsePos = oldSparsePos * nnzFraction + (i % nnzFraction);\n            const newSparseIx = posToIndex(newSparsePos, newSparseStrides);\n            for (let j = 0; j < sparseShape.length; j++) {\n                newIndices.set(i * sparseShape.length + j, newSparseIx[j]);\n            }\n        }\n    }\n    return new SparseTensor(newValues, newIndices, shape, denseShape.length);\n}\n//# sourceMappingURL=cpu.js.map","import { SparseTensor } from '../../../tensor/sparse/tensor';\nimport { WASMTensor } from '../../../tensor/wasm/tensor';\nimport { compareShapes, getSize } from '../../../util/shape';\nexport function reshapeWasm(tensor, values, indices, shape, copy) {\n    const oldSparseSize = getSize(tensor.getSparseShape());\n    const sparseShape = [];\n    const denseShape = [];\n    let sparseSize = 1;\n    for (let i = 0; i < shape.length; i++) {\n        if (sparseSize < oldSparseSize) {\n            sparseSize *= shape[i];\n            sparseShape.push(shape[i]);\n        }\n        else {\n            denseShape.push(shape[i]);\n        }\n    }\n    const nnzFraction = sparseSize / oldSparseSize;\n    const nnz = tensor.nnz * nnzFraction;\n    const newValues = values.reshape([nnz, ...denseShape], copy);\n    let newIndices;\n    if (!copy && compareShapes(sparseShape, tensor.getSparseShape())) {\n        newIndices = indices;\n    }\n    else {\n        newIndices = new WASMTensor(indices.wasmTensor.reshape_sparse_indices(new Uint32Array(tensor.getSparseShape()), new Uint32Array(shape)), undefined, 'uint32');\n    }\n    return new SparseTensor(newValues, newIndices, shape, denseShape.length);\n}\n//# sourceMappingURL=wasm.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { max } from '../../ops/sparse/aggregate/max/max';\nimport { min } from '../../ops/sparse/aggregate/min/min';\nimport { product } from '../../ops/sparse/aggregate/product/product';\nimport { reduceLogSum } from '../../ops/sparse/aggregate/reduceLogSum/reduceLogSum';\nimport { reduceLogSumExp } from '../../ops/sparse/aggregate/reduceLogSumExp/reduceLogSumExp';\nimport { reduceMean } from '../../ops/sparse/aggregate/reduceMean/reduceMean';\nimport { reduceMeanSquare } from '../../ops/sparse/aggregate/reduceMeanSquare/reduceMeanSquare';\nimport { sum } from '../../ops/sparse/aggregate/sum/sum';\nimport { sumSquare } from '../../ops/sparse/aggregate/sumSquare/sumSquare';\nimport { add } from '../../ops/sparse/binary/add/add';\nimport { divide } from '../../ops/sparse/binary/divide/divide';\nimport { multiply } from '../../ops/sparse/binary/multiply/multiply';\nimport { subtract } from '../../ops/sparse/binary/subtract/subtract';\nimport { concat } from '../../ops/sparse/concat/concat';\nimport { matMul } from '../../ops/sparse/matMul/matMul';\nimport { repeat } from '../../ops/sparse/repeat/repeat';\nimport { reshape } from '../../ops/sparse/reshape/reshape';\nimport Tensor, { tensorValuesConstructor, } from '../../types';\nimport { computeStrides, getSize, indexToPos, posToIndex, } from '../../util/shape';\nimport { CPUTensor } from '../cpu/tensor';\nexport class SparseTensor extends Tensor {\n    /**\n     * Creates a new sparse tensor in coordinate format. The tensor has\n     * a number of sparse dimensions and optionally a number of dense\n     * dimensions. The shape of a sparse tensor can thus be decomposed\n     * into [...S, ...D], where S is the shape of the sparse dimensions\n     * and D the shape of the dense dimensions. By default the number of\n     * dense dimensions is zero\n     *\n     * The values tensor holds all non-zero values and has shape [NNZ, ...D]\n     * where NNZ is the number of non-zero entries. The indices tensor\n     * holds the location of all non-zero entries of the tensor and\n     * has shape [NNZ, |S|] (where |S| is the number of sparse dimensions).\n     *\n     * Note that all indexes that are not specified are implicitly zero.\n     * This does however **not** mean that they become non-zero on\n     * certain element wise operations. Instead element wise operations\n     * maintain the sparsity pattern. Otherwise, many operations would\n     * create effectively dense tensors (eg. exp()), or would simply not be\n     * well defined (eg. log()).\n     *\n     * @example\n     *\n     * If you want to create a sparse tensor, equivalent to the following CPU\n     * tensor:\n     * ```typescript\n     * const a = new CPUTensor([3,3],[1,0,0,0,2,0,0,3,4]);\n     * ```\n     * you collect the indices, where the value is nonzero:\n     * ```typescript\n     * const indices = [\n     *  0,0,  // Corresponds to value 1\n     *  1,1,  // Corresponds to value 2\n     *  2,1,  // Corresponds to value 3\n     *  2,2   // Corresponds to value 4\n     * ];\n     * const indiceTensor = new CPUTensor([4, 2], indices, 'uint32');\n     * ```\n     * and the corresponding values:\n     * ```typescript\n     * const values = [1,2,3,4];\n     * const valueTensor = new CPUTensor([4],values);\n     *\n     * const sparseTensor = new SparseTensor(valueTensor, indiceTensor, [3,3]);\n     * ```\n     */\n    constructor(\n    /**\n     * Values of the nonzero entries\n     */\n    values, \n    /**\n     * Coordinates of the nonzero entries. Has shape [nnz, S],\n     * where nnz is the number of nonzero entries and S the number of\n     * sparse dimension.\n     *\n     * Each row contains the coordinate of the respective nonzero entry.\n     */\n    indices, \n    /**\n     * Shape of the tensor\n     */\n    shape, \n    /**\n     * Number of dense dimensions. Defaults to 0\n     */\n    denseDims = 0) {\n        super(values.dtype);\n        this.values = values;\n        this.indices = indices;\n        this.shape = shape;\n        this.denseDims = denseDims;\n        this.size = getSize(shape);\n        this.strides = computeStrides(shape);\n        this.nnz = this.indices.getShape()[0];\n        this.sparseDims = this.shape.length - this.denseDims;\n    }\n    /**\n     * Creates a sparse tensor with zero dense dimensions from a dense CPU tensor.\n     *\n     * @example\n     * ```typescript\n     * const denseTensor = new CPUTensor([3,3],[1,0,0,0,2,0,0,3,4]);\n     *\n     * const sparseTensor = SparseTensor.fromDense(denseTensor);\n     * console.log(sparseTensor.nnz); // Will log '4'\n     * console.log(sparseTensor.sparseDims); // Will log '2'\n     * ```\n     */\n    static fromDense(tensor) {\n        let nnz = 0;\n        const ix = [];\n        const vals = [];\n        for (let i = 0; i < tensor.size; i++) {\n            if (tensor.get(i) !== 0) {\n                nnz++;\n                const index = posToIndex(i, tensor.strides);\n                for (let j = 0; j < tensor.shape.length; j++) {\n                    ix.push(index[j]);\n                }\n                vals.push(tensor.get(i));\n            }\n        }\n        const indices = new CPUTensor([nnz, tensor.shape.length], ix, 'uint32');\n        const values = new CPUTensor([nnz], vals, tensor.dtype);\n        return new SparseTensor(values, indices, tensor.shape);\n    }\n    getValues() {\n        return __awaiter(this, void 0, void 0, function* () {\n            const vals = yield this.values.getValues();\n            const indices = yield this.indices.getValues();\n            const denseSize = getSize(this.getDenseShape(), 1);\n            const sparseStrides = computeStrides(this.getSparseShape());\n            const result = new tensorValuesConstructor[this.values.dtype](this.size);\n            for (let i = 0; i < this.nnz; i++) {\n                const sparseIx = [];\n                for (let j = 0; j < this.sparseDims; j++) {\n                    sparseIx.push(indices[i * this.sparseDims + j]);\n                }\n                const sparsePos = indexToPos(sparseIx, sparseStrides);\n                for (let j = 0; j < denseSize; j++) {\n                    result[sparsePos * denseSize + j] = vals[i * denseSize + j];\n                }\n            }\n            return result;\n        });\n    }\n    /**\n     * Sparse part of the shape of the tensor, ie. the S first values of\n     * the shape, where S is then number of sparse dimension.\n     */\n    getSparseShape() {\n        return this.shape.slice(0, this.shape.length - this.denseDims);\n    }\n    /**\n     * Dense part of the shape of the tensor, ie. the D last values of\n     * the shape, where D is then number of dense dimension.\n     */\n    getDenseShape() {\n        return this.shape.slice(this.shape.length - this.denseDims);\n    }\n    getShape() {\n        return this.shape;\n    }\n    /**\n     * Creates a new sparse tensor with the same sparsity shape\n     * and the given value everywhere.\n     *\n     * @param value Constant value to set at every position\n     */\n    constantLike(value) {\n        return new SparseTensor(this.values.constantLike(value), this.indices.copy(), this.shape, this.denseDims);\n    }\n    /**\n     * Not implemented yet\n     */\n    singleConstant(value) {\n        throw new Error('Method not implemented.');\n    }\n    cast(dtype) {\n        return new SparseTensor(this.values.cast(dtype), this.indices.copy(), this.shape, this.denseDims);\n    }\n    delete() {\n        this.values.delete();\n        this.indices.delete();\n    }\n    reshape_impl(shape, copy) {\n        return reshape(this, shape, copy);\n    }\n    exp() {\n        return new SparseTensor(this.values.exp(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    log() {\n        return new SparseTensor(this.values.log(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    sqrt() {\n        return new SparseTensor(this.values.sqrt(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    abs() {\n        return new SparseTensor(this.values.abs(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    sin() {\n        return new SparseTensor(this.values.sin(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    cos() {\n        return new SparseTensor(this.values.cos(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    tan() {\n        return new SparseTensor(this.values.tan(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    asin() {\n        return new SparseTensor(this.values.asin(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    acos() {\n        return new SparseTensor(this.values.acos(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    atan() {\n        return new SparseTensor(this.values.atan(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    sinh() {\n        return new SparseTensor(this.values.sinh(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    cosh() {\n        return new SparseTensor(this.values.cosh(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    tanh() {\n        return new SparseTensor(this.values.tanh(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    asinh() {\n        return new SparseTensor(this.values.asinh(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    acosh() {\n        return new SparseTensor(this.values.acosh(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    atanh() {\n        return new SparseTensor(this.values.atanh(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    negate() {\n        return new SparseTensor(this.values.negate(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    powerScalar(power, factor) {\n        return new SparseTensor(this.values.powerScalar(power, factor), this.indices.copy(), this.shape, this.denseDims);\n    }\n    sigmoid() {\n        return new SparseTensor(this.values.sigmoid(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    hardSigmoid(alpha, beta) {\n        return new SparseTensor(this.values.hardSigmoid(alpha, beta), this.indices.copy(), this.shape, this.denseDims);\n    }\n    sign() {\n        return new SparseTensor(this.values.sign(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    addMultiplyScalar(factor, add) {\n        return new SparseTensor(this.values.addMultiplyScalar(factor, add), this.indices.copy(), this.shape, this.denseDims);\n    }\n    /**\n     * Calculates the matrix product. This tensor should have shape [M,N]\n     *\n     * Two cases are supported for sparse tensors:\n     * - If this tensor has one sparse dimension, the resulting tensor is\n     *   a sparse tensor with the same number of non-zero entries\n     * - If this tensor has two sparse dimensions, the resulting tensor\n     *   is dense.\n     * Right now this only supports sparse-dense matrix multiplication.\n     * Supported on\n     * - All backends if the sparse tensor has 1 sparse dimensions\n     * - Only on CPU/WASM if the sparse tensor has no sparse dimensions\n     *\n     * @param tensor Dense matrix to multiply with. Should have shape [N,O]\n     *\n     * @result Tensor with shape [M,O]\n     */\n    matMul(tensor) {\n        return matMul(this, tensor);\n    }\n    /**\n     * Concatenate the two tensors along the given axis\n     *\n     * Note that at the moment, only concatenation along\n     * sparse dimensions is supported!\n     *\n     */\n    concat(tensor, axis) {\n        if (!(tensor instanceof SparseTensor)) {\n            throw new Error('Can only concatenate sparse tensors!');\n        }\n        return concat(this, tensor, axis);\n    }\n    clip(min, max) {\n        return new SparseTensor(this.values.clip(min, max), this.indices.copy(), this.shape, this.denseDims);\n    }\n    /**\n     * Not implemented yet\n     */\n    clipBackward(grad, min, max) {\n        throw new Error('Method not implemented.');\n    }\n    repeat(repeats) {\n        return repeat(this, repeats);\n    }\n    /**\n     * Not implemented yet\n     */\n    expand(shape) {\n        throw new Error('Method not implemented.');\n    }\n    copy() {\n        return new SparseTensor(this.values.copy(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    /**\n     * Not implemented yet\n     */\n    gather(axis, indices) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Not implemented yet\n     */\n    setValues(values, starts) {\n        throw new Error('Method not implemented.');\n    }\n    floor() {\n        return new SparseTensor(this.values.floor(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    ceil() {\n        return new SparseTensor(this.values.ceil(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    round() {\n        return new SparseTensor(this.values.round(), this.indices.copy(), this.shape, this.denseDims);\n    }\n    /**\n     * Not implemented yet\n     */\n    upsample(scales) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Not implemented yet\n     */\n    normalize(mean, variance, epsilon, scale, bias) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Adds a second tensor, which can either be a sparse or a dense tensor:\n     * - If the second tensor is a dense tensor, it is assumed that it has a rank at most\n     *   equal to the dense dimensions of the first tensor.\n     *   If this is not the case, entries in the second tensors that are zero in the first\n     *   tensor are simply ignored!\n     *   This also means that broadcasting in the first tensor is only supported\n     *   on the dense dimensions!\n     * - If the second tensor is a sparse tensor, it is assumed that the first and\n     *   second tensor have exactly the same sparsity pattern!\n     *\n     * This is not supported on the WebGL backend yet.\n     */\n    add(tensor, alpha, beta) {\n        return super.add(tensor, alpha, beta);\n    }\n    /**\n     * Subtracts a second tensor, which can either be a sparse or a dense tensor.\n     * The same restrictions as for {@link SparseTensor.add} apply!\n     */\n    subtract(tensor, alpha, beta) {\n        return super.subtract(tensor, alpha, beta);\n    }\n    /**\n     * Multiplies a second tensor element wise, which can either be a sparse or a dense tensor.\n     * The same restrictions as for {@link SparseTensor.add} apply!\n     */\n    multiply(tensor, alpha) {\n        return super.multiply(tensor, alpha);\n    }\n    /**\n     * Divides a second tensor element wise, which can either be a sparse or a dense tensor.\n     * The same restrictions as for {@link SparseTensor.add} apply!\n     */\n    divide(tensor, alpha) {\n        return super.divide(tensor, alpha);\n    }\n    add_impl(th, tensor, resultShape, alpha, beta) {\n        return add(th, tensor, resultShape, alpha, beta);\n    }\n    subtract_impl(th, tensor, resultShape, alpha, beta) {\n        return subtract(th, tensor, resultShape, alpha, beta);\n    }\n    multiply_impl(th, tensor, resultShape, alpha) {\n        return multiply(th, tensor, resultShape, alpha);\n    }\n    divide_impl(th, tensor, resultShape, alpha) {\n        return divide(th, tensor, resultShape, alpha);\n    }\n    /**\n     * Not implemented yet\n     */\n    power_impl(th, tensor, resultShape) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Not implemented yet\n     */\n    gemm_impl(b, aTranspose, bTranspose, alpha, beta, C) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Sums over sparse and/or dense dimensions according to the specified\n     * axes.\n     *\n     * - If summing only over dense dimensions, all backends are supported.\n     * - If summing over sparse dimensions, only CPU/WebGL are supported\n     */\n    sum(axes, keepDims) {\n        return super.sum(axes, keepDims);\n    }\n    sum_impl(axes, keepDims) {\n        return sum(this, axes, keepDims);\n    }\n    sumSquare_impl(axes, keepDims) {\n        return sumSquare(this, axes, keepDims);\n    }\n    product_impl(axes, keepDims) {\n        return product(this, axes, keepDims);\n    }\n    max_impl(axes, keepDims) {\n        return max(this, axes, keepDims);\n    }\n    min_impl(axes, keepDims) {\n        return min(this, axes, keepDims);\n    }\n    reduceMean_impl(axes, keepDims) {\n        return reduceMean(this, axes, keepDims);\n    }\n    reduceMeanSquare_impl(axes, keepDims) {\n        return reduceMeanSquare(this, axes, keepDims);\n    }\n    reduceLogSum_impl(axes, keepDims) {\n        return reduceLogSum(this, axes, keepDims);\n    }\n    reduceLogSumExp_impl(axes, keepDims) {\n        return reduceLogSumExp(this, axes, keepDims);\n    }\n    /**\n     * Not implemented yet\n     */\n    conv_impl(kernel, dilations, group, pads, strides, activation, bias) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Not implemented yet\n     */\n    convTranspose_impl(kernel, dilations, group, pads, strides) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Not implemented yet\n     */\n    pad_impl(pads, mode, value) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Not implemented yet\n     */\n    averagePool_impl(kernelShape, pads, strides, includePad) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Not implemented yet\n     */\n    transpose_impl(permutation) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Not implemented yet\n     */\n    slice_impl(starts, ends, axes, steps) {\n        throw new Error('Method not implemented.');\n    }\n}\n//# sourceMappingURL=tensor.js.map","export class AbsBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const sign = this.input.value.sign();\n        const gradAbs = grad.multiply(sign);\n        sign.delete();\n        const needed = this.input.backward(gradAbs);\n        if (!needed) {\n            gradAbs.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=absBack.js.map","export class ExpBack {\n    constructor(input, exp) {\n        this.input = input;\n        this.exp = exp;\n    }\n    backward(grad) {\n        const gradExp = grad.multiply(this.exp);\n        const needed = this.input.backward(gradExp);\n        if (!needed) {\n            gradExp.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=expBack.js.map","export class LogBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const gradLog = grad.divide(this.input.value);\n        const needed = this.input.backward(gradLog);\n        if (!needed) {\n            gradLog.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=logBack.js.map","export class MatMulBack {\n    constructor(a, b) {\n        this.a = a;\n        this.b = b;\n    }\n    backward(grad) {\n        if (!this.b.noGrad) {\n            const gradB = this.a.value.gemm(grad, true, false);\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n        if (!this.a.noGrad) {\n            const gradA = grad.gemm(this.b.value, false, true);\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n        if (!this.b.isLeaf()) {\n            this.b.delete();\n        }\n    }\n}\n//# sourceMappingURL=matMulBack.js.map","export class NegateBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const gradIn = grad.negate();\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            gradIn.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=negateBack.js.map","export class SqrtBack {\n    constructor(input, sqrt) {\n        this.input = input;\n        this.sqrt = sqrt;\n    }\n    backward(grad) {\n        const gradSqrt = grad.divide(this.sqrt, 0.5);\n        const needed = this.input.backward(gradSqrt);\n        if (!needed) {\n            gradSqrt.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=sqrtBack.js.map","export class ConcatBack {\n    constructor(a, b, axis) {\n        this.a = a;\n        this.b = b;\n        this.axis = axis;\n    }\n    backward(grad) {\n        let axis = this.axis;\n        if (axis < 0) {\n            axis += this.a.getShape().length;\n        }\n        if (!this.a.noGrad) {\n            const gradA = grad.slice([0], [this.a.getShape()[axis]], [axis]);\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n        if (!this.b.noGrad) {\n            const gradB = grad.slice([this.a.getShape()[axis]], [grad.getShape()[axis]], [axis]);\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n        if (!this.b.isLeaf()) {\n            this.b.delete();\n        }\n    }\n}\n//# sourceMappingURL=concatBack.js.map","export class ClipBack {\n    constructor(input, min, max) {\n        this.input = input;\n        this.min = min;\n        this.max = max;\n    }\n    backward(grad) {\n        const gradIn = this.input.value.clipBackward(grad, this.min, this.max);\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            gradIn.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=clipBack.js.map","export class RepeatBack {\n    constructor(a, repeats) {\n        this.a = a;\n        this.repeats = repeats;\n    }\n    backward(grad) {\n        const shapeA = this.a.getShape();\n        const gradNewShape = [];\n        const sumAxes = [];\n        for (let i = 0; i < shapeA.length; i++) {\n            gradNewShape.push(this.repeats[i], shapeA[i]);\n            sumAxes.push(i * 2);\n        }\n        const gradA = grad.reshape(gradNewShape, false).sum(sumAxes, false);\n        const needed = this.a.backward(gradA);\n        if (!needed) {\n            gradA.delete();\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n    }\n}\n//# sourceMappingURL=repeatBack.js.map","export class ExpandBack {\n    constructor(a, shape) {\n        this.a = a;\n        this.shape = shape;\n    }\n    backward(grad) {\n        // eslint-disable-next-line @typescript-eslint/no-unused-vars\n        const [_shape, goal, resultShape] = this.a.value.alignShapes(this.a.getShape(), this.shape);\n        const sumDims = [];\n        for (let i = 0; i < _shape.length; i++) {\n            if (_shape[i] < goal[i]) {\n                sumDims.push(i);\n            }\n        }\n        const gradA = grad.sum(sumDims).reshape(this.a.getShape());\n        const needed = this.a.backward(gradA);\n        if (!needed) {\n            gradA.delete();\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n    }\n}\n//# sourceMappingURL=expandBack.js.map","export class ReshapeBack {\n    constructor(a) {\n        this.a = a;\n    }\n    backward(grad) {\n        const shapeA = this.a.getShape();\n        if (!this.a.noGrad) {\n            const gradA = grad.reshape(shapeA);\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n    }\n}\n//# sourceMappingURL=reshapeBack.js.map","export class AddBack {\n    constructor(a, b, shape, alpha, beta) {\n        this.a = a;\n        this.b = b;\n        this.shape = shape;\n        this.alpha = alpha;\n        this.beta = beta;\n    }\n    backward(grad) {\n        const shapeA = this.a.getShape();\n        const shapeB = this.b.getShape();\n        const sumADims = [];\n        const sumBDims = [];\n        for (let i = 0; i < this.shape.length; i++) {\n            if (shapeA[i] < this.shape[i]) {\n                sumADims.push(i);\n            }\n            if (shapeB[i] < this.shape[i]) {\n                sumBDims.push(i);\n            }\n        }\n        if (!this.a.noGrad) {\n            let gradA;\n            if (sumADims.length === 0) {\n                if (this.alpha === 1) {\n                    gradA = grad.reshape(shapeA);\n                }\n                else {\n                    gradA = grad.multiplyScalar(this.alpha).reshape(shapeA, false);\n                }\n            }\n            else {\n                if (this.alpha === 1) {\n                    gradA = grad.sum(sumADims).reshape(shapeA, false);\n                }\n                else {\n                    const summed = grad.sum(sumADims);\n                    const scaled = summed.multiplyScalar(this.alpha);\n                    summed.delete();\n                    gradA = scaled.reshape(shapeA, false);\n                }\n            }\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n        if (!this.b.noGrad) {\n            let gradB;\n            if (sumBDims.length === 0) {\n                if (this.beta === 1) {\n                    gradB = grad.reshape(shapeB);\n                }\n                else {\n                    gradB = grad.multiplyScalar(this.beta).reshape(shapeB, false);\n                }\n            }\n            else {\n                if (this.beta === 1) {\n                    gradB = grad.sum(sumBDims).reshape(shapeB, false);\n                }\n                else {\n                    const summed = grad.sum(sumBDims);\n                    const scaled = summed.multiplyScalar(this.beta);\n                    summed.delete();\n                    gradB = scaled.reshape(shapeB, false);\n                }\n            }\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n        if (!this.b.isLeaf()) {\n            this.b.delete();\n        }\n    }\n}\n//# sourceMappingURL=addBack.js.map","export class SubtractBack {\n    constructor(a, b, shape, alpha, beta) {\n        this.a = a;\n        this.b = b;\n        this.shape = shape;\n        this.alpha = alpha;\n        this.beta = beta;\n    }\n    backward(grad) {\n        const shapeA = this.a.getShape();\n        const shapeB = this.b.getShape();\n        const sumADims = [];\n        const sumBDims = [];\n        for (let i = 0; i < this.shape.length; i++) {\n            if (shapeA[i] < this.shape[i]) {\n                sumADims.push(i);\n            }\n            if (shapeB[i] < this.shape[i]) {\n                sumBDims.push(i);\n            }\n        }\n        if (!this.a.noGrad) {\n            let gradA;\n            if (sumADims.length === 0) {\n                if (this.alpha === 1) {\n                    gradA = grad.reshape(shapeA);\n                }\n                else {\n                    gradA = grad.multiplyScalar(this.alpha).reshape(shapeA, false);\n                }\n            }\n            else {\n                if (this.alpha === 1) {\n                    gradA = grad.sum(sumADims).reshape(shapeA, false);\n                }\n                else {\n                    const summed = grad.sum(sumADims);\n                    const scaled = summed.multiplyScalar(this.alpha);\n                    summed.delete();\n                    gradA = scaled.reshape(shapeA, false);\n                }\n            }\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n        if (!this.b.noGrad) {\n            let gradB;\n            if (sumBDims.length === 0) {\n                gradB = grad.multiplyScalar(-this.beta).reshape(shapeB, false);\n            }\n            else {\n                const summed = grad.sum(sumBDims);\n                const scaled = summed.multiplyScalar(-this.beta);\n                summed.delete();\n                gradB = scaled.reshape(shapeB, false);\n            }\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n        if (!this.b.isLeaf()) {\n            this.b.delete();\n        }\n    }\n}\n//# sourceMappingURL=subtractBack.js.map","export class MultiplyBack {\n    constructor(a, b, shape, alpha) {\n        this.a = a;\n        this.b = b;\n        this.shape = shape;\n        this.alpha = alpha;\n    }\n    backward(grad) {\n        const shapeA = this.a.getShape();\n        const shapeB = this.b.getShape();\n        const sumADims = [];\n        const sumBDims = [];\n        for (let i = 0; i < this.shape.length; i++) {\n            if (shapeA[i] < this.shape[i]) {\n                sumADims.push(i);\n            }\n            if (shapeB[i] < this.shape[i]) {\n                sumBDims.push(i);\n            }\n        }\n        if (!this.a.noGrad) {\n            let gradA;\n            if (sumADims.length === 0) {\n                gradA = grad.multiply(this.b.value, this.alpha).reshape(shapeA, false);\n            }\n            else {\n                const mult = grad.multiply(this.b.value, this.alpha);\n                const summed = mult.sum(sumADims);\n                mult.delete();\n                gradA = summed.reshape(shapeA, false);\n            }\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n        if (!this.b.noGrad) {\n            let gradB;\n            if (sumBDims.length === 0) {\n                gradB = grad.multiply(this.a.value, this.alpha).reshape(shapeB, false);\n            }\n            else {\n                const mult = grad.multiply(this.a.value, this.alpha);\n                const summed = mult.sum(sumBDims);\n                mult.delete();\n                gradB = summed.reshape(shapeB, false);\n            }\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n        if (!this.b.isLeaf()) {\n            this.b.delete();\n        }\n    }\n}\n//# sourceMappingURL=multiplyBack.js.map","export class ConvBack {\n    constructor(x, w, strides, padding, dilations, group, b) {\n        this.x = x;\n        this.w = w;\n        this.strides = strides;\n        this.padding = padding;\n        this.dilations = dilations;\n        this.group = group;\n        this.b = b;\n    }\n    backward(grad) {\n        if (!this.w.noGrad) {\n            const gradW = this.x.value.conv(grad, undefined, this.strides, this.group, this.padding, this.dilations);\n            const needed = this.w.backward(gradW);\n            if (!needed) {\n                gradW.delete();\n            }\n        }\n        if (this.b !== undefined && !this.b.noGrad) {\n            const biasSum = [0];\n            for (let i = 0; i < this.dilations.length; i++) {\n                biasSum.push(i + 2);\n            }\n            const gradB = grad.sum(biasSum);\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n        if (!this.x.noGrad) {\n            const wShape = this.w.getShape();\n            let xPads = [];\n            for (let i = 0; i < this.dilations.length; i++) {\n                xPads.push(wShape[i + 2] - this.padding[i] + this.dilations[i] - 2);\n            }\n            xPads = [...xPads, ...xPads];\n            const gradX = grad.convTranspose(this.w.value, this.dilations, this.group, xPads, this.strides);\n            const needed = this.x.backward(gradX);\n            if (!needed) {\n                gradX.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.x.isLeaf()) {\n            this.x.delete();\n        }\n        if (!this.w.isLeaf()) {\n            this.w.delete();\n        }\n        if (this.b !== undefined && !this.b.isLeaf()) {\n            this.b.delete();\n        }\n    }\n}\n//# sourceMappingURL=convBack.js.map","export class DivideBack {\n    constructor(a, b, divResult, shape, alpha) {\n        this.a = a;\n        this.b = b;\n        this.divResult = divResult;\n        this.shape = shape;\n        this.alpha = alpha;\n    }\n    backward(grad) {\n        const shapeA = this.a.getShape();\n        const shapeB = this.b.getShape();\n        const sumADims = [];\n        const sumBDims = [];\n        for (let i = 0; i < this.shape.length; i++) {\n            if (shapeA[i] < this.shape[i]) {\n                sumADims.push(i);\n            }\n            if (shapeB[i] < this.shape[i]) {\n                sumBDims.push(i);\n            }\n        }\n        if (!this.a.noGrad) {\n            let gradA;\n            if (sumADims.length === 0) {\n                gradA = grad.divide(this.b.value, this.alpha).reshape(shapeA, false);\n            }\n            else {\n                const mult = grad.divide(this.b.value, this.alpha);\n                const summed = mult.sum(sumADims);\n                mult.delete();\n                gradA = summed.reshape(shapeA, false);\n            }\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n        if (!this.b.noGrad) {\n            let gradB;\n            if (sumBDims.length === 0) {\n                const multiplied = grad.multiply(this.divResult);\n                const divided = multiplied.divide(this.b.value, -this.alpha);\n                multiplied.delete();\n                gradB = divided.reshape(shapeB, false);\n            }\n            else {\n                const multiplied = grad.multiply(this.divResult);\n                const divided = multiplied.divide(this.b.value, -this.alpha);\n                multiplied.delete();\n                const summed = divided.sum(sumBDims);\n                divided.delete();\n                gradB = summed.reshape(shapeB, false);\n            }\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n        if (!this.b.isLeaf()) {\n            this.b.delete();\n        }\n    }\n}\n//# sourceMappingURL=divideBack.js.map","export class PowerBack {\n    constructor(a, b, powerResult, shape) {\n        this.a = a;\n        this.b = b;\n        this.powerResult = powerResult;\n        this.shape = shape;\n    }\n    backward(grad) {\n        const shapeA = this.a.getShape();\n        const shapeB = this.b.getShape();\n        const sumADims = [];\n        const sumBDims = [];\n        for (let i = 0; i < this.shape.length; i++) {\n            if (shapeA[i] < this.shape[i]) {\n                sumADims.push(i);\n            }\n            if (shapeB[i] < this.shape[i]) {\n                sumBDims.push(i);\n            }\n        }\n        if (!this.a.noGrad) {\n            let gradA;\n            if (sumADims.length === 0) {\n                const multiplied = this.powerResult.multiply(this.b.value);\n                const divided = multiplied.divide(this.a.value);\n                multiplied.delete();\n                const gradPowA = grad.multiply(divided);\n                divided.delete();\n                gradA = gradPowA.reshape(shapeA, false);\n            }\n            else {\n                const multiplied = this.powerResult.multiply(this.b.value);\n                const divided = multiplied.divide(this.a.value);\n                multiplied.delete();\n                const gradPowA = grad.multiply(divided);\n                divided.delete();\n                const summed = gradPowA.sum(sumADims);\n                gradPowA.delete();\n                gradA = summed.reshape(shapeA, false);\n            }\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n        if (!this.b.noGrad) {\n            let gradB;\n            if (sumBDims.length === 0) {\n                const lnA = this.a.value.log();\n                const mult = this.powerResult.multiply(lnA);\n                lnA.delete();\n                gradB = grad.multiply(mult);\n                mult.delete();\n                gradB = gradB.reshape(shapeB, false);\n            }\n            else {\n                const lnA = this.a.value.log();\n                const mult = this.powerResult.multiply(lnA);\n                lnA.delete();\n                const _gradB = grad.multiply(mult);\n                mult.delete();\n                const summed = _gradB.sum(sumBDims);\n                _gradB.delete();\n                gradB = summed.reshape(shapeB, false);\n            }\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n        if (!this.b.isLeaf()) {\n            this.b.delete();\n        }\n    }\n}\n//# sourceMappingURL=powerBack.js.map","export class GemmBack {\n    constructor(a, b, transA, transB, alpha, beta, c) {\n        this.a = a;\n        this.b = b;\n        this.transA = transA;\n        this.transB = transB;\n        this.alpha = alpha;\n        this.beta = beta;\n        this.c = c;\n    }\n    backward(grad) {\n        if (!this.b.noGrad) {\n            let gradB;\n            if (this.transB) {\n                gradB = grad.gemm(this.a.value, true, this.transA, this.alpha);\n            }\n            else {\n                gradB = this.a.value.gemm(grad, !this.transA, false, this.alpha);\n            }\n            const needed = this.b.backward(gradB);\n            if (!needed) {\n                gradB.delete();\n            }\n        }\n        if (!this.a.noGrad) {\n            let gradA;\n            if (this.transA) {\n                gradA = this.b.value.gemm(grad, this.transB, true, this.alpha);\n            }\n            else {\n                gradA = grad.gemm(this.b.value, false, !this.transB, this.alpha);\n            }\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n        if (this.c !== undefined && !this.c.noGrad) {\n            const gradShape = grad.getShape();\n            const cShape = this.c.getShape();\n            const cSumDims = [];\n            for (let i = 0; i < gradShape.length; i++) {\n                if (cShape[i] < gradShape[i]) {\n                    cSumDims.push(i);\n                }\n            }\n            let gradC = grad.sum(cSumDims).reshape(cShape, false);\n            if (this.beta !== 1) {\n                const oldGradC = gradC;\n                gradC = gradC.multiplyScalar(this.beta);\n                oldGradC.delete();\n            }\n            const needed = this.c.backward(gradC);\n            if (!needed) {\n                gradC.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n        if (!this.b.isLeaf()) {\n            this.b.delete();\n        }\n        if (this.c !== undefined && !this.c.isLeaf()) {\n            this.c.delete();\n        }\n    }\n}\n//# sourceMappingURL=gemmBack.js.map","export class TransposeBack {\n    constructor(a, permutation) {\n        this.a = a;\n        this.permutation = permutation;\n    }\n    backward(grad) {\n        const inversePerm = new Array(this.permutation.length);\n        for (let i = 0; i < this.permutation.length; i++) {\n            inversePerm[this.permutation[i]] = i;\n        }\n        const gradA = grad.transpose(inversePerm);\n        const needed = this.a.backward(gradA);\n        if (!needed) {\n            gradA.delete();\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n    }\n}\n//# sourceMappingURL=transposeBack.js.map","export class SumBack {\n    constructor(input, sumDims, keepDims) {\n        this.input = input;\n        this.sumDims = sumDims;\n        this.keepDims = keepDims;\n    }\n    backward(grad) {\n        const inShape = this.input.value.getShape();\n        if (!this.keepDims) {\n            const newShape = [];\n            let sumI = 0;\n            for (let i = 0; i < inShape.length; i++) {\n                if (sumI < this.sumDims.length && this.sumDims[sumI] === i) {\n                    newShape.push(1);\n                    sumI++;\n                }\n                else {\n                    newShape.push(inShape[i]);\n                }\n            }\n            grad = grad.reshape(newShape, false);\n        }\n        grad = grad.expand(inShape);\n        const needed = this.input.backward(grad);\n        if (!needed) {\n            grad.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=sumBack.js.map","export class SumSquareBack {\n    constructor(input, sumDims, keepDims) {\n        this.input = input;\n        this.sumDims = sumDims;\n        this.keepDims = keepDims;\n    }\n    backward(grad) {\n        const inShape = this.input.value.getShape();\n        if (!this.keepDims) {\n            const newShape = [];\n            let sumI = 0;\n            for (let i = 0; i < inShape.length; i++) {\n                if (sumI < this.sumDims.length && this.sumDims[sumI] === i) {\n                    newShape.push(1);\n                    sumI++;\n                }\n                else {\n                    newShape.push(inShape[i]);\n                }\n            }\n            grad = grad.reshape(newShape, false);\n        }\n        const expanded = grad.expand(inShape);\n        const gradIn = expanded.multiply(this.input.value, 2);\n        expanded.delete();\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            gradIn.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=sumSquareBack.js.map","export class AddMultiplyScalarBack {\n    constructor(input, scalar) {\n        this.input = input;\n        this.scalar = scalar;\n    }\n    backward(grad) {\n        const gradIn = grad.multiplyScalar(this.scalar);\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            gradIn.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=addMultiplyScalarBack.js.map","export class MeanBack {\n    constructor(input, sumDims, keepDims) {\n        this.input = input;\n        this.sumDims = sumDims;\n        this.keepDims = keepDims;\n    }\n    backward(grad) {\n        const inShape = this.input.value.getShape();\n        if (!this.keepDims) {\n            const newShape = [];\n            let sumI = 0;\n            for (let i = 0; i < inShape.length; i++) {\n                if (sumI < this.sumDims.length && this.sumDims[sumI] === i) {\n                    newShape.push(1);\n                    sumI++;\n                }\n                else {\n                    newShape.push(inShape[i]);\n                }\n            }\n            grad = grad.reshape(newShape, false);\n        }\n        let sumSize = 1;\n        for (let i = 0; i < this.sumDims.length; i++) {\n            sumSize *= inShape[this.sumDims[i]];\n        }\n        const multiplied = grad.multiplyScalar(1 / sumSize);\n        const gradIn = multiplied.expand(inShape);\n        multiplied.delete;\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            grad.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=meanBack.js.map","export class MeanSquareBack {\n    constructor(input, sumDims, keepDims) {\n        this.input = input;\n        this.sumDims = sumDims;\n        this.keepDims = keepDims;\n    }\n    backward(grad) {\n        const inShape = this.input.value.getShape();\n        if (!this.keepDims) {\n            const newShape = [];\n            let sumI = 0;\n            for (let i = 0; i < inShape.length; i++) {\n                if (sumI < this.sumDims.length && this.sumDims[sumI] === i) {\n                    newShape.push(1);\n                    sumI++;\n                }\n                else {\n                    newShape.push(inShape[i]);\n                }\n            }\n            grad = grad.reshape(newShape, false);\n        }\n        let sumSize = 1;\n        for (let i = 0; i < this.sumDims.length; i++) {\n            sumSize *= inShape[this.sumDims[i]];\n        }\n        const expanded = grad.expand(inShape);\n        const gradIn = expanded.multiply(this.input.value, 2 / sumSize);\n        expanded.delete();\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            gradIn.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=meanSquareBack.js.map","export class SliceBack {\n    constructor(a, starts, ends, axes, steps) {\n        this.a = a;\n        this.starts = starts;\n        this.ends = ends;\n        this.axes = axes;\n        this.steps = steps;\n        if (steps.find(x => x !== 1) !== undefined) {\n            throw new Error('Slice backward pass only supports step size of 1');\n        }\n    }\n    backward(grad) {\n        if (!this.a.noGrad) {\n            const shapeA = this.a.getShape();\n            const rank = shapeA.length;\n            const pads = new Array(rank * 2).fill(0);\n            for (let i = 0; i < this.axes.length; i++) {\n                pads[this.axes[i]] = this.starts[i];\n                pads[rank + this.axes[i]] = shapeA[this.axes[i]] - this.ends[i];\n            }\n            const gradA = grad.pad(pads, 'constant', 0);\n            const needed = this.a.backward(gradA);\n            if (!needed) {\n                gradA.delete();\n            }\n        }\n    }\n    delete() {\n        if (!this.a.isLeaf()) {\n            this.a.delete();\n        }\n    }\n}\n//# sourceMappingURL=sliceBack.js.map","export class AveragePoolBack {\n    constructor(x, kernelShape, pads, strides, includePad) {\n        this.x = x;\n    }\n    backward(grad) {\n        throw new Error('Backward pass not implemented for average pool');\n    }\n    delete() {\n        if (!this.x.isLeaf()) {\n            this.x.delete();\n        }\n    }\n}\n//# sourceMappingURL=averagePoolBack.js.map","export class PadBack {\n    constructor(x, pads, mode, value) {\n        this.x = x;\n        this.pads = pads;\n        this.mode = mode;\n        this.value = value;\n    }\n    backward(grad) {\n        throw new Error('Backward pass not implemented for pad');\n    }\n    delete() {\n        if (!this.x.isLeaf()) {\n            this.x.delete();\n        }\n    }\n}\n//# sourceMappingURL=padBack.js.map","export class ProductBack {\n    constructor(input, product, sumDims, keepDims) {\n        this.input = input;\n        this.product = product;\n        this.sumDims = sumDims;\n        this.keepDims = keepDims;\n    }\n    backward(grad) {\n        const inShape = this.input.value.getShape();\n        let mult = grad.multiply(this.product);\n        if (!this.keepDims) {\n            const newShape = [];\n            let sumI = 0;\n            for (let i = 0; i < inShape.length; i++) {\n                if (sumI < this.sumDims.length && this.sumDims[sumI] === i) {\n                    newShape.push(1);\n                    sumI++;\n                }\n                else {\n                    newShape.push(inShape[i]);\n                }\n            }\n            mult = mult.reshape(newShape, false);\n        }\n        const expanded = mult.expand(inShape);\n        mult.delete();\n        const gradIn = expanded.divide(this.input.value);\n        expanded.delete();\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            gradIn.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=productBack.js.map","export class SigmoidBack {\n    constructor(input, sigmoid) {\n        this.input = input;\n        this.sigmoid = sigmoid;\n    }\n    backward(grad) {\n        const oneMinus = this.sigmoid.addMultiplyScalar(-1, 1);\n        const mult = this.sigmoid.multiply(oneMinus);\n        oneMinus.delete();\n        const gradIn = mult.multiply(grad);\n        mult.delete();\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            gradIn.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=sigmoidBack.js.map","export class SinBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const cos = this.input.value.cos();\n        const gradAbs = grad.multiply(cos);\n        cos.delete();\n        const needed = this.input.backward(gradAbs);\n        if (!needed) {\n            gradAbs.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class ASinBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const squared = this.input.value.multiply(this.input.value);\n        const oneMinus = squared.addMultiplyScalar(-1, 1);\n        squared.delete();\n        const sqrt = oneMinus.sqrt();\n        oneMinus.delete();\n        const gradASin = grad.divide(sqrt);\n        sqrt.delete();\n        const needed = this.input.backward(gradASin);\n        if (!needed) {\n            gradASin.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class SinHBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const cosh = this.input.value.cosh();\n        const gradCosH = grad.multiply(cosh);\n        cosh.delete();\n        const needed = this.input.backward(gradCosH);\n        if (!needed) {\n            gradCosH.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class ASinHBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const squared = this.input.value.multiply(this.input.value);\n        const onePlus = squared.addMultiplyScalar(1, 1);\n        squared.delete();\n        const sqrt = onePlus.sqrt();\n        onePlus.delete();\n        const gradASinH = grad.divide(sqrt);\n        sqrt.delete();\n        const needed = this.input.backward(gradASinH);\n        if (!needed) {\n            gradASinH.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=sinBack.js.map","export class CosBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const sin = this.input.value.sin();\n        const gradAbs = grad.multiply(sin, -1);\n        sin.delete();\n        const needed = this.input.backward(gradAbs);\n        if (!needed) {\n            gradAbs.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class ACosBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const squared = this.input.value.multiply(this.input.value);\n        const oneMinus = squared.addMultiplyScalar(-1, 1);\n        squared.delete();\n        const sqrt = oneMinus.sqrt();\n        oneMinus.delete();\n        const gradACos = grad.divide(sqrt, -1);\n        sqrt.delete();\n        const needed = this.input.backward(gradACos);\n        if (!needed) {\n            gradACos.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class CosHBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const sinh = this.input.value.sinh();\n        const gradCosH = grad.multiply(sinh);\n        sinh.delete();\n        const needed = this.input.backward(gradCosH);\n        if (!needed) {\n            gradCosH.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class ACosHBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const squared = this.input.value.multiply(this.input.value);\n        const onePlus = squared.addMultiplyScalar(1, -1);\n        squared.delete();\n        const sqrt = onePlus.sqrt();\n        onePlus.delete();\n        const gradACosH = grad.divide(sqrt);\n        sqrt.delete();\n        const needed = this.input.backward(gradACosH);\n        if (!needed) {\n            gradACosH.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=cosBack.js.map","export class TanBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const cos = this.input.value.cos();\n        const cos2 = cos.multiply(cos);\n        cos.delete();\n        const gradTan = grad.divide(cos2);\n        cos2.delete();\n        const needed = this.input.backward(gradTan);\n        if (!needed) {\n            gradTan.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class ATanBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const squared = this.input.value.multiply(this.input.value);\n        const onePlus = squared.addMultiplyScalar(1, 1);\n        squared.delete();\n        const gradATan = grad.divide(onePlus);\n        onePlus.delete();\n        const needed = this.input.backward(gradATan);\n        if (!needed) {\n            gradATan.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class TanHBack {\n    constructor(input, tanH) {\n        this.input = input;\n        this.tanH = tanH;\n    }\n    backward(grad) {\n        const squared = this.tanH.multiply(this.tanH);\n        const onePlus = squared.addMultiplyScalar(-1, 1);\n        squared.delete();\n        const gradTanH = grad.multiply(onePlus);\n        onePlus.delete();\n        const needed = this.input.backward(gradTanH);\n        if (!needed) {\n            gradTanH.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\nexport class ATanHBack {\n    constructor(input) {\n        this.input = input;\n    }\n    backward(grad) {\n        const squared = this.input.value.multiply(this.input.value);\n        const onePlus = squared.addMultiplyScalar(-1, 1);\n        squared.delete();\n        const gradATanH = grad.divide(onePlus);\n        onePlus.delete();\n        const needed = this.input.backward(gradATanH);\n        if (!needed) {\n            gradATanH.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=tanBack.js.map","export class LogSumBack {\n    constructor(input, sumDims, keepDims) {\n        this.input = input;\n        this.sumDims = sumDims;\n        this.keepDims = keepDims;\n    }\n    backward(grad) {\n        const inShape = this.input.value.getShape();\n        const sum = this.input.value.sum(this.sumDims, this.keepDims);\n        let gradLogSum = grad.divide(sum);\n        sum.delete();\n        if (!this.keepDims) {\n            const newShape = [];\n            let sumI = 0;\n            for (let i = 0; i < inShape.length; i++) {\n                if (sumI < this.sumDims.length && this.sumDims[sumI] === i) {\n                    newShape.push(1);\n                    sumI++;\n                }\n                else {\n                    newShape.push(inShape[i]);\n                }\n            }\n            gradLogSum = gradLogSum.reshape(newShape, false);\n        }\n        const expanded = gradLogSum.expand(inShape);\n        gradLogSum.delete();\n        const needed = this.input.backward(expanded);\n        if (!needed) {\n            expanded.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=logSumBack.js.map","export class LogSumExpBack {\n    constructor(input, sumDims, keepDims) {\n        this.input = input;\n        this.sumDims = sumDims;\n        this.keepDims = keepDims;\n    }\n    backward(grad) {\n        const inShape = this.input.value.getShape();\n        const exp = this.input.value.exp();\n        const sum = exp.sum(this.sumDims, true);\n        const div = exp.divide(sum);\n        exp.delete();\n        sum.delete();\n        if (!this.keepDims) {\n            const newShape = [];\n            let sumI = 0;\n            for (let i = 0; i < inShape.length; i++) {\n                if (sumI < this.sumDims.length && this.sumDims[sumI] === i) {\n                    newShape.push(1);\n                    sumI++;\n                }\n                else {\n                    newShape.push(inShape[i]);\n                }\n            }\n            grad = grad.reshape(newShape, false);\n        }\n        const expanded = grad.expand(inShape);\n        const gradLogSumExp = expanded.multiply(div);\n        expanded.delete();\n        const needed = this.input.backward(gradLogSumExp);\n        if (!needed) {\n            gradLogSumExp.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=logSumExpBack.js.map","export class PowerScalarBack {\n    constructor(input, power, factor) {\n        this.input = input;\n        this.power = power;\n        this.factor = factor;\n    }\n    backward(grad) {\n        const pow = this.input.value.powerScalar(this.power - 1, this.factor * this.power);\n        const gradIn = grad.multiply(pow);\n        pow.delete();\n        const needed = this.input.backward(gradIn);\n        if (!needed) {\n            gradIn.delete();\n        }\n    }\n    delete() {\n        if (!this.input.isLeaf()) {\n            this.input.delete();\n        }\n    }\n}\n//# sourceMappingURL=powerScalarBack.js.map","/* eslint-disable @typescript-eslint/no-unused-vars */\nimport { Tensor } from '../library';\nimport { CPUTensor } from '../tensor/cpu/tensor';\nimport { AbsBack } from './ops/unary/absBack';\nimport { ExpBack } from './ops/unary/expBack';\nimport { LogBack } from './ops/unary/logBack';\nimport { MatMulBack } from './ops/matMul/matMulBack';\nimport { NegateBack } from './ops/unary/negateBack';\nimport { SqrtBack } from './ops/unary/sqrtBack';\nimport { ConcatBack } from './ops/util/concatBack';\nimport { ClipBack } from './ops/unary/clipBack';\nimport { RepeatBack } from './ops/util/repeatBack';\nimport { ExpandBack } from './ops/util/expandBack';\nimport { ReshapeBack } from './ops/util/reshapeBack';\nimport { AddBack } from './ops/binary/addBack';\nimport { SubtractBack } from './ops/binary/subtractBack';\nimport { MultiplyBack } from './ops/binary/multiplyBack';\nimport { ConvBack } from './ops/conv/convBack';\nimport { DivideBack } from './ops/binary/divideBack';\nimport { PowerBack } from './ops/binary/powerBack';\nimport { GemmBack } from './ops/matMul/gemmBack';\nimport { TransposeBack } from './ops/util/transposeBack';\nimport { SumBack } from './ops/reduce/sumBack';\nimport { SumSquareBack } from './ops/reduce/sumSquareBack';\nimport { AddMultiplyScalarBack } from './ops/unary/addMultiplyScalarBack';\nimport { MeanBack } from './ops/reduce/meanBack';\nimport { MeanSquareBack } from './ops/reduce/meanSquareBack';\nimport { SliceBack } from './ops/util/sliceBack';\nimport { AveragePoolBack } from './ops/conv/averagePoolBack';\nimport { PadBack } from './ops/conv/padBack';\nimport { ProductBack } from './ops/reduce/productBack';\nimport { SigmoidBack } from './ops/unary/sigmoidBack';\nimport { WASMTensor } from '../tensor/wasm/tensor';\nimport { GPUTensor } from '../tensor/gpu/tensor';\nimport { ASinBack, ASinHBack, SinBack, SinHBack } from './ops/unary/sinBack';\nimport { ACosBack, ACosHBack, CosBack, CosHBack } from './ops/unary/cosBack';\nimport { ATanBack, ATanHBack, TanBack, TanHBack } from './ops/unary/tanBack';\nimport { LogSumBack } from './ops/reduce/logSumBack';\nimport { LogSumExpBack } from './ops/reduce/logSumExpBack';\nimport { PowerScalarBack } from './ops/unary/powerScalarBack';\n/**\n * Tensor that also has a gradient associated to it\n * When noGrad is false, a dynamic computation graph on\n * this variable will be build.\n *\n * Once backward on a scalar variable (eg. a variable with shape [1])\n * is called, the gradients for all variables will be computed\n */\nexport class Variable extends Tensor {\n    /**\n     * Creates a variable whose value is the specified value\n     */\n    constructor(value, options) {\n        super(value.dtype);\n        this.value = value;\n        if (options === undefined) {\n            options = {};\n        }\n        this.grad = options.grad;\n        if (options.backEdge !== undefined) {\n            this.backEdge = options.backEdge;\n        }\n        this.noGrad = options.noGrad || false;\n    }\n    static create(shape, values, backend, options, dtype) {\n        if (dtype === undefined) {\n            dtype = 'float32';\n        }\n        let value;\n        if (backend === 'CPU') {\n            value = new CPUTensor(shape, values);\n        }\n        else if (backend === 'WASM') {\n            value = new WASMTensor(values, new Uint32Array(shape), dtype);\n        }\n        else {\n            value = new GPUTensor(values, shape, dtype);\n        }\n        return new Variable(value, options);\n    }\n    /**\n     * Creates a GPU variable from texture data (eg. Image/Video element)\n     */\n    static fromData(data, options) {\n        const tensor = GPUTensor.fromData(data);\n        return new Variable(tensor, options);\n    }\n    cast(dtype) {\n        throw new Error('Method not implemented.');\n    }\n    /**\n     * Performs a backward pass and returns wether the grad is needed or can be deleted\n     */\n    backward(grad) {\n        if (grad === undefined) {\n            const ownShape = this.value.getShape();\n            if (ownShape.length === 1 && ownShape[0] === 1) {\n                grad = this.value;\n            }\n            else {\n                throw new Error('Backward without an input gradient can only be done for tensors with shape [1]');\n            }\n        }\n        let needed = false;\n        if (this.grad !== undefined) {\n            const oldGrad = this.grad;\n            this.grad = this.grad.add(grad);\n            oldGrad.delete();\n        }\n        else {\n            this.grad = grad;\n            needed = true;\n        }\n        if (this.backEdge !== undefined) {\n            this.backEdge.backward(grad);\n        }\n        return needed;\n    }\n    isLeaf() {\n        return this.backEdge === undefined;\n    }\n    constantLike(value) {\n        return new Variable(this.value.constantLike(value), { noGrad: true });\n    }\n    singleConstant(value) {\n        return new Variable(this.value.singleConstant(value), { noGrad: true });\n    }\n    getValues() {\n        return this.value.getValues();\n    }\n    getShape() {\n        return this.value.getShape();\n    }\n    delete() {\n        this.value.delete();\n        if (this.grad !== undefined) {\n            this.grad.delete();\n        }\n        if (this.backEdge !== undefined) {\n            this.backEdge.delete();\n        }\n    }\n    reshape_impl(shape, copy) {\n        return new Variable(this.value.reshape(shape), {\n            backEdge: this.noGrad ? undefined : new ReshapeBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    exp() {\n        const exp = this.value.exp();\n        return new Variable(exp, {\n            backEdge: this.noGrad ? undefined : new ExpBack(this, exp),\n            noGrad: this.noGrad,\n        });\n    }\n    log() {\n        return new Variable(this.value.log(), {\n            backEdge: this.noGrad ? undefined : new LogBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    sqrt() {\n        const sqrt = this.value.sqrt();\n        return new Variable(sqrt, {\n            backEdge: this.noGrad ? undefined : new SqrtBack(this, sqrt),\n            noGrad: this.noGrad,\n        });\n    }\n    abs() {\n        return new Variable(this.value.abs(), {\n            backEdge: this.noGrad ? undefined : new AbsBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    sin() {\n        return new Variable(this.value.sin(), {\n            backEdge: this.noGrad ? undefined : new SinBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    cos() {\n        return new Variable(this.value.cos(), {\n            backEdge: this.noGrad ? undefined : new CosBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    tan() {\n        return new Variable(this.value.tan(), {\n            backEdge: this.noGrad ? undefined : new TanBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    asin() {\n        return new Variable(this.value.asin(), {\n            backEdge: this.noGrad ? undefined : new ASinBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    acos() {\n        return new Variable(this.value.acos(), {\n            backEdge: this.noGrad ? undefined : new ACosBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    atan() {\n        return new Variable(this.value.atan(), {\n            backEdge: this.noGrad ? undefined : new ATanBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    sinh() {\n        return new Variable(this.value.sinh(), {\n            backEdge: this.noGrad ? undefined : new SinHBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    cosh() {\n        return new Variable(this.value.cosh(), {\n            backEdge: this.noGrad ? undefined : new CosHBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    tanh() {\n        const tanh = this.value.tanh();\n        return new Variable(tanh, {\n            backEdge: this.noGrad ? undefined : new TanHBack(this, tanh),\n            noGrad: this.noGrad,\n        });\n    }\n    asinh() {\n        return new Variable(this.value.asinh(), {\n            backEdge: this.noGrad ? undefined : new ASinHBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    acosh() {\n        return new Variable(this.value.acosh(), {\n            backEdge: this.noGrad ? undefined : new ACosHBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    atanh() {\n        return new Variable(this.value.atanh(), {\n            backEdge: this.noGrad ? undefined : new ATanHBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    sigmoid() {\n        const sigmoid = this.value.sigmoid();\n        return new Variable(sigmoid, {\n            backEdge: this.noGrad ? undefined : new SigmoidBack(this, sigmoid),\n            noGrad: this.noGrad,\n        });\n    }\n    hardSigmoid(alpha, beta) {\n        throw new Error('Method not implemented.');\n    }\n    sign() {\n        // No back edge since the gradient will be zero anyway\n        return new Variable(this.value.sign());\n    }\n    negate() {\n        return new Variable(this.value.negate(), {\n            backEdge: this.noGrad ? undefined : new NegateBack(this),\n            noGrad: this.noGrad,\n        });\n    }\n    addMultiplyScalar(factor, add) {\n        return new Variable(this.value.addMultiplyScalar(factor, add), {\n            backEdge: this.noGrad\n                ? undefined\n                : new AddMultiplyScalarBack(this, factor),\n            noGrad: this.noGrad,\n        });\n    }\n    powerScalar(power, factor) {\n        return new Variable(this.value.powerScalar(power, factor), {\n            backEdge: this.noGrad\n                ? undefined\n                : new PowerScalarBack(this, power, factor),\n            noGrad: this.noGrad,\n        });\n    }\n    setValues(values, starts) {\n        throw new Error('Method not implemented.');\n    }\n    matMul(tensor) {\n        if (!(tensor instanceof Variable)) {\n            throw new Error('MatMul can only be done with another variable');\n        }\n        const noGrad = this.noGrad && tensor.noGrad;\n        return new Variable(this.value.matMul(tensor.value), {\n            backEdge: noGrad ? undefined : new MatMulBack(this, tensor),\n            noGrad,\n        });\n    }\n    concat(tensor, axis) {\n        if (!(tensor instanceof Variable)) {\n            throw new Error('Concat can only be done with another variable');\n        }\n        const noGrad = this.noGrad && tensor.noGrad;\n        return new Variable(this.value.concat(tensor.value, axis), {\n            backEdge: noGrad ? undefined : new ConcatBack(this, tensor, axis),\n            noGrad,\n        });\n    }\n    clip(min, max) {\n        return new Variable(this.value.clip(min, max), {\n            backEdge: this.noGrad ? undefined : new ClipBack(this, min, max),\n            noGrad: this.noGrad,\n        });\n    }\n    clipBackward(grad, min, max) {\n        throw new Error('Clip backward not implemented for Variable');\n    }\n    repeat(repeats) {\n        return new Variable(this.value.repeat(repeats), {\n            backEdge: this.noGrad ? undefined : new RepeatBack(this, repeats),\n            noGrad: this.noGrad,\n        });\n    }\n    expand(shape) {\n        return new Variable(this.value.expand(shape), {\n            backEdge: this.noGrad ? undefined : new ExpandBack(this, shape),\n            noGrad: this.noGrad,\n        });\n    }\n    copy() {\n        return new Variable(this.value.copy(), {\n            grad: this.grad !== undefined ? this.grad.copy() : undefined,\n        });\n    }\n    gather(axis, indices) {\n        throw new Error('Method not implemented.');\n    }\n    floor() {\n        return new Variable(this.value.floor());\n    }\n    ceil() {\n        return new Variable(this.value.ceil());\n    }\n    round() {\n        return new Variable(this.value.round());\n    }\n    upsample(scales) {\n        throw new Error('Method not implemented.');\n    }\n    normalize(mean, variance, epsilon, scale, bias) {\n        throw new Error('Method not implemented.');\n    }\n    add_impl(th, tensor, resultShape, alpha, beta) {\n        if (!(tensor instanceof Variable) || !(th instanceof Variable)) {\n            throw new Error('Can only add Variable tensor to Variable tensor');\n        }\n        const noGrad = th.noGrad && tensor.noGrad;\n        return new Variable(th.value.add_impl(th.value, tensor.value, resultShape, alpha, beta), {\n            backEdge: noGrad\n                ? undefined\n                : new AddBack(th, tensor, resultShape, alpha, beta),\n            noGrad,\n        });\n    }\n    subtract_impl(th, tensor, resultShape, alpha, beta) {\n        if (!(tensor instanceof Variable) || !(th instanceof Variable)) {\n            throw new Error('Can only add Variable tensor to Variable tensor');\n        }\n        const noGrad = th.noGrad && tensor.noGrad;\n        return new Variable(th.value.subtract_impl(th.value, tensor.value, resultShape, alpha, beta), {\n            backEdge: noGrad\n                ? undefined\n                : new SubtractBack(th, tensor, resultShape, alpha, beta),\n            noGrad,\n        });\n    }\n    multiply_impl(th, tensor, resultShape, alpha) {\n        if (!(tensor instanceof Variable) || !(th instanceof Variable)) {\n            throw new Error('Can only add Variable tensor to Variable tensor');\n        }\n        const noGrad = th.noGrad && tensor.noGrad;\n        return new Variable(th.value.multiply_impl(th.value, tensor.value, resultShape, alpha), {\n            backEdge: noGrad\n                ? undefined\n                : new MultiplyBack(th, tensor, resultShape, alpha),\n            noGrad,\n        });\n    }\n    divide_impl(th, tensor, resultShape, alpha) {\n        if (!(tensor instanceof Variable) || !(th instanceof Variable)) {\n            throw new Error('Can only divide Variable tensor by Variable tensor');\n        }\n        const divResult = th.value.divide_impl(th.value, tensor.value, resultShape, alpha);\n        const noGrad = th.noGrad && tensor.noGrad;\n        return new Variable(divResult, {\n            backEdge: noGrad\n                ? undefined\n                : new DivideBack(th, tensor, divResult, resultShape, alpha),\n            noGrad,\n        });\n    }\n    power_impl(th, tensor, resultShape) {\n        if (!(tensor instanceof Variable) || !(th instanceof Variable)) {\n            throw new Error('Can only take Variable tensor to power of Variable tensor');\n        }\n        const powerResult = th.value.power_impl(th.value, tensor.value, resultShape);\n        const noGrad = th.noGrad && tensor.noGrad;\n        return new Variable(powerResult, {\n            backEdge: noGrad\n                ? undefined\n                : new PowerBack(th, tensor, powerResult, resultShape),\n            noGrad,\n        });\n    }\n    gemm_impl(b, aTranspose, bTranspose, alpha, beta, C) {\n        if (!(b instanceof Variable) ||\n            (C !== undefined && !(C instanceof Variable))) {\n            throw new Error('Can only do gemm with variable tensors');\n        }\n        const noGrad = this.noGrad && b.noGrad && (C !== undefined ? C.noGrad : true);\n        return new Variable(this.value.gemm_impl(b.value, aTranspose, bTranspose, alpha, beta, C !== undefined ? C.value : undefined), {\n            backEdge: noGrad\n                ? undefined\n                : new GemmBack(this, b, aTranspose, bTranspose, alpha, beta, C),\n            noGrad,\n        });\n    }\n    sum_impl(axes, keepDims) {\n        return new Variable(this.value.sum(axes, keepDims), {\n            backEdge: this.noGrad ? undefined : new SumBack(this, axes, keepDims),\n            noGrad: this.noGrad,\n        });\n    }\n    sumSquare_impl(axes, keepDims) {\n        return new Variable(this.value.sumSquare(axes, keepDims), {\n            backEdge: this.noGrad\n                ? undefined\n                : new SumSquareBack(this, axes, keepDims),\n            noGrad: this.noGrad,\n        });\n    }\n    product_impl(axes, keepDims) {\n        const product = this.value.product(axes, keepDims);\n        return new Variable(product, {\n            backEdge: this.noGrad\n                ? undefined\n                : new ProductBack(this, product, axes, keepDims),\n            noGrad: this.noGrad,\n        });\n    }\n    max_impl(axes, keepDims) {\n        throw new Error('Method not implemented.');\n    }\n    min_impl(axes, keepDims) {\n        throw new Error('Method not implemented.');\n    }\n    reduceMean_impl(axes, keepDims) {\n        return new Variable(this.value.reduceMean(axes, keepDims), {\n            backEdge: this.noGrad ? undefined : new MeanBack(this, axes, keepDims),\n            noGrad: this.noGrad,\n        });\n    }\n    reduceMeanSquare_impl(axes, keepDims) {\n        return new Variable(this.value.reduceMeanSquare(axes, keepDims), {\n            backEdge: this.noGrad\n                ? undefined\n                : new MeanSquareBack(this, axes, keepDims),\n            noGrad: this.noGrad,\n        });\n    }\n    reduceLogSum_impl(axes, keepDims) {\n        return new Variable(this.value.reduceLogSum(axes, keepDims), {\n            backEdge: this.noGrad ? undefined : new LogSumBack(this, axes, keepDims),\n            noGrad: this.noGrad,\n        });\n    }\n    reduceLogSumExp_impl(axes, keepDims) {\n        return new Variable(this.value.reduceLogSumExp(axes, keepDims), {\n            backEdge: this.noGrad\n                ? undefined\n                : new LogSumExpBack(this, axes, keepDims),\n            noGrad: this.noGrad,\n        });\n    }\n    conv_impl(kernel, dilations, group, pads, strides, activation, bias) {\n        if (!(kernel instanceof Variable) ||\n            (bias !== undefined && !(bias instanceof Variable))) {\n            throw new Error('Can only do convolution with variable as kernel and bias');\n        }\n        if (activation !== 'id') {\n            throw new Error('Activation has to be ID for convolution with variables');\n        }\n        const noGrad = this.noGrad && kernel.noGrad && (bias !== undefined ? bias.noGrad : true);\n        return new Variable(this.value.conv(kernel.value, bias !== undefined ? bias.value : undefined, dilations, group, pads, strides), {\n            backEdge: noGrad\n                ? undefined\n                : new ConvBack(this, kernel, strides, pads, dilations, group, bias),\n            noGrad,\n        });\n    }\n    convTranspose_impl(kernel, dilations, group, pads, strides) {\n        throw new Error('Method not implemented.');\n    }\n    pad_impl(pads, mode, value) {\n        return new Variable(this.value.pad(pads, mode, value), {\n            backEdge: this.noGrad ? undefined : new PadBack(this, pads, mode, value),\n            noGrad: this.noGrad,\n        });\n    }\n    averagePool_impl(kernelShape, pads, strides, includePad) {\n        return new Variable(this.value.averagePool(kernelShape, pads, strides, includePad), {\n            backEdge: this.noGrad\n                ? undefined\n                : new AveragePoolBack(this, kernelShape, pads, strides, includePad),\n            noGrad: this.noGrad,\n        });\n    }\n    transpose_impl(permutation) {\n        return new Variable(this.value.transpose(permutation), {\n            backEdge: this.noGrad ? undefined : new TransposeBack(this, permutation),\n            noGrad: this.noGrad,\n        });\n    }\n    slice_impl(starts, ends, axes, steps) {\n        return new Variable(this.value.slice(starts, ends, axes, steps), {\n            backEdge: this.noGrad\n                ? undefined\n                : new SliceBack(this, starts, ends, axes, steps),\n            noGrad: this.noGrad,\n        });\n    }\n}\n//# sourceMappingURL=variable.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { Variable } from '../autograd/variable';\nimport { CPUTensor } from '../tensor/cpu/tensor';\nimport { GPUTensor } from '../tensor/gpu/tensor';\nimport { SparseTensor } from '../tensor/sparse/tensor';\nimport { WASMTensor } from '../tensor/wasm/tensor';\n/**\n * Convert a tensor to the specified backend\n */\nexport function toBackend(tensor, backend) {\n    return __awaiter(this, void 0, void 0, function* () {\n        if (backend === 'CPU') {\n            return toCPU(tensor);\n        }\n        else if (backend === 'WASM') {\n            return toWASM(tensor);\n        }\n        else {\n            return toGPU(tensor);\n        }\n    });\n}\nexport function toCPU(tensor) {\n    return __awaiter(this, void 0, void 0, function* () {\n        if (tensor instanceof Variable) {\n            return new Variable(yield toCPU(tensor.value), {\n                grad: tensor.grad !== undefined ? yield toCPU(tensor.grad) : undefined,\n            });\n        }\n        else if (tensor instanceof SparseTensor) {\n            return new SparseTensor(yield toCPU(tensor.values), yield toCPU(tensor.indices), tensor.shape, tensor.denseDims);\n        }\n        if (tensor instanceof CPUTensor) {\n            return tensor;\n        }\n        const values = yield tensor.getValues();\n        return new CPUTensor(tensor.getShape(), values, tensor.dtype);\n    });\n}\nexport function toWASM(tensor) {\n    return __awaiter(this, void 0, void 0, function* () {\n        if (tensor.dtype === 'float16') {\n            throw new Error('Cant represent float16 tensor on Wasm backend');\n        }\n        if (tensor instanceof Variable) {\n            return new Variable(yield toWASM(tensor.value), {\n                grad: tensor.grad !== undefined ? yield toWASM(tensor.grad) : undefined,\n            });\n        }\n        else if (tensor instanceof SparseTensor) {\n            return new SparseTensor(yield toWASM(tensor.values), yield toWASM(tensor.indices), tensor.shape, tensor.denseDims);\n        }\n        if (tensor instanceof WASMTensor) {\n            return tensor;\n        }\n        const values = yield tensor.getValues();\n        if (tensor instanceof CPUTensor && values instanceof Int32Array) {\n            return tensor;\n        }\n        return new WASMTensor(Array.from(values), new Uint32Array(tensor.getShape()), tensor.dtype);\n    });\n}\nexport function toGPU(tensor) {\n    return __awaiter(this, void 0, void 0, function* () {\n        if (tensor.dtype === 'float64') {\n            throw new Error('Cant represent float64 tensor on WebGL backend');\n        }\n        if (tensor instanceof Variable) {\n            return new Variable(yield toGPU(tensor.value), {\n                grad: tensor.grad !== undefined ? yield toGPU(tensor.grad) : undefined,\n            });\n        }\n        else if (tensor instanceof SparseTensor) {\n            return new SparseTensor(yield toGPU(tensor.values), yield toGPU(tensor.indices), tensor.shape, tensor.denseDims);\n        }\n        if (tensor instanceof GPUTensor) {\n            return tensor;\n        }\n        const values = yield tensor.getValues();\n        if (tensor instanceof CPUTensor && values instanceof Int32Array) {\n            return tensor;\n        }\n        return new GPUTensor(Array.from(values), tensor.getShape(), tensor.dtype);\n    });\n}\n/**\n * Determines if the two tensors are of the same type, ie. if they are on the same backend\n */\nexport function sameType(a, b) {\n    if (a.dtype !== b.dtype) {\n        return false;\n    }\n    if (a instanceof Variable && b instanceof Variable) {\n        return sameType(a.value, b.value);\n    }\n    if (a instanceof CPUTensor && b instanceof CPUTensor) {\n        return true;\n    }\n    if (a instanceof WASMTensor && b instanceof WASMTensor) {\n        return true;\n    }\n    if (a instanceof GPUTensor && b instanceof GPUTensor) {\n        return true;\n    }\n    return false;\n}\n//# sourceMappingURL=convert.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { Variable } from '../autograd/variable';\nimport Tensor from '../types';\nimport { toCPU, toWASM, toGPU } from '../util/convert';\n/**\n * A module is a self contained unit that transforms\n * a list of inputs when forward is called.\n *\n * It can be in two modes, training and inference.\n * In training mode, gradients will be tracked, while\n * in inference mode, only the forward pass will be calculated\n */\nexport class Module {\n    constructor() {\n        this.backend = 'CPU';\n        this.mode = 'train';\n    }\n    getSubModules() {\n        const modules = [];\n        for (const k of Object.keys(this)) {\n            //@ts-ignore\n            if (this[k] instanceof Module) {\n                //@ts-ignore\n                modules.push(this[k]);\n            }\n        }\n        return modules;\n    }\n    getParameters() {\n        let parameters = [];\n        for (const k of Object.keys(this)) {\n            //@ts-ignore\n            if (this[k] instanceof Variable) {\n                //@ts-ignore\n                parameters.push(this[k]);\n            }\n        }\n        const modules = this.getSubModules();\n        for (const module of modules) {\n            const params = module.getParameters();\n            parameters = parameters.concat(params);\n        }\n        return parameters;\n    }\n    toBackend(backend) {\n        if (backend === 'CPU') {\n            return this.toCPU();\n        }\n        else if (backend === 'WASM') {\n            return this.toWASM();\n        }\n        else {\n            return this.toGPU();\n        }\n    }\n    toCPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            const submodules = this.getSubModules();\n            for (const submodule of submodules) {\n                yield submodule.toCPU();\n            }\n            for (const k of Object.keys(this)) {\n                //@ts-ignore\n                if (this[k] instanceof Tensor) {\n                    //@ts-ignore\n                    this[k] = yield toCPU(this[k]);\n                }\n            }\n            this.backend = 'CPU';\n        });\n    }\n    toWASM() {\n        return __awaiter(this, void 0, void 0, function* () {\n            const submodules = this.getSubModules();\n            for (const submodule of submodules) {\n                yield submodule.toWASM();\n            }\n            for (const k of Object.keys(this)) {\n                //@ts-ignore\n                if (this[k] instanceof Tensor) {\n                    //@ts-ignore\n                    this[k] = yield toWASM(this[k]);\n                }\n            }\n            this.backend = 'WASM';\n        });\n    }\n    toGPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            const submodules = this.getSubModules();\n            for (const submodule of submodules) {\n                yield submodule.toGPU();\n            }\n            for (const k of Object.keys(this)) {\n                //@ts-ignore\n                if (this[k] instanceof Tensor) {\n                    //@ts-ignore\n                    this[k] = yield toGPU(this[k]);\n                }\n            }\n            this.backend = 'GPU';\n        });\n    }\n}\n//# sourceMappingURL=module.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\n// eslint-disable-next-line node/no-extraneous-import\nimport Long from 'long';\nimport { Module } from '../model/module';\nimport { CPUTensor } from '../tensor/cpu/tensor';\nimport { toCPU } from '../util/convert';\nexport class OnnxNode extends Module {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super();\n        this.attributes = {};\n        this.mode = mode;\n        for (let i = 0; i < attributes.length; i++) {\n            this.attributes[attributes[i].name] = attributes[i];\n        }\n        this.inputs = inputs;\n        this.outputs = outputs;\n        this.onnxVersion = onnxVersion;\n        this.variableInputs = 0;\n        for (let i = 0; i < this.inputs.length; i++) {\n            if (constants[this.inputs[i]] === undefined) {\n                this.variableInputs++;\n            }\n        }\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    initialize(resolveConstant) { }\n    getAttribute(name) {\n        return this.attributes[name];\n    }\n    getAttributeString(name) {\n        const attr = this.attributes[name];\n        if (attr !== undefined) {\n            const str = attr.s;\n            if (str !== undefined && str !== null) {\n                // eslint-disable-next-line node/no-unsupported-features/node-builtins\n                return new TextDecoder('utf-8').decode(str);\n            }\n            return undefined;\n        }\n        return undefined;\n    }\n    getAttributeInts(name) {\n        const attr = this.attributes[name];\n        if (attr !== undefined) {\n            const result = this.attributes[name].ints;\n            if (result !== undefined && result !== null) {\n                for (let i = 0; i < result.length; i++) {\n                    if (Long.isLong(result[i])) {\n                        result[i] = result[i].toNumber();\n                    }\n                }\n                return result;\n            }\n        }\n        return undefined;\n    }\n    getAttributeInt(name) {\n        const attr = this.attributes[name];\n        if (attr !== undefined) {\n            let result = attr.i;\n            if (Long.isLong(result)) {\n                result = result.toNumber();\n            }\n            return result;\n        }\n        return undefined;\n    }\n    getAttributeFloat(name) {\n        const attr = this.attributes[name];\n        if (attr !== undefined) {\n            const result = attr.f;\n            return result;\n        }\n        return undefined;\n    }\n    getAttributeFloats(name) {\n        const attr = this.attributes[name];\n        if (attr !== undefined) {\n            const result = attr.floats;\n            return result;\n        }\n        return undefined;\n    }\n    getAttributeTensor(name) {\n        const attr = this.attributes[name];\n        if (attr !== undefined) {\n            const result = attr.t;\n            return result;\n        }\n        return undefined;\n    }\n    toValues(tensor) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (!(tensor instanceof CPUTensor)) {\n                console.warn('Tensor for values not on CPU, need to transfer!');\n                tensor = yield toCPU(tensor);\n            }\n            const sc = tensor;\n            const values = new Array(sc.size);\n            for (let i = 0; i < sc.size; i++) {\n                values[i] = sc.get(i);\n            }\n            return values;\n        });\n    }\n    toCPU() {\n        return __awaiter(this, void 0, void 0, function* () { });\n    }\n    toWASM() {\n        return __awaiter(this, void 0, void 0, function* () { });\n    }\n    toGPU() {\n        return __awaiter(this, void 0, void 0, function* () { });\n    }\n}\n//# sourceMappingURL=node.js.map","import { CPUTensor } from '../tensor/cpu/tensor';\nimport { TENSOR_FLOAT, TENSOR_INT64 } from './definitions';\n// eslint-disable-next-line node/no-extraneous-import\nimport Long from 'long';\nimport { getSize } from '../util/shape';\nexport function createTensor(tensorProto, castFloats = false) {\n    if (tensorProto.segment !== undefined && tensorProto.segment !== null) {\n        throw new Error('Handling of tensor proto segment not yet implemented');\n    }\n    let shape = tensorProto.dims;\n    if (shape === undefined || shape === null) {\n        throw new Error('Tensor shape must be specified');\n    }\n    for (let i = 0; i < shape.length; i++) {\n        if (Long.isLong(shape[i])) {\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            shape[i] = shape[i].toNumber();\n        }\n    }\n    if (shape.length === 0) {\n        shape = [1];\n    }\n    const size = getSize(shape);\n    if (tensorProto.dataType === TENSOR_FLOAT) {\n        if (tensorProto.floatData && tensorProto.floatData.length > 0) {\n            return new CPUTensor(shape, tensorProto.floatData);\n        }\n        else if (tensorProto.rawData && tensorProto.rawData.length > 0) {\n            const buffer = tensorProto.rawData.buffer.slice(tensorProto.rawData.byteOffset, tensorProto.rawData.byteOffset + tensorProto.rawData.byteLength);\n            const values = new Float32Array(buffer);\n            return new CPUTensor(shape, values, castFloats ? 'float16' : 'float32');\n        }\n        else if (size === 0) {\n            return new CPUTensor(shape);\n        }\n        else {\n            throw new Error('Cant process float tensor without float or raw data');\n        }\n    }\n    else if (tensorProto.dataType === TENSOR_INT64) {\n        if (tensorProto.rawData && tensorProto.rawData.length > 0) {\n            const values = new Int32Array(tensorProto.rawData.length / 8);\n            for (let i = 0; i < tensorProto.rawData.length; i += 8) {\n                const value = Long.fromBytesLE(Array.from(tensorProto.rawData.slice(i, i + 8))).toNumber();\n                values[i / 8] = value;\n            }\n            return new CPUTensor(shape, values, 'int32');\n        }\n        else {\n            throw new Error('Cant process int64 tensor without raw data');\n        }\n    }\n    else {\n        throw new Error(`Handling of tensor type ${tensorProto.dataType} not yet implemented`);\n    }\n}\n//# sourceMappingURL=util.js.map","// Attribute types\nexport const ATTRIBUTE_UNDEFINED = 0;\nexport const ATTRIBUTE_FLOAT = 1; // Float32\nexport const ATTRIBUTE_INT = 2; // Int64\nexport const ATTRIBUTE_STRING = 3;\nexport const ATTRIBUTE_TENSOR = 4;\nexport const ATTRIBUTE_GRAPH = 5;\nexport const ATTRIBUTE_SPARSE_TENSOR = 11;\nexport const ATTRIBUTE_FLOATS = 6;\nexport const ATTRIBUTE_INTS = 7;\nexport const ATTRIBUTE_STRINGS = 8;\nexport const ATTRIBUTE_TENSORS = 9;\nexport const ATTRIBUTE_GRAPHS = 10;\nexport const ATTRIBUTE_SPARSE_TENSORS = 12;\n// Tensor types\nexport const TENSOR_FLOAT = 1; // float  (32 bits)\nexport const TENSOR_UINT8 = 2; // uint8_t\nexport const TENSOR_INT8 = 3; // int8_t\nexport const TENSOR_UINT16 = 4; // uint16_t\nexport const TENSOR_INT16 = 5; // int16_t\nexport const TENSOR_INT32 = 6; // int32_t\nexport const TENSOR_INT64 = 7; // int64_t\nexport const TENSOR_STRING = 8; // string\nexport const TENSOR_BOOL = 9; // bool\nexport const TENSOR_FLOAT16 = 10;\nexport const TENSOR_DOUBLE = 11;\nexport const TENSOR_UINT32 = 12;\nexport const TENSOR_UINT64 = 13;\nexport const TENSOR_COMPLEX64 = 14; // complex with float32 real and imaginary components\nexport const TENSOR_COMPLEX128 = 15; // complex with float64 real and imaginary components\n// Non-IEEE floating-point format based on IEEE754 single-precision\n// floating-point number truncated to 16 bits.\n// This format has 1 sign bit, 8 exponent bits, and 7 mantissa bits.\nexport const TENSOR_BFLOAT16 = 16;\n//# sourceMappingURL=definitions.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { Variable } from '../../autograd';\nimport { toCPU, toGPU, toWASM } from '../../util/convert';\nimport { OnnxNode } from '../node';\nimport { createTensor } from '../util';\nexport class ConstantNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        const tensor = this.getAttributeTensor('value');\n        if (tensor !== undefined && tensor !== null) {\n            this.tensor = createTensor(tensor);\n            if (mode === 'train' && this.tensor !== undefined) {\n                this.tensor = new Variable(this.tensor);\n            }\n        }\n        else {\n            throw new Error('Constant needs tensor value, but attribute \"value\" was not defined');\n        }\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.tensor !== undefined) {\n                return [this.tensor];\n            }\n            throw new Error('Constant without tensor value doesnt work');\n        });\n    }\n    toCPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.tensor !== undefined) {\n                this.tensor = yield toCPU(this.tensor);\n            }\n        });\n    }\n    toWASM() {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.tensor !== undefined) {\n                this.tensor = yield toWASM(this.tensor);\n            }\n        });\n    }\n    toGPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.tensor !== undefined) {\n                this.tensor = yield toGPU(this.tensor);\n            }\n        });\n    }\n    getType() {\n        return 'Constant';\n    }\n    delete() {\n        if (this.tensor !== undefined) {\n            this.tensor.delete();\n        }\n    }\n}\n//# sourceMappingURL=constant.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { Variable } from '../../../autograd';\nimport { toCPU, toGPU, toWASM } from '../../../util/convert';\nimport { OnnxNode } from '../../node';\nexport class ConvNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode, kernel, bias, activation) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        const autoPad = this.getAttributeString('autoPad');\n        if (autoPad !== undefined) {\n            throw new Error('Autopad in conv not supported yet');\n        }\n        if (activation === undefined) {\n            activation = 'id';\n        }\n        this.activation = activation;\n        this.group = this.getAttributeInt('group') || 1;\n        this.dilations = this.getAttributeInts('dilations');\n        this.pads = this.getAttributeInts('pads');\n        this.strides = this.getAttributeInts('strides');\n        this.kernel = kernel;\n        this.bias = bias;\n        if (mode === 'train' && this.kernel !== undefined) {\n            this.kernel = new Variable(this.kernel);\n        }\n        if (mode === 'train' && this.bias !== undefined) {\n            this.bias = new Variable(this.bias);\n        }\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const w = this.kernel !== undefined ? this.kernel : inputs[1];\n            const b = inputs.length > 2 ? inputs[2] : this.bias;\n            return [\n                x.conv(w, b, this.dilations, this.group, this.pads, this.strides, this.activation),\n            ];\n        });\n    }\n    getDilations(rank) {\n        if (this.dilations !== undefined) {\n            return this.dilations;\n        }\n        return new Array(rank).fill(1);\n    }\n    getPads(rank) {\n        if (this.pads !== undefined) {\n            return this.pads;\n        }\n        return new Array(rank * 2).fill(0);\n    }\n    getStrides(rank) {\n        if (this.strides !== undefined) {\n            return this.strides;\n        }\n        return new Array(rank).fill(1);\n    }\n    getType() {\n        return 'Conv';\n    }\n    toCPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.kernel !== undefined) {\n                this.kernel = yield toCPU(this.kernel);\n            }\n            if (this.bias !== undefined) {\n                this.bias = yield toCPU(this.bias);\n            }\n        });\n    }\n    toWASM() {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.kernel !== undefined) {\n                this.kernel = yield toWASM(this.kernel);\n            }\n            if (this.bias !== undefined) {\n                this.bias = yield toWASM(this.bias);\n            }\n        });\n    }\n    toGPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.kernel !== undefined) {\n                this.kernel = yield toGPU(this.kernel);\n            }\n            if (this.bias !== undefined) {\n                this.bias = yield toGPU(this.bias);\n            }\n        });\n    }\n    delete() {\n        if (this.kernel !== undefined) {\n            this.kernel.delete();\n        }\n        if (this.bias !== undefined) {\n            this.bias.delete();\n        }\n    }\n}\n//# sourceMappingURL=conv.js.map","export class Optimization {\n}\nexport class SequenceOptimization extends Optimization {\n    constructor(nodeTypes) {\n        super();\n        this.nodeTypes = nodeTypes;\n    }\n    findApplications(model) {\n        const results = [];\n        const nodes = model.getNodes();\n        for (const nodeId in Object.keys(nodes)) {\n            const node = nodes[nodeId];\n            if (node !== undefined && node.getType() === this.nodeTypes[0]) {\n                const app = this.checkApplication(model, nodeId);\n                if (app !== undefined) {\n                    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n                    results.push(app);\n                }\n            }\n        }\n        return results;\n    }\n    checkApplication(model, nodeId) {\n        const nodes = model.getNodes();\n        const nodeSeq = [nodeId];\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        let lastNode = nodes[nodeId];\n        const nodeInstances = [lastNode];\n        for (let i = 1; i < this.nodeTypes.length; i++) {\n            const nextNodeId = model.getNodeWithInput(lastNode.outputs[0]);\n            if (nextNodeId !== undefined) {\n                const nextNode = nodes[nextNodeId];\n                if (nextNode.getType() === this.nodeTypes[i]) {\n                    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n                    nodeSeq.push(nextNodeId);\n                    lastNode = nextNode;\n                    nodeInstances.push(lastNode);\n                }\n                else {\n                    return undefined;\n                }\n            }\n            else {\n                return undefined;\n            }\n        }\n        if (this.canApply(nodeInstances)) {\n            return nodeSeq;\n        }\n        else {\n            return undefined;\n        }\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    canApply(nodes) {\n        return true;\n    }\n}\n//# sourceMappingURL=optimization.js.map","import { ConvNode } from '../nodes/conv/conv';\nimport { SequenceOptimization } from './optimization';\nexport class ConvBatchNorm extends SequenceOptimization {\n    constructor() {\n        super(['Conv', 'BatchNormalization']);\n    }\n    apply(nodes, resolveConstant, constants, onnxVersion) {\n        const conv = nodes[0];\n        const batchNorm = nodes[1];\n        const kernelConv = resolveConstant(conv.inputs[1]);\n        const biasConv = resolveConstant(conv.inputs[2]);\n        const scaleBN = resolveConstant(batchNorm.inputs[1]);\n        const biasBN = resolveConstant(batchNorm.inputs[2]);\n        const meanBN = resolveConstant(batchNorm.inputs[3]);\n        const varianceBN = resolveConstant(batchNorm.inputs[4]);\n        const varSqrt = varianceBN.add(batchNorm.epsTensor).sqrt();\n        const scale = scaleBN.divide(varSqrt);\n        varSqrt.delete();\n        const bias = biasBN.subtract(meanBN.multiply(scale));\n        const newShape = [\n            ...scale.getShape(),\n            ...new Array(kernelConv.getShape().length - scale.getShape().length).fill(1),\n        ];\n        const newKernel = kernelConv.multiply(scale.reshape(newShape, false));\n        let newBias = bias;\n        if (biasConv !== undefined) {\n            const scaledBias = biasConv.multiply(scale);\n            newBias = newBias.add(scaledBias);\n            scaledBias.delete();\n        }\n        return new ConvNode(Object.entries(conv.attributes).map(x => x[1]), [conv.inputs[0]], batchNorm.outputs, constants, onnxVersion, conv.mode, newKernel, newBias);\n    }\n}\n//# sourceMappingURL=convBatchnorm.js.map","import { ConvNode } from '../nodes/conv/conv';\nimport { SequenceOptimization } from './optimization';\nexport class ConvRelu extends SequenceOptimization {\n    constructor() {\n        super(['Conv', 'Relu']);\n    }\n    apply(nodes, resolveConstant, constants, onnxVersion) {\n        const conv = nodes[0];\n        const relu = nodes[1];\n        return new ConvNode(Object.entries(conv.attributes).map(x => x[1]), conv.inputs, relu.outputs, constants, onnxVersion, conv.mode, conv.kernel, conv.bias, 'relu');\n    }\n}\n//# sourceMappingURL=convRelu.js.map","import { ConvNode } from '../nodes/conv/conv';\nimport { SequenceOptimization } from './optimization';\nexport class ConvRelu6 extends SequenceOptimization {\n    constructor() {\n        super(['Conv', 'Clip']);\n    }\n    apply(nodes, resolveConstant, constants, onnxVersion) {\n        const conv = nodes[0];\n        const clip = nodes[1];\n        return new ConvNode(Object.entries(conv.attributes).map(x => x[1]), conv.inputs, clip.outputs, constants, onnxVersion, conv.mode, conv.kernel, conv.bias, 'relu6');\n    }\n    canApply(nodes) {\n        const clip = nodes[1];\n        return clip.min === 0 && clip.max === 6;\n    }\n}\n//# sourceMappingURL=convRelu6.js.map","import { ConvBatchNorm } from './convBatchnorm';\nimport { ConvRelu } from './convRelu';\nimport { ConvRelu6 } from './convRelu6';\nexport const defaultOptimizations = [\n    new ConvBatchNorm(),\n    new ConvRelu(),\n    new ConvRelu6(),\n];\n//# sourceMappingURL=default.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../../node';\nexport class BinaryNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, name, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.name = name;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.onnxVersion < 13 && this.onnxVersion >= 7) {\n                const a = inputs[0];\n                const b = inputs[1];\n                return [this.compute(a, b)];\n            }\n            throw new Error(`${this.name} not implemented for onnx version ${this.onnxVersion}`);\n        });\n    }\n    getType() {\n        return this.name;\n    }\n    delete() { }\n}\n//# sourceMappingURL=binaryNode.js.map","import { BinaryNode } from './binaryNode';\nexport class AddNode extends BinaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Add', mode);\n    }\n    compute(a, b) {\n        return a.add(b);\n    }\n}\n//# sourceMappingURL=add.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { Variable } from '../../autograd/variable';\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { toCPU, toGPU, toWASM } from '../../util/convert';\nimport { OnnxNode } from '../node';\nexport class BatchNormalizationNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.epsilon = this.getAttributeFloat('epsilon') || 1e-5;\n        this.momentum = this.getAttributeFloat('momentum') || 0.9;\n        this.epsTensor = new CPUTensor([1], [this.epsilon]);\n        if (mode === 'train') {\n            this.epsTensor = new Variable(this.epsTensor);\n        }\n        //TODO: Handle lower onnxversions here\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            let scale = inputs[1];\n            let B = inputs[2];\n            let mean = inputs[3];\n            let variance = inputs[4];\n            //TODO: Handle lower onnx versions here\n            const C = scale.getShape()[0];\n            const newShape = [1, C, ...new Array(x.getShape().length - 2).fill(1)];\n            scale = scale.reshape(newShape, false);\n            B = B.reshape(newShape, false);\n            mean = mean.reshape(newShape, false);\n            variance = variance.reshape(newShape, false);\n            const result = x.normalize(mean, variance, this.epsilon, scale, B);\n            return [result];\n        });\n    }\n    getType() {\n        return 'BatchNormalization';\n    }\n    toCPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            this.epsTensor = yield toCPU(this.epsTensor);\n        });\n    }\n    toWASM() {\n        return __awaiter(this, void 0, void 0, function* () {\n            this.epsTensor = yield toWASM(this.epsTensor);\n        });\n    }\n    toGPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            this.epsTensor = yield toGPU(this.epsTensor);\n        });\n    }\n    delete() {\n        this.epsTensor.delete();\n    }\n}\n//# sourceMappingURL=batchNormalization.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class CastNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        //@ts-ignore\n        this.to = this.getAttributeString('to');\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            // TODO: Convert to to correct dtype\n            return [x.cast(this.to)];\n        });\n    }\n    getType() {\n        return 'Cast';\n    }\n    delete() { }\n}\n//# sourceMappingURL=cast.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../../node';\nexport class UnaryNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, name, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.name = name;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            return [this.compute(x)];\n        });\n    }\n    getType() {\n        return this.name;\n    }\n    delete() { }\n}\n//# sourceMappingURL=unaryNode.js.map","import { UnaryNode } from './unaryNode';\nexport class CeilNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Ceil', mode);\n    }\n    compute(x) {\n        return x.ceil();\n    }\n}\n//# sourceMappingURL=ceil.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class ClipNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        if (onnxVersion < 11) {\n            //@ts-ignore\n            this.min = this.getAttributeFloat('min');\n            //@ts-ignore\n            this.max = this.getAttributeFloat('max');\n        }\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            if (this.onnxVersion < 11) {\n                return [x.clip(this.min, this.max)];\n            }\n            else {\n                const min = inputs.length > 1 ? inputs[1] : undefined;\n                const max = inputs.length > 2 ? inputs[2] : undefined;\n                if (min === undefined && max === undefined) {\n                    return [x.copy()];\n                }\n                throw new Error('Clip with onnx version >= 11 not yet implemented');\n            }\n        });\n    }\n    getType() {\n        return 'Clip';\n    }\n    delete() { }\n}\n//# sourceMappingURL=clip.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../../node';\nexport class ConcatNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        if (onnxVersion < 13) {\n            //@ts-ignore\n            this.axis = this.getAttributeInt('axis');\n        }\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (inputs.length > 2) {\n                // This logging seems to slow down the operation more than the operation itself\n                //console.warn(`Concat with more than 2 tensors is currently slow. Doing concat with ${inputs.length} tensors`);\n            }\n            if (this.onnxVersion < 13 && this.axis !== undefined) {\n                let result = inputs[0];\n                for (let i = 1; i < inputs.length; i++) {\n                    const newRes = result.concat(inputs[i], this.axis);\n                    if (i > 1) {\n                        result.delete();\n                    }\n                    result = newRes;\n                }\n                return [result];\n            }\n            throw new Error(`Concat not implemented for onnx version ${this.onnxVersion}`);\n        });\n    }\n    getType() {\n        return 'Concat';\n    }\n    delete() { }\n}\n//# sourceMappingURL=concat.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { getSize } from '../../util/shape';\nimport { OnnxNode } from '../node';\nimport { createTensor } from '../util';\n// This does not support gradients right now, mainly because\n// the forward pass needs to directly access the constant value\nexport class ConstantOfShapeNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        if (onnxVersion < 11) {\n            const tensor = this.getAttributeTensor('value');\n            if (tensor !== null && tensor !== undefined) {\n                this.tensor = createTensor(tensor);\n            }\n        }\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const _shape = inputs[0];\n            if (this.onnxVersion < 11 && this.tensor !== undefined) {\n                if (!(_shape instanceof CPUTensor)) {\n                    throw new Error('ConstantOfShape needs cpu tensor as shape tensor');\n                }\n                const shape = new Array(_shape.size);\n                for (let i = 0; i < _shape.size; i++) {\n                    shape[i] = _shape.get(i);\n                }\n                const size = getSize(shape);\n                const values = new Float32Array(size).fill(this.tensor.get(0));\n                return [new CPUTensor(shape, values, this.tensor.dtype)];\n            }\n            throw new Error(`ConstantOfShape not implemented for onnx version ${this.onnxVersion}`);\n        });\n    }\n    getType() {\n        return 'ConstantOfShape';\n    }\n    delete() {\n        if (this.tensor !== undefined) {\n            this.tensor.delete();\n        }\n    }\n}\n//# sourceMappingURL=constantOfShape.js.map","import { BinaryNode } from './binaryNode';\nexport class DivNode extends BinaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Div', mode);\n    }\n    compute(a, b) {\n        return a.divide(b);\n    }\n}\n//# sourceMappingURL=div.js.map","import { UnaryNode } from './unaryNode';\nexport class ExpNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Exp', mode);\n    }\n    compute(x) {\n        return x.exp();\n    }\n}\n//# sourceMappingURL=exp.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { OnnxNode } from '../node';\nexport class ExpandNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.onnxVersion < 13) {\n                const tensor = inputs[0];\n                const _shape = inputs[1];\n                if (!(_shape instanceof CPUTensor)) {\n                    throw new Error('Expand needs cpu tensor as shape tensor');\n                }\n                const shape = new Array(_shape.size);\n                for (let i = 0; i < _shape.size; i++) {\n                    shape[i] = _shape.get(i);\n                }\n                return [tensor.expand(shape)];\n            }\n            throw new Error(`Expand not yet implemented for onnx version ${this.onnxVersion}`);\n        });\n    }\n    getType() {\n        return 'Expand';\n    }\n    delete() { }\n}\n//# sourceMappingURL=expand.js.map","import { UnaryNode } from './unaryNode';\nexport class FloorNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Floor', mode);\n    }\n    compute(x) {\n        return x.floor();\n    }\n}\n//# sourceMappingURL=floor.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { OnnxNode } from '../node';\nexport class GatherNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.axis = this.getAttributeInt('axis') || 0;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const indices = inputs[1];\n            if (!(indices instanceof CPUTensor)) {\n                throw new Error('Gather requires CPU tensor for the indices');\n            }\n            return [x.gather(this.axis, indices)];\n        });\n    }\n    getType() {\n        return 'Gather';\n    }\n    delete() { }\n}\n//# sourceMappingURL=gather.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class GemmNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.alpha = this.getAttributeFloat('alpha') || 1.0;\n        this.beta = this.getAttributeFloat('beta') || 1.0;\n        const transA = this.getAttributeInt('transA');\n        const transB = this.getAttributeInt('transB');\n        this.transA = transA === 1;\n        this.transB = transB === 1;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const a = inputs[0];\n            const b = inputs[1];\n            const c = inputs[2];\n            return [a.gemm(b, this.transA, this.transB, this.alpha, c, this.beta)];\n        });\n    }\n    getType() {\n        return 'Gemm';\n    }\n    delete() { }\n}\n//# sourceMappingURL=gemm.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { glContext } from '../../../tensor/gpu/gl';\nimport { OnnxNode } from '../../node';\nexport class InstanceNormalizationNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.epsilon = this.getAttributeFloat('epsilon') || 1e-5;\n        //TODO: Handle onnx versions < 6 here\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            let scale = inputs[1];\n            let B = inputs[2];\n            const dataRank = x.getShape().length - 2;\n            const C = scale.getShape()[0];\n            const newShape = [1, C, ...new Array(dataRank).fill(1)];\n            scale = scale.reshape(newShape, false);\n            B = B.reshape(newShape, false);\n            const reduceAxes = new Array(x.getShape().length - 2);\n            for (let i = 0; i < dataRank; i++) {\n                reduceAxes[i] = i + 2;\n            }\n            const mean = x.reduceMean(reduceAxes, true);\n            glContext.flush();\n            const diff = x.subtract(mean);\n            glContext.flush();\n            const variance = diff.reduceMeanSquare(reduceAxes, true);\n            glContext.flush();\n            const result = x.normalize(mean, variance, this.epsilon, scale, B);\n            mean.delete();\n            diff.delete();\n            variance.delete();\n            return [result];\n        });\n    }\n    getType() {\n        return 'InstanceNormalization';\n    }\n    delete() { }\n}\n//# sourceMappingURL=instanceNormalization.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class MatMulNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const A = inputs[0];\n            const B = inputs[1];\n            if (this.onnxVersion < 13) {\n                if (A.getShape().length !== B.getShape().length) {\n                    throw new Error('Automatic broadcasting in MatMul not supported yet');\n                }\n                return [A.gemm(B)];\n            }\n            throw new Error(`Matmul with onnx version ${this.onnxVersion} not yet implemented`);\n        });\n    }\n    getType() {\n        return 'MatMul';\n    }\n    delete() { }\n}\n//# sourceMappingURL=matMul.js.map","import { BinaryNode } from './binaryNode';\nexport class MulNode extends BinaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Mul', mode);\n    }\n    compute(a, b) {\n        return a.multiply(b);\n    }\n}\n//# sourceMappingURL=mul.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../../node';\nexport class PadNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.padMode = (this.getAttributeString('mode') || 'constant');\n        //@ts-ignore\n        this.pads = this.getAttributeInts('pads');\n        this.value = this.getAttributeFloat('value') || 0;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.onnxVersion < 11) {\n                return [inputs[0].pad(this.pads, this.padMode, this.value)];\n            }\n            throw new Error(`Pad not implemented for onnx version ${this.onnxVersion}`);\n        });\n    }\n    getType() {\n        return 'Pad';\n    }\n    delete() { }\n}\n//# sourceMappingURL=pad.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../../node';\nexport class ReduceNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, name, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.axes = this.getAttributeInts('axes');\n        const keep = this.getAttributeInt('keepdims');\n        this.keepDims = keep === 1 || keep === undefined;\n        this.name = name;\n    }\n    getAxes(input) {\n        if (this.axes !== undefined) {\n            return this.axes;\n        }\n        else {\n            const rank = input.getShape().length;\n            const res = new Array(rank);\n            for (let i = 0; i < rank; i++) {\n                res[i] = i;\n            }\n            return res;\n        }\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.onnxVersion < 13) {\n                return [this.calc(inputs[0])];\n            }\n            throw new Error(`${this.name} is not implemented for onnx version ${this.onnxVersion}`);\n        });\n    }\n    getType() {\n        return this.name;\n    }\n    delete() { }\n}\n//# sourceMappingURL=reduceNode.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceMaxNode extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceMax', mode);\n    }\n    calc(input) {\n        return input.max(this.axes, this.keepDims);\n    }\n}\n//# sourceMappingURL=reduceMax.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceMeanNode extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceMean', mode);\n    }\n    calc(input) {\n        return input.reduceMean(this.axes, this.keepDims);\n    }\n}\n//# sourceMappingURL=reduceMean.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceSumNode extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceSum', mode);\n    }\n    calc(input) {\n        return input.sum(this.axes, this.keepDims);\n    }\n}\n//# sourceMappingURL=reduceSum.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceSumSquareNode extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceSumSquare', mode);\n    }\n    calc(input) {\n        return input.sumSquare(this.axes, this.keepDims);\n    }\n}\n//# sourceMappingURL=reduceSumSquare.js.map","import { ClipNode } from './clip';\nexport class ReluNode extends ClipNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.max = undefined;\n        this.min = 0;\n    }\n    getType() {\n        return 'Relu';\n    }\n}\n//# sourceMappingURL=relu.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { OnnxNode } from '../node';\nexport class ReshapeNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const shape = inputs[1];\n            if (!(shape instanceof CPUTensor)) {\n                throw new Error('Reshape only works with CPU tensor as shape tensor');\n            }\n            if (this.onnxVersion < 13) {\n                const _shape = new Array(shape.size);\n                for (let i = 0; i < shape.size; i++) {\n                    _shape[i] = shape.get(i);\n                }\n                return [x.reshape(_shape)];\n            }\n            throw new Error(`Reshape with onnx version ${this.onnxVersion} not yet implemented`);\n        });\n    }\n    getType() {\n        return 'Reshape';\n    }\n    delete() { }\n}\n//# sourceMappingURL=reshape.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { OnnxNode } from '../node';\nexport class ShapeNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.onnxVersion < 13) {\n                const a = inputs[0];\n                const shape = a.getShape();\n                return [new CPUTensor([shape.length], [...shape], 'int32')];\n            }\n            throw new Error(`Shape not implemented for onnx version ${this.onnxVersion}`);\n        });\n    }\n    getType() {\n        return 'Shape';\n    }\n    delete() { }\n}\n//# sourceMappingURL=shape.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class SliceNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.axes = this.getAttributeInts('axes');\n        this.starts = this.getAttributeInts('starts');\n        this.ends = this.getAttributeInts('ends');\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.onnxVersion < 10) {\n                if (this.starts === undefined || this.ends === undefined) {\n                    throw new Error('Slice with onnx version < 10 needs starts and ends defined as attributes');\n                }\n                const x = inputs[0];\n                return [x.slice(this.starts, this.ends, this.axes)];\n            }\n            else {\n                const x = inputs[0];\n                const starts = inputs[1];\n                const ends = inputs[2];\n                const axes = inputs[3];\n                const steps = inputs[4];\n                const startValues = yield this.toValues(starts);\n                const endValues = yield this.toValues(ends);\n                const axesValues = axes !== undefined ? yield this.toValues(axes) : undefined;\n                const stepValues = steps !== undefined ? yield this.toValues(steps) : undefined;\n                return [x.slice(startValues, endValues, axesValues, stepValues)];\n            }\n        });\n    }\n    getType() {\n        return 'Slice';\n    }\n    delete() { }\n}\n//# sourceMappingURL=slice.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class SoftmaxNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        //@ts-ignore\n        this.axis = this.getAttributeInt('axis');\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const shapeX = x.getShape();\n            let ax = this.axis;\n            if (ax === undefined) {\n                if (this.onnxVersion < 13) {\n                    ax = 1;\n                }\n                else {\n                    ax = shapeX.length - 1;\n                }\n            }\n            const sh1 = shapeX.slice(0, ax).reduce((x, y) => x * y, 1);\n            const reshaped = x.reshape([sh1, -1], false);\n            const max = reshaped.max(1, true);\n            const normalized = reshaped.subtract(max);\n            const exp = normalized.exp();\n            const sum = exp.sum(1, true);\n            const result = exp.divide(sum);\n            max.delete();\n            normalized.delete();\n            exp.delete();\n            sum.delete();\n            return [result.reshape(shapeX, false)];\n        });\n    }\n    getType() {\n        return 'Softmax';\n    }\n    delete() { }\n}\n//# sourceMappingURL=softmax.js.map","import { BinaryNode } from './binaryNode';\nexport class SubNode extends BinaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Sub', mode);\n    }\n    compute(a, b) {\n        return a.subtract(b);\n    }\n}\n//# sourceMappingURL=sub.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { OnnxNode } from '../node';\nexport class TileNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const repeats = inputs[1];\n            if (!(repeats instanceof CPUTensor)) {\n                throw new Error('Tile only works with CPU tensor as repeats');\n            }\n            if (this.onnxVersion < 13 && this.onnxVersion >= 6) {\n                const _repeats = new Array(repeats.size);\n                for (let i = 0; i < repeats.size; i++) {\n                    _repeats[i] = repeats.get(i);\n                }\n                return [x.repeat(_repeats)];\n            }\n            throw new Error(`Tile with onnx version ${this.onnxVersion} not yet implemented`);\n        });\n    }\n    getType() {\n        return 'Tile';\n    }\n    delete() { }\n}\n//# sourceMappingURL=tile.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class TransposeNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.permutation = this.getAttributeInts('perm');\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const a = inputs[0];\n            return [a.transpose(this.permutation)];\n        });\n    }\n    getType() {\n        return 'Transpose';\n    }\n    delete() { }\n}\n//# sourceMappingURL=transpose.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class UnsqueezeNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        if (onnxVersion < 13) {\n            this.axes = this.getAttributeInts('axes');\n        }\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            if (this.onnxVersion < 13 && this.axes !== undefined) {\n                const currShape = x.getShape();\n                const newShape = [];\n                let axIx = 0;\n                for (let i = 0; i < currShape.length; i++) {\n                    if (axIx < this.axes.length && this.axes[axIx] === i) {\n                        newShape.push(1);\n                        axIx++;\n                    }\n                    newShape.push(currShape[i]);\n                }\n                if (this.axes[this.axes.length - 1] === currShape.length) {\n                    newShape.push(1);\n                }\n                return [x.reshape(newShape)];\n            }\n            throw new Error(`Unsqueeze with onnx version ${this.onnxVersion} not yet implemented`);\n        });\n    }\n    getType() {\n        return 'Unsqueeze';\n    }\n    delete() { }\n}\n//# sourceMappingURL=unsqueeze.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { toCPU } from '../../util/convert';\nimport { OnnxNode } from '../node';\nexport class UpsampleNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        //@ts-ignore\n        this.sampleMode = this.getAttributeString('mode');\n        if (this.sampleMode !== 'nearest') {\n            throw new Error('Upsampling only supported with nearest neighbor sampling');\n        }\n        if (this.onnxVersion < 9) {\n            const scales = this.getAttributeFloats('scales');\n            if (scales !== undefined && scales !== null) {\n                this.scales = scales;\n            }\n            else {\n                throw new Error(`Upsample node with onnx version ${this.onnxVersion} is missing scales attribute`);\n            }\n        }\n    }\n    getScales(scale) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (this.onnxVersion < 9) {\n                return this.scales;\n            }\n            if (!(scale instanceof CPUTensor)) {\n                console.warn('Scales tensor for upsample not on CPU, need to transfer!');\n                scale = yield toCPU(scale);\n            }\n            const sc = scale;\n            const scales = new Array(sc.size);\n            for (let i = 0; i < sc.size; i++) {\n                scales[i] = sc.get(i);\n            }\n            return scales;\n        });\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const scale = inputs[1];\n            const scales = yield this.getScales(scale);\n            return [x.upsample(scales)];\n        });\n    }\n    getType() {\n        return 'Upsample';\n    }\n    delete() { }\n}\n//# sourceMappingURL=upsample.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../../node';\nexport class GlobalAveragePoolNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const axes = new Array(x.getShape().length - 2);\n            for (let i = 0; i < x.getShape().length - 2; i++) {\n                axes[i] = i + 2;\n            }\n            return [x.reduceMean(axes, true)];\n        });\n    }\n    getType() {\n        return 'GlobalAveragePool';\n    }\n    delete() { }\n}\n//# sourceMappingURL=globalAveragePool.js.map","import { UnaryNode } from './unaryNode';\nexport class AbsNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Abs', mode);\n    }\n    compute(x) {\n        return x.abs();\n    }\n}\n//# sourceMappingURL=abs.js.map","import { UnaryNode } from './unaryNode';\nexport class LogNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Log', mode);\n    }\n    compute(x) {\n        return x.log();\n    }\n}\n//# sourceMappingURL=log.js.map","import { UnaryNode } from './unaryNode';\nexport class SqrtNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Sqrt', mode);\n    }\n    compute(x) {\n        return x.sqrt();\n    }\n}\n//# sourceMappingURL=sqrt.js.map","import { UnaryNode } from './unaryNode';\nexport class SignNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Sign', mode);\n    }\n    compute(x) {\n        return x.sign();\n    }\n}\n//# sourceMappingURL=sign.js.map","import { UnaryNode } from './unaryNode';\nexport class TanNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Tan', mode);\n    }\n    compute(x) {\n        return x.tan();\n    }\n}\nexport class ATanNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Atan', mode);\n    }\n    compute(x) {\n        return x.atan();\n    }\n}\nexport class TanHNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'TanH', mode);\n    }\n    compute(x) {\n        return x.tanh();\n    }\n}\nexport class ATanHNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ATanH', mode);\n    }\n    compute(x) {\n        return x.atanh();\n    }\n}\n//# sourceMappingURL=tan.js.map","import { UnaryNode } from './unaryNode';\nexport class CosNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Cos', mode);\n    }\n    compute(x) {\n        return x.cos();\n    }\n}\nexport class ACosNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Acos', mode);\n    }\n    compute(x) {\n        return x.acos();\n    }\n}\nexport class CosHNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Cosh', mode);\n    }\n    compute(x) {\n        return x.cosh();\n    }\n}\nexport class ACosHNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'AcosH', mode);\n    }\n    compute(x) {\n        return x.acosh();\n    }\n}\n//# sourceMappingURL=cos.js.map","import { UnaryNode } from './unaryNode';\nexport class SinNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Sin', mode);\n    }\n    compute(x) {\n        return x.sin();\n    }\n}\nexport class ASinNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Asin', mode);\n    }\n    compute(x) {\n        return x.asin();\n    }\n}\nexport class SinHNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Sinh', mode);\n    }\n    compute(x) {\n        return x.sinh();\n    }\n}\nexport class ASinHNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Asinh', mode);\n    }\n    compute(x) {\n        return x.asinh();\n    }\n}\n//# sourceMappingURL=sin.js.map","import { UnaryNode } from './unaryNode';\nexport class SigmoidNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Sigmoid', mode);\n    }\n    compute(x) {\n        return x.sigmoid();\n    }\n}\n//# sourceMappingURL=sigmoid.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceMinNode extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceMin', mode);\n    }\n    calc(input) {\n        return input.min(this.axes, this.keepDims);\n    }\n}\n//# sourceMappingURL=reduceMin.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceProdNode extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceProd', mode);\n    }\n    calc(input) {\n        return input.product(this.axes, this.keepDims);\n    }\n}\n//# sourceMappingURL=reduceProd.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceLogSumNode extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceLogSum', mode);\n    }\n    calc(input) {\n        return input.reduceLogSum(this.axes, this.keepDims);\n    }\n}\n//# sourceMappingURL=reduceLogSum.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceLogSumExpNode extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceLogSumExp', mode);\n    }\n    calc(input) {\n        return input.reduceLogSumExp(this.axes, this.keepDims);\n    }\n}\n//# sourceMappingURL=reduceLogSumExp.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceL2Node extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceL2', mode);\n    }\n    calc(input) {\n        const sumSquare = input.sumSquare(this.axes, this.keepDims);\n        const result = sumSquare.sqrt();\n        sumSquare.delete();\n        return result;\n    }\n}\n//# sourceMappingURL=reduceL2.js.map","import { ReduceNode } from './reduceNode';\nexport class ReduceL1Node extends ReduceNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'ReduceL1', mode);\n    }\n    calc(input) {\n        const abs = input.abs();\n        const result = abs.sum(this.axes, this.keepDims);\n        abs.delete();\n        return result;\n    }\n}\n//# sourceMappingURL=reduceL1.js.map","import { BinaryNode } from './binaryNode';\nexport class PowNode extends BinaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Pow', mode);\n    }\n    compute(a, b) {\n        return a.power(b);\n    }\n}\n//# sourceMappingURL=pow.js.map","import { UnaryNode } from './unaryNode';\nexport class IdentityNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Identity', mode);\n    }\n    compute(x) {\n        return x.copy();\n    }\n}\n//# sourceMappingURL=identity.js.map","import { UnaryNode } from './unaryNode';\nexport class HardSigmoidNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'HardSigmoid', mode);\n        this.alpha = this.getAttributeFloat('alpha') || 0.2;\n        this.beta = this.getAttributeFloat('beta') || 0.5;\n    }\n    compute(x) {\n        return x.hardSigmoid(this.alpha, this.beta);\n    }\n}\n//# sourceMappingURL=hardSigmoid.js.map","import { UnaryNode } from './unaryNode';\nexport class NegNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Neg', mode);\n    }\n    compute(x) {\n        return x.negate();\n    }\n}\n//# sourceMappingURL=neg.js.map","import { UnaryNode } from './unaryNode';\nexport class ReciprocalNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Reciprocal', mode);\n    }\n    compute(x) {\n        return x.powerScalar(-1, 1);\n    }\n}\n//# sourceMappingURL=reciprocal.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class SqueezeNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const tensor = inputs[0];\n            return [tensor.squeeze()];\n        });\n    }\n    getType() {\n        return 'Squeeze';\n    }\n    delete() { }\n}\n//# sourceMappingURL=squeeze.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { getSize } from '../../util/shape';\nimport { OnnxNode } from '../node';\nexport class SizeNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const a = inputs[0];\n            const shape = a.getShape();\n            const size = getSize(shape);\n            return [new CPUTensor([1], [size], 'uint32')];\n        });\n    }\n    getType() {\n        return 'Size';\n    }\n    delete() { }\n}\n//# sourceMappingURL=size.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class LeakyReluNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.alpha = this.getAttributeFloat('alpha') || 0.01;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const tensor = inputs[0];\n            const below = tensor.clip(undefined, 0);\n            const above = tensor.clip(0, undefined);\n            const result = below.add(above, this.alpha);\n            below.delete();\n            above.delete();\n            return [result];\n        });\n    }\n    delete() { }\n    getType() {\n        return 'LeakyRelu';\n    }\n}\n//# sourceMappingURL=leakyRelu.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class EluNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.alpha = this.getAttributeFloat('alpha') || 1.0;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const tensor = inputs[0];\n            const below = tensor.clip(undefined, 0);\n            const above = tensor.clip(0, undefined);\n            const belowExp = below.exp();\n            below.delete();\n            const b = belowExp.addMultiplyScalar(this.alpha, -this.alpha);\n            belowExp.delete();\n            const result = b.add(above);\n            b.delete();\n            above.delete();\n            return [result];\n        });\n    }\n    delete() { }\n    getType() {\n        return 'Elu';\n    }\n}\n//# sourceMappingURL=elu.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class PReluNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const tensor = inputs[0];\n            const slope = inputs[1];\n            const below = tensor.clip(undefined, 0);\n            const above = tensor.clip(0, undefined);\n            const belowRes = below.multiply(slope);\n            below.delete();\n            const result = belowRes.add(above);\n            belowRes.delete();\n            above.delete();\n            return [result];\n        });\n    }\n    delete() { }\n    getType() {\n        return 'PRElu';\n    }\n}\n//# sourceMappingURL=prelu.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class SeluNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.alpha = this.getAttributeFloat('alpha') || 1.67326319217681884765625;\n        this.gamma = this.getAttributeFloat('gamma') || 1.05070102214813232421875;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const tensor = inputs[0];\n            const below = tensor.clip(undefined, 0);\n            const above = tensor.clip(0, undefined);\n            const belowExp = below.exp();\n            below.delete();\n            const b = belowExp.addMultiplyScalar(this.gamma * this.alpha, -this.alpha * this.gamma);\n            belowExp.delete();\n            const result = b.add(above, 1.0, this.gamma);\n            b.delete();\n            above.delete();\n            return [result];\n        });\n    }\n    delete() { }\n    getType() {\n        return 'Selu';\n    }\n}\n//# sourceMappingURL=selu.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class FlattenNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        const axis = this.getAttributeInt('axis');\n        if (axis !== null) {\n            this.axis = axis;\n        }\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const tensor = inputs[0];\n            return [tensor.flatten(this.axis)];\n        });\n    }\n    getType() {\n        return 'Flatten';\n    }\n    delete() { }\n}\n//# sourceMappingURL=flatten.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class SoftplusNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const exp = x.exp();\n            const p1 = exp.addScalar(1);\n            exp.delete();\n            const result = p1.log();\n            p1.delete();\n            return [result];\n        });\n    }\n    getType() {\n        return 'Softplus';\n    }\n    delete() { }\n}\n//# sourceMappingURL=softplus.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class SoftsignNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const x = inputs[0];\n            const abs = x.abs();\n            const p1 = abs.addScalar(1);\n            abs.delete();\n            const result = x.divide(p1);\n            p1.delete();\n            return [result];\n        });\n    }\n    getType() {\n        return 'Softsign';\n    }\n    delete() { }\n}\n//# sourceMappingURL=softsign.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../../node';\nexport class SumNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (inputs.length > 2) {\n                // This logging seems to slow down the operation more than the operation itself\n                //console.warn(`Sum with more than 2 tensors is currently slow. Doing concat with ${inputs.length} tensors`);\n            }\n            let result = inputs[0];\n            for (let i = 1; i < inputs.length; i++) {\n                const newRes = result.add(inputs[i]);\n                if (i > 1) {\n                    result.delete();\n                }\n                result = newRes;\n            }\n            return [result];\n        });\n    }\n    getType() {\n        return 'Sum';\n    }\n    delete() { }\n}\n//# sourceMappingURL=sum.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../../node';\nexport class MeanNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (inputs.length > 2) {\n                // This logging seems to slow down the operation more than the operation itself\n                //console.warn(`Sum with more than 2 tensors is currently slow. Doing concat with ${inputs.length} tensors`);\n            }\n            let result = inputs[0];\n            for (let i = 1; i < inputs.length; i++) {\n                const newRes = result.add(inputs[i]);\n                if (i > 1) {\n                    result.delete();\n                }\n                result = newRes;\n            }\n            const mean = result.multiplyScalar(1 / inputs.length);\n            if (inputs.length > 2) {\n                result.delete();\n            }\n            return [mean];\n        });\n    }\n    getType() {\n        return 'Mean';\n    }\n    delete() { }\n}\n//# sourceMappingURL=mean.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { OnnxNode } from '../node';\nexport class CeluNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.alpha = this.getAttributeFloat('alpha') || 1.0;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const tensor = inputs[0];\n            const below = tensor.clip(undefined, 0);\n            const above = tensor.clip(0, undefined);\n            // max(0,x) + min(0,alpha*(exp(x/alpha)-1))\n            const belowDiv = below.multiplyScalar(1 / this.alpha);\n            below.delete();\n            const belowExp = belowDiv.exp();\n            belowDiv.delete();\n            const belowRes = belowExp.addMultiplyScalar(this.alpha, -this.alpha);\n            belowExp.delete();\n            const result = belowRes.add(above);\n            belowRes.delete();\n            above.delete();\n            return [result];\n        });\n    }\n    delete() { }\n    getType() {\n        return 'Celu';\n    }\n}\n//# sourceMappingURL=celu.js.map","import { UnaryNode } from './unaryNode';\nexport class RoundNode extends UnaryNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, 'Round', mode);\n    }\n    compute(x) {\n        return x.round();\n    }\n}\n//# sourceMappingURL=round.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { CPUTensor } from '../../tensor/cpu/tensor';\nimport { GPUTensor } from '../../tensor/gpu/tensor';\nimport { WASMTensor } from '../../tensor/wasm/tensor';\nimport { OnnxNode } from '../node';\nexport class RangeNode extends OnnxNode {\n    constructor(attributes, inputs, outputs, constants, onnxVersion, mode) {\n        super(attributes, inputs, outputs, constants, onnxVersion, mode);\n        this.backend = 'CPU';\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const start = inputs[0];\n            const limit = inputs[1];\n            const delta = inputs[2];\n            const startValue = (yield this.toValues(start))[0];\n            const limitValue = (yield this.toValues(limit))[0];\n            const deltaValue = (yield this.toValues(delta))[0];\n            if (this.backend === 'CPU') {\n                return [CPUTensor.range(startValue, limitValue, deltaValue)];\n            }\n            else if (this.backend === 'WASM') {\n                return [WASMTensor.range(startValue, limitValue, deltaValue)];\n            }\n            else {\n                return [GPUTensor.range(startValue, limitValue, deltaValue)];\n            }\n        });\n    }\n    delete() { }\n    getType() {\n        return 'Range';\n    }\n    toCPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            this.backend = 'CPU';\n        });\n    }\n    toWASM() {\n        return __awaiter(this, void 0, void 0, function* () {\n            this.backend = 'WASM';\n        });\n    }\n    toGPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            this.backend = 'GPU';\n        });\n    }\n}\n//# sourceMappingURL=range.js.map","import { AddNode } from './nodes/binary/add';\nimport { BatchNormalizationNode } from './nodes/batchNormalization';\nimport { CastNode } from './nodes/cast';\nimport { CeilNode } from './nodes/unary/ceil';\nimport { ClipNode } from './nodes/clip';\nimport { ConcatNode } from './nodes/nary/concat';\nimport { ConstantNode } from './nodes/constant';\nimport { ConstantOfShapeNode } from './nodes/constantOfShape';\nimport { ConvNode } from './nodes/conv/conv';\nimport { DivNode } from './nodes/binary/div';\nimport { ExpNode } from './nodes/unary/exp';\nimport { ExpandNode } from './nodes/expand';\nimport { FloorNode } from './nodes/unary/floor';\nimport { GatherNode } from './nodes/gather';\nimport { GemmNode } from './nodes/gemm';\nimport { InstanceNormalizationNode } from './nodes/conv/instanceNormalization';\nimport { MatMulNode } from './nodes/matMul';\nimport { MulNode } from './nodes/binary/mul';\nimport { PadNode } from './nodes/conv/pad';\nimport { ReduceMaxNode } from './nodes/reduce/reduceMax';\nimport { ReduceMeanNode } from './nodes/reduce/reduceMean';\nimport { ReduceSumNode } from './nodes/reduce/reduceSum';\nimport { ReduceSumSquareNode } from './nodes/reduce/reduceSumSquare';\nimport { ReluNode } from './nodes/relu';\nimport { ReshapeNode } from './nodes/reshape';\nimport { ShapeNode } from './nodes/shape';\nimport { SliceNode } from './nodes/slice';\nimport { SoftmaxNode } from './nodes/softmax';\nimport { SubNode } from './nodes/binary/sub';\nimport { TileNode } from './nodes/tile';\nimport { TransposeNode } from './nodes/transpose';\nimport { UnsqueezeNode } from './nodes/unsqueeze';\nimport { UpsampleNode } from './nodes/upsample';\nimport { GlobalAveragePoolNode } from './nodes/conv/globalAveragePool';\nimport { AbsNode } from './nodes/unary/abs';\nimport { LogNode } from './nodes/unary/log';\nimport { SqrtNode } from './nodes/unary/sqrt';\nimport { SignNode } from './nodes/unary/sign';\nimport { ATanHNode, ATanNode, TanHNode, TanNode } from './nodes/unary/tan';\nimport { ACosHNode, ACosNode, CosHNode, CosNode } from './nodes/unary/cos';\nimport { ASinHNode, ASinNode, SinHNode, SinNode } from './nodes/unary/sin';\nimport { SigmoidNode } from './nodes/unary/sigmoid';\nimport { ReduceMinNode } from './nodes/reduce/reduceMin';\nimport { ReduceProdNode } from './nodes/reduce/reduceProd';\nimport { ReduceLogSumNode } from './nodes/reduce/reduceLogSum';\nimport { ReduceLogSumExpNode } from './nodes/reduce/reduceLogSumExp';\nimport { ReduceL2Node } from './nodes/reduce/reduceL2';\nimport { ReduceL1Node } from './nodes/reduce/reduceL1';\nimport { PowNode } from './nodes/binary/pow';\nimport { IdentityNode } from './nodes/unary/identity';\nimport { HardSigmoidNode } from './nodes/unary/hardSigmoid';\nimport { NegNode } from './nodes/unary/neg';\nimport { ReciprocalNode } from './nodes/unary/reciprocal';\nimport { SqueezeNode } from './nodes/squeeze';\nimport { SizeNode } from './nodes/size';\nimport { LeakyReluNode } from './nodes/leakyRelu';\nimport { EluNode } from './nodes/elu';\nimport { PReluNode } from './nodes/prelu';\nimport { SeluNode } from './nodes/selu';\nimport { FlattenNode } from './nodes/flatten';\nimport { SoftplusNode } from './nodes/softplus';\nimport { SoftsignNode } from './nodes/softsign';\nimport { SumNode } from './nodes/nary/sum';\nimport { MeanNode } from './nodes/nary/mean';\nimport { CeluNode } from './nodes/celu';\nimport { RoundNode } from './nodes/unary/round';\nimport { RangeNode } from './nodes/range';\nexport const nodeResolve = {\n    Conv: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ConvNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    BatchNormalization: (attributes, inputs, outputs, constants, onnxVersion, mode) => new BatchNormalizationNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Clip: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ClipNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Add: (attributes, inputs, outputs, constants, onnxVersion, mode) => new AddNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Pow: (attributes, inputs, outputs, constants, onnxVersion, mode) => new PowNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceMean: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceMeanNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Gemm: (attributes, inputs, outputs, constants, onnxVersion, mode) => new GemmNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Constant: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ConstantNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Reshape: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReshapeNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Range: (attributes, inputs, outputs, constants, onnxVersion, mode) => new RangeNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Flatten: (attributes, inputs, outputs, constants, onnxVersion, mode) => new FlattenNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Tile: (attributes, inputs, outputs, constants, onnxVersion, mode) => new TileNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    MatMul: (attributes, inputs, outputs, constants, onnxVersion, mode) => new MatMulNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Exp: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ExpNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Log: (attributes, inputs, outputs, constants, onnxVersion, mode) => new LogNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Sqrt: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SqrtNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Sign: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SignNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceSum: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceSumNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceProd: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceProdNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceMax: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceMaxNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceMin: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceMinNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceSumSquare: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceSumSquareNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceL2: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceL2Node(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceL1: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceL1Node(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceLogSum: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceLogSumNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ReduceLogSumExp: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReduceLogSumExpNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Sub: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SubNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Mul: (attributes, inputs, outputs, constants, onnxVersion, mode) => new MulNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Div: (attributes, inputs, outputs, constants, onnxVersion, mode) => new DivNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Unsqueeze: (attributes, inputs, outputs, constants, onnxVersion, mode) => new UnsqueezeNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Concat: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ConcatNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Sum: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SumNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Mean: (attributes, inputs, outputs, constants, onnxVersion, mode) => new MeanNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    ConstantOfShape: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ConstantOfShapeNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Expand: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ExpandNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Squeeze: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SqueezeNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    InstanceNormalization: (attributes, inputs, outputs, constants, onnxVersion, mode) => new InstanceNormalizationNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Pad: (attributes, inputs, outputs, constants, onnxVersion, mode) => new PadNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Relu: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReluNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    LeakyRelu: (attributes, inputs, outputs, constants, onnxVersion, mode) => new LeakyReluNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Elu: (attributes, inputs, outputs, constants, onnxVersion, mode) => new EluNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    PRelu: (attributes, inputs, outputs, constants, onnxVersion, mode) => new PReluNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Selu: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SeluNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Celu: (attributes, inputs, outputs, constants, onnxVersion, mode) => new CeluNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Shape: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ShapeNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Size: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SizeNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Gather: (attributes, inputs, outputs, constants, onnxVersion, mode) => new GatherNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Cast: (attributes, inputs, outputs, constants, onnxVersion, mode) => new CastNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Floor: (attributes, inputs, outputs, constants, onnxVersion, mode) => new FloorNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Ceil: (attributes, inputs, outputs, constants, onnxVersion, mode) => new CeilNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Round: (attributes, inputs, outputs, constants, onnxVersion, mode) => new RoundNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Abs: (attributes, inputs, outputs, constants, onnxVersion, mode) => new AbsNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Neg: (attributes, inputs, outputs, constants, onnxVersion, mode) => new NegNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Reciprocal: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ReciprocalNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Identity: (attributes, inputs, outputs, constants, onnxVersion, mode) => new IdentityNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Sigmoid: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SigmoidNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    HardSigmoid: (attributes, inputs, outputs, constants, onnxVersion, mode) => new HardSigmoidNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Sin: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SinNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Cos: (attributes, inputs, outputs, constants, onnxVersion, mode) => new CosNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Tan: (attributes, inputs, outputs, constants, onnxVersion, mode) => new TanNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Asin: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ASinNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Acos: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ACosNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Atan: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ATanNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Sinh: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SinHNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Cosh: (attributes, inputs, outputs, constants, onnxVersion, mode) => new CosHNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Tanh: (attributes, inputs, outputs, constants, onnxVersion, mode) => new TanHNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Asinh: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ASinHNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Acosh: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ACosHNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Atanh: (attributes, inputs, outputs, constants, onnxVersion, mode) => new ATanHNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Slice: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SliceNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Upsample: (attributes, inputs, outputs, constants, onnxVersion, mode) => new UpsampleNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Transpose: (attributes, inputs, outputs, constants, onnxVersion, mode) => new TransposeNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Softmax: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SoftmaxNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Softplus: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SoftplusNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    Softsign: (attributes, inputs, outputs, constants, onnxVersion, mode) => new SoftsignNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n    GlobalAveragePool: (attributes, inputs, outputs, constants, onnxVersion, mode) => new GlobalAveragePoolNode(attributes, inputs, outputs, constants, onnxVersion, mode),\n};\n//# sourceMappingURL=resolve.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\n// eslint-disable-next-line node/no-extraneous-import\nimport Long from 'long';\nimport { onnx } from 'onnx-proto';\nimport { Variable } from '../autograd/variable';\nimport { Module } from '../model/module';\nimport { glContext } from '../tensor/gpu/gl';\nimport { toCPU, toGPU, toWASM } from '../util/convert';\nimport { ConstantNode } from './nodes/constant';\nimport { defaultOptimizations } from './optimizations/default';\nimport { nodeResolve } from './resolve';\nimport { createTensor } from './util';\nexport class OnnxModel extends Module {\n    /**\n     * Builds a new onnx model\n     *\n     * @param buffer Onnx model\n     * @param args Optional arguments for the model\n     */\n    constructor(buffer, args) {\n        super();\n        this.inputSet = new Set();\n        this.nodes = {};\n        this.nodeIds = [];\n        this.defaultReady = [];\n        this.intermediaries = {};\n        this.constants = {};\n        this.nodeIdCounter = 10000;\n        if (args === undefined) {\n            args = {};\n        }\n        this.noConvertConstants = new Set(args.noConvertConstants !== undefined ? args.noConvertConstants : []);\n        this.noConvertNodes = new Set(args.noConvertNodes !== undefined ? args.noConvertNodes : []);\n        this.mode = args.mode || 'inference';\n        this.precision = args.precision || 32;\n        let arr;\n        if (buffer instanceof ArrayBuffer) {\n            arr = new Uint8Array(buffer);\n        }\n        else {\n            arr = buffer;\n        }\n        this.modelProto = onnx.ModelProto.decode(arr);\n        let ver = this.modelProto.opsetImport[0].version;\n        if (Long.isLong(ver)) {\n            ver = ver.toNumber();\n        }\n        this.version = ver;\n        //@ts-ignore\n        this.inputs = this.modelProto.graph.input;\n        for (let i = 0; i < this.inputs.length; i++) {\n            this.inputSet.add(this.inputs[i].name);\n        }\n        //@ts-ignore\n        this.outputs = this.modelProto.graph.output.map(x => x.name);\n        //@ts-ignore\n        this.initializer(this.modelProto.graph.initializer);\n        this.initNodes(this.modelProto);\n    }\n    initNodes(modelProto) {\n        //@ts-ignore\n        for (let i = 0; i < modelProto.graph.node.length; i++) {\n            //@ts-ignore\n            const nodeData = modelProto.graph.node[i];\n            //@ts-ignore\n            const cls = nodeResolve[nodeData.opType];\n            if (cls === undefined) {\n                throw new Error(`Node operator ${nodeData.opType} can not be resolved`);\n            }\n            const attributes = nodeData.attribute || [];\n            const inputs = nodeData.input || [];\n            const outputs = nodeData.output || [];\n            const node = cls(attributes, inputs, outputs, this.constants, this.version, this.mode);\n            this.nodes[i] = node;\n            this.nodeIds.push(i);\n            for (let j = 0; j < inputs.length; j++) {\n                const input = inputs[j];\n                if (this.intermediaries[input] === undefined) {\n                    this.intermediaries[input] = {\n                        to: [],\n                        deletable: true,\n                    };\n                }\n                this.intermediaries[input].to.push(i);\n            }\n            if (node.variableInputs === 0) {\n                this.defaultReady.push(i);\n            }\n            if (nodeData.opType === 'Constant') {\n                //@ts-ignore\n                if (this.intermediaries[nodeData.output[0]] === undefined) {\n                    //@ts-ignore\n                    this.intermediaries[nodeData.output[0]] = {\n                        to: [],\n                        deletable: false,\n                    };\n                }\n                else {\n                    //@ts-ignore\n                    this.intermediaries[nodeData.output[0]].deletable = false;\n                }\n            }\n        }\n        for (const nodeId of this.nodeIds) {\n            this.nodes[nodeId].initialize(name => this.resolveConstant(name));\n        }\n    }\n    initializer(initializer) {\n        for (let i = 0; i < initializer.length; i++) {\n            const tensorProto = initializer[i];\n            let tensor = createTensor(tensorProto, this.precision === 16);\n            if (this.mode === 'train') {\n                tensor = new Variable(tensor);\n            }\n            //@ts-ignore\n            this.constants[tensorProto.name] = tensor;\n        }\n    }\n    /**\n     * Do a forward pass for the specified inputs\n     *\n     * @param wait Number of milliseconds to wait between each layer. This\n     *             is especially useful, if your model is complex and\n     *             you dont want your model to block your whole application.\n     * @param returnIntermediary return after the given intermediary result\n     *                           has been computed.\n     */\n    forward(inputs, wait) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const intermediaryRes = {};\n            const nodes = {};\n            for (const i of this.nodeIds) {\n                nodes[i] = {\n                    variableInputs: 0,\n                };\n            }\n            const nodesReady = [...this.defaultReady];\n            this.initializeForward(inputs, intermediaryRes, nodes, nodesReady);\n            while (nodesReady.length > 0) {\n                const nodeId = nodesReady.shift();\n                //@ts-ignore\n                const node = this.nodes[nodeId];\n                const { inputs, toDelete } = this.getInputsToNode(node, intermediaryRes);\n                let outputs;\n                try {\n                    outputs = yield node.forward(inputs);\n                }\n                catch (e) {\n                    console.error(`Error occurred in node ${nodeId} with inputs ${node.inputs} from nodes ${node.inputs.map((x) => this.getNodeWithOutput(x))}`);\n                    throw e;\n                }\n                glContext.flush();\n                this.propagateResults(node, intermediaryRes, outputs, nodes, nodesReady);\n                for (let i = 0; i < toDelete.length; i++) {\n                    if (!this.inputSet.has(toDelete[i])) {\n                        const inter = intermediaryRes[toDelete[i]];\n                        inter.value.delete();\n                        delete intermediaryRes[toDelete[i]];\n                    }\n                }\n                if (wait !== undefined) {\n                    yield new Promise(resolve => {\n                        setTimeout(resolve, wait);\n                    });\n                }\n            }\n            const outputs = [];\n            for (let i = 0; i < this.outputs.length; i++) {\n                outputs.push(intermediaryRes[this.outputs[i]].value);\n            }\n            return outputs;\n        });\n    }\n    initializeForward(inputs, intermediaryRes, nodes, nodesReady) {\n        for (let i = 0; i < inputs.length; i++) {\n            //@ts-ignore\n            intermediaryRes[this.inputs[i].name] = {\n                value: inputs[i],\n                used: 0,\n            };\n            //@ts-ignore\n            const inter = this.intermediaries[this.inputs[i].name];\n            for (let j = 0; j < inter.to.length; j++) {\n                const id = inter.to[j];\n                nodes[id].variableInputs++;\n                if (nodes[id].variableInputs === this.nodes[id].variableInputs) {\n                    nodesReady.push(id);\n                    delete nodes[id];\n                }\n            }\n        }\n    }\n    getInputsToNode(node, intermediaryRes) {\n        const inputs = [];\n        const toDelete = [];\n        for (let i = 0; i < node.inputs.length; i++) {\n            const input = node.inputs[i];\n            if (this.constants[input] !== undefined) {\n                inputs.push(this.constants[input]);\n            }\n            else {\n                const inter = intermediaryRes[input];\n                inter.used++;\n                if (inter.used >= this.intermediaries[input].to.length &&\n                    this.intermediaries[input].deletable) {\n                    toDelete.push(input);\n                }\n                inputs.push(inter.value);\n            }\n        }\n        return { inputs, toDelete };\n    }\n    propagateResults(node, intermediaryRes, outputs, nodes, nodesReady) {\n        for (let i = 0; i < node.outputs.length; i++) {\n            const output = node.outputs[i];\n            intermediaryRes[output] = {\n                value: outputs[i],\n                used: 0,\n            };\n            const inter = this.intermediaries[output];\n            if (inter !== undefined) {\n                for (let j = 0; j < inter.to.length; j++) {\n                    const id = inter.to[j];\n                    nodes[id].variableInputs++;\n                    if (nodes[id].variableInputs === this.nodes[id].variableInputs) {\n                        nodesReady.push(id);\n                        delete nodes[id];\n                    }\n                }\n            }\n        }\n    }\n    /**\n     * Transfer the model to the CPU\n     */\n    toCPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            for (const i in this.constants) {\n                if (!this.noConvertConstants.has(i)) {\n                    this.constants[i] = yield toCPU(this.constants[i]);\n                }\n            }\n            for (const i of this.nodeIds) {\n                if (!this.noConvertNodes.has(i)) {\n                    yield this.nodes[i].toCPU();\n                }\n            }\n        });\n    }\n    /**\n     * Transfer the model to WASM\n     */\n    toWASM() {\n        return __awaiter(this, void 0, void 0, function* () {\n            for (const i in this.constants) {\n                if (!this.noConvertConstants.has(i)) {\n                    this.constants[i] = yield toWASM(this.constants[i]);\n                }\n            }\n            for (const i of this.nodeIds) {\n                if (!this.noConvertNodes.has(i)) {\n                    yield this.nodes[i].toWASM();\n                }\n            }\n        });\n    }\n    /**\n     * Transfer the model to the GPU\n     */\n    toGPU() {\n        return __awaiter(this, void 0, void 0, function* () {\n            for (const i in this.constants) {\n                if (!this.noConvertConstants.has(i)) {\n                    this.constants[i] = yield toGPU(this.constants[i]);\n                }\n            }\n            for (const i of this.nodeIds) {\n                if (!this.noConvertNodes.has(i)) {\n                    yield this.nodes[i].toGPU();\n                }\n            }\n        });\n    }\n    /**\n     * Optimize the model.\n     */\n    optimize() {\n        for (const optimization of defaultOptimizations) {\n            //@ts-ignore\n            const applications = optimization.findApplications(this);\n            for (const nodeIds of applications) {\n                const nodes = nodeIds.map(x => this.nodes[x]);\n                const newNode = optimization.apply(nodes, name => this.resolveConstant(name), this.constants, this.version);\n                const outputs = new Set(newNode.outputs);\n                for (const nodeId of nodeIds) {\n                    this.removeNode(nodeId, outputs);\n                }\n                this.insertNode(newNode);\n            }\n        }\n        this.prune();\n    }\n    prune(intermediariesToDelete) {\n        // eslint-disable-next-line no-constant-condition\n        while (true) {\n            const nodesToDelete = this.pruneIntermediaries(intermediariesToDelete);\n            intermediariesToDelete = [];\n            if (nodesToDelete.size > 0) {\n                nodesToDelete.forEach(id => {\n                    const interToDelete = this.removeNode(id, new Set());\n                    intermediariesToDelete = intermediariesToDelete === null || intermediariesToDelete === void 0 ? void 0 : intermediariesToDelete.concat(interToDelete);\n                });\n            }\n            else {\n                break;\n            }\n        }\n    }\n    pruneIntermediaries(intermediariesToDelete) {\n        const nodesToDelete = new Set();\n        if (intermediariesToDelete === undefined) {\n            intermediariesToDelete = [];\n        }\n        for (let i = 0; i < intermediariesToDelete.length; i++) {\n            const id = intermediariesToDelete[i];\n            const nodeOutputId = this.getNodeWithOutput(id);\n            if (nodeOutputId !== undefined) {\n                nodesToDelete.add(nodeOutputId);\n            }\n            const nodeInputId = this.getNodeWithInput(id);\n            if (nodeInputId !== undefined) {\n                nodesToDelete.add(nodeInputId);\n            }\n        }\n        for (const id in this.intermediaries) {\n            const intermediary = this.intermediaries[id];\n            if (intermediary.to.length === 0 &&\n                this.outputs.find(x => x === id) === undefined) {\n                intermediariesToDelete.push(id);\n                const nodeOutputId = this.getNodeWithOutput(id);\n                if (nodeOutputId !== undefined) {\n                    nodesToDelete.add(nodeOutputId);\n                }\n                const nodeInputId = this.getNodeWithInput(id);\n                if (nodeInputId !== undefined) {\n                    nodesToDelete.add(nodeInputId);\n                }\n            }\n        }\n        for (const id of intermediariesToDelete) {\n            delete this.intermediaries[id];\n        }\n        return nodesToDelete;\n    }\n    removeNode(nodeId, preserveIntermediaries) {\n        const node = this.nodes[nodeId];\n        for (const input of node.inputs) {\n            if (this.intermediaries[input] !== undefined) {\n                this.intermediaries[input].to = this.intermediaries[input].to.filter(x => x.toString() !== nodeId.toString());\n            }\n        }\n        const intermediariesToDelete = [];\n        if (!preserveIntermediaries.has(node.outputs[0])) {\n            intermediariesToDelete.push(node.outputs[0]);\n        }\n        this.nodeIds = this.nodeIds.filter(x => x.toString() !== nodeId.toString());\n        this.nodes[nodeId].delete();\n        delete this.nodes[nodeId];\n        this.defaultReady = this.defaultReady.filter(x => x !== nodeId);\n        return intermediariesToDelete;\n    }\n    insertNode(node) {\n        const id = this.nodeIdCounter++;\n        this.nodeIds.push(id);\n        this.nodes[id] = node;\n        for (const input of node.inputs) {\n            this.intermediaries[input].to.push(id);\n        }\n    }\n    // Utility functions\n    getNodeWithOutput(output) {\n        for (const id of this.nodeIds) {\n            if (this.nodes[id].outputs.findIndex(x => x === output) !== -1) {\n                return id;\n            }\n        }\n        return undefined;\n    }\n    getNodeWithInput(output) {\n        for (const id of this.nodeIds) {\n            if (this.nodes[id].inputs.findIndex(x => x === output) !== -1) {\n                return id;\n            }\n        }\n        return undefined;\n    }\n    resolveConstant(name) {\n        if (this.constants[name] !== undefined) {\n            return this.constants[name];\n        }\n        const nodeIdOut = this.getNodeWithOutput(name);\n        //@ts-ignore\n        const nodeOut = this.nodes[nodeIdOut];\n        if (nodeOut instanceof ConstantNode) {\n            return nodeOut.tensor;\n        }\n        return undefined;\n    }\n    getNodes() {\n        return this.nodes;\n    }\n    /**\n     * Deletes the model\n     *\n     * This will release the memory/framebuffers (depending on the backend)\n     */\n    delete() {\n        for (const c in this.constants) {\n            this.constants[c].delete();\n        }\n        for (const nodeId of this.nodeIds) {\n            this.nodes[nodeId].delete();\n        }\n    }\n    getSubModules() {\n        const modules = super.getSubModules();\n        for (const nodeId of this.nodeIds) {\n            modules.push(this.nodes[nodeId]);\n        }\n        return modules;\n    }\n    getParameters() {\n        const parameters = super.getParameters();\n        for (const c in this.constants) {\n            if (this.constants[c] instanceof Variable) {\n                parameters.push(this.constants[c]);\n            }\n        }\n        return parameters;\n    }\n}\n//# sourceMappingURL=model.js.map","var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nimport { Variable } from '../autograd/variable';\nimport { CPUTensor } from '../tensor/cpu/tensor';\nimport { normal } from '../util/math';\nimport { Module } from './module';\n/**\n * Linear layer calculates y=xW + b\n *\n * W is initialized with Xavier initialization, while the bias is\n * initialized to zeros\n */\nexport class Linear extends Module {\n    /**\n     * Creates a linear layer\n     * @param dimIn Feature dimension of the input\n     * @param dimOut Feature dimension of the output\n     * @param bias Wether a bias should be added or not. Defaults to true\n     */\n    constructor(dimIn, dimOut, bias) {\n        super();\n        bias = bias === undefined ? true : bias;\n        const weightVals = normal(dimIn * dimOut, 0, 2 / (dimIn + dimOut));\n        const tensor = new CPUTensor([dimIn, dimOut], weightVals);\n        this.weights = new Variable(tensor);\n        if (bias) {\n            const biasVals = new Array(dimOut).fill(0);\n            const tensorBias = new CPUTensor([1, dimOut], biasVals);\n            this.bias = new Variable(tensorBias);\n        }\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return [inputs[0].gemm(this.weights, false, false, 1, this.bias)];\n        });\n    }\n}\n/**\n * Rectified linear unit, calculates y = max(x,0)\n */\nexport class Relu extends Module {\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return [inputs[0].clip(0)];\n        });\n    }\n}\n/**\n * Sequence of modules. Passes the input sequentially into the specified modules\n */\nexport class Sequential extends Module {\n    constructor(modules) {\n        super();\n        this.modules = modules;\n    }\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            let x = inputs;\n            for (let i = 0; i < this.modules.length; i++) {\n                const oldX = x;\n                x = yield this.modules[i].forward(x);\n                if (this.mode === 'inference' && i > 0) {\n                    for (let j = 0; j < oldX.length; j++) {\n                        oldX[j].delete();\n                    }\n                }\n            }\n            return x;\n        });\n    }\n    getSubModules() {\n        const modules = super.getSubModules();\n        return modules.concat(this.modules);\n    }\n}\n/**\n * Dictionary of modules. Use this if you want to store submodules in a dictionary\n */\nexport class ModuleDict extends Module {\n    constructor(modules = {}) {\n        super();\n        this.modules = modules;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            throw new Error('Module dict does not support forward');\n        });\n    }\n    getSubModules() {\n        const modules = [];\n        for (const k in this.modules) {\n            modules.push(this.modules[k]);\n        }\n        return modules;\n    }\n    get(key) {\n        return this.modules[key];\n    }\n    set(key, module) {\n        this.modules[key] = module;\n    }\n}\n/**\n * List of modules. Use this if you want to store submodules in a list\n */\nexport class ModuleList extends Module {\n    constructor(modules = []) {\n        super();\n        this.modules = modules;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    forward(inputs) {\n        return __awaiter(this, void 0, void 0, function* () {\n            throw new Error('Module list does not support forward');\n        });\n    }\n    getSubModules() {\n        return this.modules;\n    }\n    get(index) {\n        return this.modules[index];\n    }\n    set(index, module) {\n        this.modules[index] = module;\n    }\n    push(module) {\n        this.modules.push(module);\n    }\n    pop() {\n        return this.modules.pop();\n    }\n}\n//# sourceMappingURL=basic.js.map","/**\n * Base class for all gradient based model optimizers.\n */\nexport class Optimizer {\n    /**\n     * Construct a new optimizer for a particular model\n     */\n    constructor(model) {\n        this.model = model;\n        this.parameters = model.getParameters();\n    }\n    /**\n     * Zeros all gradients of the model parameters. This should be called\n     * after each optimization step, when the gradients are no longer needed.\n     */\n    zeroGrads() {\n        for (const parameter of this.parameters) {\n            if (parameter.grad !== undefined) {\n                parameter.grad.delete();\n                parameter.grad = undefined;\n            }\n        }\n    }\n}\n//# sourceMappingURL=optimizer.js.map","import { getSize } from '../../../util/shape';\nimport { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { Operation } from '../../../ops/gpu/operation';\nimport { Dispatcher } from '../../../ops/gpu/dispatcher';\nimport { gpuConstructor } from '../../../tensor/gpu/tensor';\nexport class UpdateMomentOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('beta1')} float beta1;\n    ${this.getVarModifier('beta2')} float beta2;\n    ${this.getVarModifier('t')} int t;\n    `;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    void main() {\n      initVars();\n\n      int pos = coordinateToPos(uv, widthOutput, heightOutput);\n\n      float m1 = getValueAtPos(pos, widthMoments, heightMoments, Moments);\n      float m2 = getValueAtPos(pos+2, widthMoments, heightMoments, Moments);\n      float grad = getValueAtPos(pos/4, widthGrad, heightGrad, Grad);\n\n      m1 = beta1*m1 + (1.0-beta1)*grad;\n      m2 = beta2*m2 + (1.0-beta2)*grad*grad;\n\n      float m1Corr = m1/(1.0-pow(beta1, float(t)));\n      float m2Corr = m2/(1.0-pow(beta2, float(t)));\n\n      gl_FragColor = vec4(m1,m1Corr,m2,m2Corr);\n    }\n    `;\n    }\n    getTextureNames() {\n        return ['Grad', 'Moments'];\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'beta1', type: 'float' },\n            { name: 'beta2', type: 'float' },\n            { name: 't', type: 'int' },\n        ];\n    }\n    calc(input) {\n        return this.compute(input.Moments.shape, { Grad: input.Grad, Moments: input.Moments }, {\n            beta1: input.beta1,\n            beta2: input.beta2,\n            t: input.t,\n        });\n    }\n    getOutputShape(input) {\n        return input.Moments.shape;\n    }\n    compile(info) {\n        if (info.shapeMoments !== undefined) {\n            this.maxRank = info.shapeMoments.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeGrad: input.Grad.shape,\n            widthGrad: input.Grad.memory.width,\n            heightGrad: input.Grad.memory.height,\n            shapeMoments: input.Moments.shape,\n            widthMoments: input.Moments.memory.width,\n            heightMoments: input.Moments.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            beta1: input.beta1,\n            beta2: input.beta2,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.Grad.shape}-${input.Moments.shape}-${input.beta1}-${input.beta2}`;\n    }\n}\nexport const defaultUpdateMomentsD = new Dispatcher((dtype) => new UpdateMomentOperation(gpuConstructor, dtype));\n//# sourceMappingURL=updateMoments.js.map","import { getSize } from '../../../util/shape';\nimport { defaultAllocator } from '../../../tensor/gpu/gl';\nimport { Operation } from '../../../ops/gpu/operation';\nimport { Dispatcher } from '../../../ops/gpu/dispatcher';\nimport { gpuConstructor } from '../../../tensor/gpu/tensor';\nexport class UpdateValueOperation extends Operation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n        this.maxIterations = 1000000;\n    }\n    getVariables() {\n        return `\n    ${this.getVarModifier('alpha')} float alpha;\n    ${this.getVarModifier('epsilon')} float epsilon;\n    `;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    getFragmentShader(info) {\n        return `\n    float newVal(float m1Corr, float m2Corr, float value) {\n      return value - alpha*(m1Corr/(sqrt(m2Corr)+epsilon));\n    }\n\n    void main() {\n      initVars();\n\n      int pos = coordinateToPos(uv, widthOutput, heightOutput);\n\n      vec4 result = vec4(0,0,0,0);\n\n      float m1Corr = getValueAtPos(pos*4+1, widthMoments, heightMoments, Moments);\n      float m2Corr = getValueAtPos(pos*4+3, widthMoments, heightMoments, Moments);\n      float value = getValueAtPos(pos, widthValue, heightValue, Value);\n      result.r = newVal(m1Corr, m2Corr, value);\n\n      pos++;\n      m1Corr = getValueAtPos(pos*4+1, widthMoments, heightMoments, Moments);\n      m2Corr = getValueAtPos(pos*4+3, widthMoments, heightMoments, Moments);\n      value = getValueAtPos(pos, widthValue, heightValue, Value);\n      result.g = newVal(m1Corr, m2Corr, value);\n\n      pos++;\n      m1Corr = getValueAtPos(pos*4+1, widthMoments, heightMoments, Moments);\n      m2Corr = getValueAtPos(pos*4+3, widthMoments, heightMoments, Moments);\n      value = getValueAtPos(pos, widthValue, heightValue, Value);\n      result.b = newVal(m1Corr, m2Corr, value);\n\n      pos++;\n      m1Corr = getValueAtPos(pos*4+1, widthMoments, heightMoments, Moments);\n      m2Corr = getValueAtPos(pos*4+3, widthMoments, heightMoments, Moments);\n      value = getValueAtPos(pos, widthValue, heightValue, Value);\n      result.a = newVal(m1Corr, m2Corr, value);\n\n\n      gl_FragColor = result;\n    }\n    `;\n    }\n    getTextureNames() {\n        return ['Value', 'Moments'];\n    }\n    getUniformAttrs() {\n        return [\n            { name: 'alpha', type: 'float' },\n            { name: 'epsilon', type: 'float' },\n        ];\n    }\n    calc(input) {\n        return this.compute(input.Value.shape, { Value: input.Value, Moments: input.Moments }, {\n            alpha: input.alpha,\n            epsilon: input.epsilon,\n        });\n    }\n    getOutputShape(input) {\n        return input.Value.shape;\n    }\n    compile(info) {\n        if (info.shapeMoments !== undefined) {\n            this.maxRank = info.shapeMoments.length;\n        }\n        super.compile(info);\n    }\n    getCompilationInfo(input) {\n        const outputShape = this.getOutputShape(input);\n        const outputSize = defaultAllocator.getAllocationDimensions(getSize(outputShape), this.dtype);\n        return {\n            shapeValue: input.Value.shape,\n            widthValue: input.Value.memory.width,\n            heightValue: input.Value.memory.height,\n            shapeMoments: input.Moments.shape,\n            widthMoments: input.Moments.memory.width,\n            heightMoments: input.Moments.memory.height,\n            shapeOutput: outputShape,\n            widthOutput: outputSize.width,\n            heightOutput: outputSize.height,\n            alpha: input.alpha,\n            epsilon: input.epsilon,\n        };\n    }\n    getInputInfoString(input) {\n        return `${input.Value.shape}-${input.Moments.shape}-${input.alpha}-${input.epsilon}`;\n    }\n}\nexport const defaultUpdateValueD = new Dispatcher((dtype) => new UpdateValueOperation(gpuConstructor, dtype));\n//# sourceMappingURL=updateParams.js.map","import { GPUTensor } from '../../../tensor/gpu/tensor';\nimport { Optimizer } from '../optimizer';\nimport { defaultUpdateMomentsD } from './updateMoments';\nimport { defaultUpdateValueD } from './updateParams';\n/**\n * Implements the Adam optimizer\n *\n * This is currently quite slow on the CPU and WASM backends. On the GPU\n * backend, one update step is only slightly slower than an update step of SGD\n * and will converge a lot quicker.\n */\nexport class Adam extends Optimizer {\n    constructor(model, lr = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 10e-8) {\n        super(model);\n        this.lr = lr;\n        this.beta1 = beta1;\n        this.beta2 = beta2;\n        this.epsilon = epsilon;\n        this.t = 0;\n        const params = this.parameters;\n        if (params[0].value instanceof GPUTensor) {\n            this.moments = new Array(params.length);\n            for (let i = 0; i < params.length; i++) {\n                this.moments[i] = new GPUTensor(new Array(params[i].value.size * 4).fill(0), [...params[i].getShape(), 4], params[i].value.dtype);\n            }\n        }\n        else {\n            this.moment1 = new Array(params.length).fill(undefined);\n            this.moment2 = new Array(params.length).fill(undefined);\n        }\n    }\n    step() {\n        this.t++;\n        if (this.moment1 !== undefined && this.moment2 !== undefined) {\n            for (let i = 0; i < this.parameters.length; i++) {\n                const parameter = this.parameters[i];\n                if (parameter.grad !== undefined) {\n                    const oldValue = parameter.value;\n                    const { newValue, moment1, moment2 } = this.paramStep(parameter.value, parameter.grad, this.moment1[i], this.moment2[i]);\n                    parameter.value = newValue;\n                    this.moment1[i] = moment1;\n                    this.moment2[i] = moment2;\n                    oldValue.delete();\n                }\n            }\n        }\n        else if (this.moments !== undefined) {\n            for (let i = 0; i < this.parameters.length; i++) {\n                const parameter = this.parameters[i];\n                if (parameter.grad !== undefined) {\n                    const oldValue = parameter.value;\n                    const { newValue, moments } = this.gpuParamStep(parameter.value, parameter.grad, this.moments[i]);\n                    parameter.value = newValue;\n                    this.moments[i] = moments;\n                    oldValue.delete();\n                }\n            }\n        }\n    }\n    updateMoments(grad, moment1, moment2) {\n        let moment1New;\n        if (moment1 === undefined) {\n            moment1New = grad.multiplyScalar(1 - this.beta1);\n        }\n        else {\n            const oldMoment1 = moment1;\n            moment1New = moment1.add(grad, this.beta1, 1 - this.beta1);\n            oldMoment1.delete();\n        }\n        let moment2New;\n        if (moment2 === undefined) {\n            moment2New = grad.multiply(grad, 1 - this.beta2);\n        }\n        else {\n            const gradSquared = grad.multiply(grad);\n            const oldMoment2 = moment2;\n            moment2New = moment2.add(gradSquared, this.beta2, 1 - this.beta2);\n            gradSquared.delete();\n            oldMoment2.delete();\n        }\n        return { moment1New, moment2New };\n    }\n    getCorrectedMoments(moment1, moment2) {\n        const correctMoment1 = moment1.addMultiplyScalar(1 / (1 - Math.pow(this.beta1, this.t)), 0);\n        const correctMoment2 = moment2.addMultiplyScalar(1 / (1 - Math.pow(this.beta2, this.t)), 0);\n        return { correctMoment1, correctMoment2 };\n    }\n    paramStep(value, grad, moment1, moment2) {\n        const { moment1New, moment2New } = this.updateMoments(grad, moment1, moment2);\n        // This is not 100% correct, in the original paper\n        // the epsilon occurs outside of the square root\n        // It does not make much of a difference though\n        // and is slightly faster\n        const correctMoment2 = moment2New.addMultiplyScalar(1 / (1 - Math.pow(this.beta2, this.t)), this.epsilon);\n        const moment2Sqrt = correctMoment2.sqrt();\n        correctMoment2.delete();\n        const step = moment1New.divide(moment2Sqrt, -this.lr / (1 - Math.pow(this.beta1, this.t)));\n        moment2Sqrt.delete();\n        const newValue = value.add(step);\n        step.delete();\n        return { newValue, moment1, moment2 };\n    }\n    gpuParamStep(value, grad, moments) {\n        const newMoments = defaultUpdateMomentsD.calc({\n            Grad: grad,\n            Moments: moments,\n            beta1: this.beta1,\n            beta2: this.beta2,\n            t: this.t,\n        }, value.dtype);\n        moments.delete();\n        const newValue = defaultUpdateValueD.calc({\n            Value: value,\n            Moments: newMoments,\n            alpha: this.lr,\n            epsilon: this.epsilon,\n        }, value.dtype);\n        return { newValue, moments: newMoments };\n    }\n}\n//# sourceMappingURL=Adam.js.map","import { BinaryOperation } from '../../../../ops/gpu/binary/binaryOperation';\nimport { Dispatcher } from '../../../../ops/gpu/dispatcher';\nimport { gpuConstructor } from '../../../../tensor/gpu/tensor';\nexport class BCEBackOperation extends BinaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getOp(a, b) {\n        return `${b} == 1.0 ? -1.0/${a} : 1.0/(1.0-${a})`;\n    }\n}\nexport const defaultBCEBackD = new Dispatcher((dtype) => new BCEBackOperation(gpuConstructor, dtype));\n//# sourceMappingURL=gpu.js.map","import { CPUTensor } from '../../../../tensor/cpu/tensor';\nimport { WASMTensor } from '../../../../tensor/wasm/tensor';\nimport { bceBack } from './cpu';\nimport { defaultBCEBackD } from './gpu';\nexport class BCEBack {\n    constructor(x, y) {\n        this.x = x;\n        this.y = y;\n    }\n    backward(grad) {\n        let gradX;\n        if (grad instanceof CPUTensor) {\n            const back = bceBack(this.x.value, this.y.value);\n            gradX = grad.multiply(back);\n            back.delete();\n        }\n        else if (grad instanceof WASMTensor) {\n            const back = this.x.value\n                .wasmTensor.bce_back(this.y.value.wasmTensor);\n            gradX = new WASMTensor(grad.wasmTensor.multiply(back, 1.0), grad.dtype);\n            back.free();\n        }\n        else {\n            const back = defaultBCEBackD.calc({\n                A: this.x.value,\n                B: this.y.value,\n                outputShape: this.x.getShape(),\n            }, this.x.value.dtype);\n            gradX = grad.multiply(back);\n            back.delete();\n        }\n        const needed = this.x.backward(gradX);\n        if (!needed) {\n            gradX.delete();\n        }\n    }\n    delete() {\n        if (!this.x.isLeaf()) {\n            this.x.delete();\n        }\n        if (!this.y.isLeaf()) {\n            this.y.delete();\n        }\n    }\n}\n//# sourceMappingURL=back.js.map","import { positionWiseBinaryOp } from '../../../../ops/cpu/basic';\nexport function bceBack(x, y) {\n    return positionWiseBinaryOp(x, y, (x, y) => {\n        if (y === 1) {\n            return -1 / x;\n        }\n        else {\n            return 1 / (1 - x);\n        }\n    }, x.shape);\n}\n//# sourceMappingURL=cpu.js.map","import { BinaryOperation } from '../../../ops/gpu/binary/binaryOperation';\nimport { Dispatcher } from '../../../ops/gpu/dispatcher';\nimport { gpuConstructor } from '../../../tensor/gpu/tensor';\nexport class BCEOperation extends BinaryOperation {\n    constructor(tensorConstructor, dtype, allocator) {\n        super(tensorConstructor, dtype, allocator);\n    }\n    getOp(a, b) {\n        return `${b} == 1.0 ? -log(${a}) : -log(1.0-${a})`;\n    }\n}\nexport const defaultBCED = new Dispatcher((dtype) => new BCEOperation(gpuConstructor, dtype));\n//# sourceMappingURL=gpu.js.map","import { Variable } from '../../../autograd';\nimport { CPUTensor } from '../../../tensor/cpu/tensor';\nimport { GPUTensor } from '../../../tensor/gpu/tensor';\nimport { WASMTensor } from '../../../tensor/wasm/tensor';\nimport { sameType } from '../../../util/convert';\nimport { BCEBack } from './back/back';\nimport { bce as bceCPU } from './cpu';\nimport { defaultBCED } from './gpu';\n/**\n * Calculates the binary cross entropy loss, given probabilities x\n * and ground truth y. Returns a tensor of the same shape as\n * x. To use for a loss, you have to sum over the result:\n * ```typescript\n * const loss = bce(x,y).sum();\n * ```\n *\n * @param x Probabilities in [0,1]\n * @param y Ground truth labels of the same shape as x.\n */\nexport function bce(x, y) {\n    if (!sameType(x, y)) {\n        throw new Error('BCE can only be computed for tensors of the same type');\n    }\n    if (x instanceof CPUTensor && y instanceof CPUTensor) {\n        return bceCPU(x, y);\n    }\n    else if (x instanceof WASMTensor && y instanceof WASMTensor) {\n        return new WASMTensor(x.wasmTensor.bce(y.wasmTensor));\n    }\n    else if (x instanceof GPUTensor && y instanceof GPUTensor) {\n        return defaultBCED.calc({\n            A: x,\n            B: y,\n            outputShape: x.getShape(),\n        }, x.dtype);\n    }\n    else {\n        return new Variable(bce(x.value, y.value), {\n            noGrad: x.noGrad,\n            backEdge: x.noGrad\n                ? undefined\n                : new BCEBack(x, y),\n        });\n    }\n}\n//# sourceMappingURL=bce.js.map","import { positionWiseBinaryOp } from '../../../ops/cpu/basic';\nexport function bce(x, y) {\n    return positionWiseBinaryOp(x, y, (x, y) => {\n        if (y === 1) {\n            return -Math.log(x);\n        }\n        else {\n            return -Math.log(1 - x);\n        }\n    }, x.shape);\n}\n//# sourceMappingURL=cpu.js.map","import * as tjs from '@hoff97/tensor-js';\n\nexport async function loadModel(name: string) {\n    const res = await fetch(`models/${name}.onnx`);\n    const buffer = await res.arrayBuffer();\n\n    const model = new tjs.onnx.model.OnnxModel(buffer);\n    await model.toGPU();\n\n    return model;\n}","import React from 'react';\nimport './App.css';\nimport { loadModel } from './inference';\n\nimport * as tjs from '@hoff97/tensor-js';\nimport { Linear } from '@hoff97/tensor-js/dist/lib/model/basic';\nimport {Variable} from '@hoff97/tensor-js/dist/lib/autograd/variable';\nimport {bce} from '@hoff97/tensor-js/dist/lib/model/functional/bce/bce';\nimport {Adam} from '@hoff97/tensor-js/dist/lib/model/optimizer/adam/Adam';\nimport Button from '@material-ui/core/Button';\nimport Paper from '@material-ui/core/Paper';\nimport Typography from '@material-ui/core/Typography';\nimport LinearProgress from '@material-ui/core/LinearProgress';\nimport Accordion from '@material-ui/core/Accordion';\nimport AccordionSummary from '@material-ui/core/AccordionSummary';\nimport AccordionDetails from '@material-ui/core/AccordionDetails';\nimport Slider from '@material-ui/core/Slider';\nimport Grid from '@material-ui/core/Grid';\nimport FormControlLabel from '@material-ui/core/FormControlLabel';\nimport Checkbox from '@material-ui/core/Checkbox';\nimport FormControl from '@material-ui/core/FormControl';\nimport InputLabel from '@material-ui/core/InputLabel';\nimport MenuItem from '@material-ui/core/MenuItem';\nimport Select from '@material-ui/core/Select';\n\n//const featureDim = 64;\nconst featureDim = 1280;\n\ntype AppStage = 'getting-video' | 'warming-up' | 'start' | 'compiling' | 'trainData-1' | 'between-stages' | 'trainData-2' | 'training' | 'monitoring';\n\ninterface AppState {\n  stage: AppStage;\n  prediction?: number;\n  countDown?: number;\n  progress?: number;\n  threshold: number;\n  numIterations: number;\n  sound: boolean;\n  notification: boolean;\n  model: string;\n  numTrainSamples: number;\n  numValidationSamples: number;\n}\n\nfunction wait(t: number) {\n  return new Promise<void>((resolve, reject) => {\n    setTimeout(() => {\n      resolve();\n    }, t);\n  });\n}\n\nasync function countDown(n: number, t: number, cb: (n: number) => void) {\n  for (let i = 0; i < n; i++) {\n    cb(n-i);\n    await wait(t);\n  }\n  cb(0);\n}\n\nconst models = [\n  {\n    size: 1.0,\n    name: 'mobilenet100'\n  },\n  {\n    size: 0.5,\n    name: 'mobilenet050'\n  },\n  {\n    size: 0.35,\n    name: 'mobilenet035'\n  },\n  {\n    size: 0.25,\n    name: 'mobilenet025',\n    outputs: ['473'],\n    prune: ['474']\n  }\n]\n\nclass App extends React.Component<{}, AppState> {\n  private model?: tjs.onnx.model.OnnxModel = undefined;\n  private classifier?: tjs.model.basic.Linear;\n\n  private mean?: tjs.Tensor;\n  private varSqrt?: tjs.Tensor;\n\n  private meanMobilenet = new tjs.tensor.gpu.GPUTensor([0.485, 0.456, 0.406], [3,1,1]);\n  private stdMobilenet = new tjs.tensor.gpu.GPUTensor([0.229, 0.224, 0.225], [3,1,1]);\n\n  private data?: tjs.tensor.gpu.GPUTensor;\n\n  private trainingResultsCollected = 0;\n\n  private videoTensor?: tjs.tensor.gpu.GPUTensor;\n\n  private audio?: HTMLAudioElement;\n\n  private numIts = 0;\n\n  constructor(props: {}) {\n    super(props);\n\n    this.state = {\n      stage: 'getting-video',\n      numIterations: 3,\n      threshold: 0.5,\n      sound: true,\n      notification: false,\n      model: 'mobilenet050',\n      numTrainSamples: 64,\n      numValidationSamples: 0,\n    };\n  }\n\n  componentWillMount() {\n    this.setModel('mobilenet050');\n\n    this.audio = new Audio('alerts/beep.wav');\n  }\n\n  componentDidMount() {\n    setTimeout(() => {\n      this.getVideo().then(x => {\n        this.checkStorage();\n      });\n    }, 1000);\n  }\n\n  async checkStorage() {\n    const model = localStorage.getItem('model');\n    const mean = localStorage.getItem('mean');\n    const varSqrt = localStorage.getItem('varSqrt');\n    const classifierValues = localStorage.getItem('classifierValues');\n\n    if (model !== null && mean !== null && varSqrt !== null && classifierValues !== null) {\n      const parsedClassifier = JSON.parse(classifierValues);\n\n      this.classifier = new Linear(featureDim, 1, true);\n      this.classifier.weights = Variable.create([featureDim, 1], parsedClassifier[0], 'GPU');\n      this.classifier.bias = Variable.create([1], parsedClassifier[1], 'GPU');\n\n      this.mean = new tjs.tensor.gpu.GPUTensor(JSON.parse(mean), [featureDim]);\n      this.varSqrt = new tjs.tensor.gpu.GPUTensor(JSON.parse(varSqrt), [featureDim]);\n\n      await this.setModel(model);\n      this.setState({\n        stage: 'monitoring',\n        prediction: 0,\n      });\n\n      await wait(1000);\n\n      const reshaped = this.prepareVideo() as tjs.Tensor;\n\n      this.model?.forward([reshaped]).then(results => {\n        reshaped.delete();\n        this.process(results);\n      });\n    }\n  }\n\n  async warmup() {\n    await wait(1000);\n\n    if (this.state.stage === 'warming-up') {\n      const reshaped = this.prepareVideo() as tjs.Tensor;\n      for (let i = 0; i < 2; i++) {\n\n        const result = await this.model?.forward([reshaped]);\n        if (result !== undefined) {\n          result[0].delete();\n        }\n      }\n      reshaped.delete();\n\n      this.setState({\n        stage: 'start'\n      });\n    }\n  }\n\n  async setModel(name: string) {\n    const model = models.find(x => x.name === name);\n\n    this.setState({\n      model: name,\n    });\n\n    this.model = await loadModel(name);\n\n    if (this.model !== undefined) {\n      this.model.outputs = model?.outputs || ['472'];\n      this.model.prune(model?.prune || [\"473\"]);\n    }\n\n    this.model?.optimize();\n  }\n\n  async getVideo() {\n    const video: HTMLVideoElement = (document.querySelector(\"#videoElement\") as any);\n\n    if (navigator.mediaDevices.getUserMedia) {\n      const stream = await navigator.mediaDevices.getUserMedia({\n        video: {\n          height: 240,\n          width: 320\n        }\n      });\n      video.srcObject = stream;\n\n      this.setState({\n        stage: 'warming-up'\n      });\n\n      this.warmup();\n    }\n  }\n\n  prepareVideo() {\n    const video: HTMLVideoElement = (document.querySelector(\"#videoElement\") as any);\n\n    this.videoTensor = tjs.tensor.gpu.GPUTensor.fromData(video);\n\n    let [height, width] = this.videoTensor.shape.slice(0,2);\n\n    const sliced = this.videoTensor.slice([0], [3], [2]);\n\n    this.videoTensor.delete();\n\n    const transposed = sliced.transpose([2, 0, 1]);\n    sliced.delete();\n\n    const scaled = transposed.subtract(this.meanMobilenet);\n    transposed.delete();\n    const normalized = scaled.divide(this.stdMobilenet);\n    scaled.delete();\n\n    const reshaped = normalized.reshape([1,3,height,width], false);\n    return reshaped;\n  }\n\n  async prepareTraining() {\n    const video: HTMLVideoElement = (document.querySelector(\"#videoElement\") as any);\n    this.videoTensor = tjs.tensor.gpu.GPUTensor.fromData(video);\n    if (this.videoTensor !== undefined) {\n      this.setState({\n        ...this.state,\n        stage: 'compiling'\n      });\n\n      const numSamples = this.state.numTrainSamples + this.state.numValidationSamples;\n\n      const reshaped = this.prepareVideo() as tjs.Tensor;\n\n      this.data = new tjs.tensor.gpu.GPUTensor(new Array(numSamples*featureDim).fill(0), [numSamples, featureDim]);\n\n      console.log('Doing forward pass');\n\n      await countDown(3, 1000, (n: number) => {\n        this.setState({\n          countDown: n\n        });\n      });\n\n      this.setState({\n        stage: 'trainData-1',\n        progress: 0\n      });\n\n      this.model?.forward([reshaped]).then(result => {\n        reshaped.delete();\n        this.handleResult(result);\n      });\n    }\n  }\n\n  processResult(tensors: tjs.Tensor[]) {\n    const res1 = tensors[0];\n    console.log(res1);\n    return res1;\n  }\n\n  async handleResult(tensors: tjs.Tensor[]) {\n    const processed = this.processResult(tensors);\n\n    const oldResults = this.data;\n    //@ts-ignore\n    this.data = this.data.setValues(processed, [this.trainingResultsCollected, 0]) as tjs.tensor.gpu.GPUTensor;\n    processed.delete();\n    //@ts-ignore\n    oldResults.delete();\n\n    const numSamples = this.state.numTrainSamples + this.state.numValidationSamples;\n\n    this.trainingResultsCollected++;\n\n    console.log(`Collected ${this.trainingResultsCollected} of ${numSamples}`);\n\n    if (this.trainingResultsCollected < numSamples/2) {\n      this.setState({\n        progress: this.trainingResultsCollected/(numSamples/2)*100\n      });\n\n      await wait(50);\n\n      const reshaped = this.prepareVideo() as tjs.Tensor;\n\n      this.model?.forward([reshaped]).then(results => {\n        reshaped.delete();\n        this.handleResult(results);\n      });\n    } else if (this.trainingResultsCollected === numSamples/2) {\n      console.log('Second batch');\n\n      this.setState({\n        stage: 'between-stages'\n      });\n\n      await countDown(3, 1000, (n: number) => {\n        this.setState({\n          countDown: n\n        });\n      });\n\n      this.setState({\n        stage: 'trainData-2',\n        progress: 0\n      });\n\n      const reshaped = this.prepareVideo() as tjs.Tensor;\n\n      this.model?.forward([reshaped]).then(results => {\n        reshaped.delete();\n        this.handleResult(results);\n      });\n    } else if (this.trainingResultsCollected < numSamples) {\n      this.setState({\n        progress: (this.trainingResultsCollected/(numSamples/2) - 1)*100\n      });\n\n      await wait(50);\n      const reshaped = this.prepareVideo() as tjs.Tensor;\n\n      this.model?.forward([reshaped]).then(results => {\n        reshaped.delete();\n        this.handleResult(results);\n      });\n    } else {\n      this.setState({\n        ...this.state,\n        stage: 'training',\n        progress: 0\n      });\n\n      await wait(50);\n\n      this.trainClassifier();\n    }\n  }\n\n  async normalizeResults(data: tjs.Tensor) {\n    this.mean = data.reduceMean(0);\n    const shifted = data.subtract(this.mean);\n\n    const variance = shifted.sumSquare(0);\n    this.varSqrt = variance.sqrt();\n    variance.delete();\n\n    const normalized = shifted.divide(this.varSqrt);\n    shifted.delete();\n    return normalized;\n  }\n\n  async prepareData() {\n    const numSamples = this.state.numTrainSamples + this.state.numValidationSamples;\n\n    //@ts-ignore\n    const trainData1 = this.data.slice([0],[this.state.numTrainSamples/2],[0]);\n    //@ts-ignore\n    const trainData2 = this.data.slice([numSamples/2],[numSamples/2 + this.state.numTrainSamples/2],[0]);\n    let trainData = trainData1.concat(trainData2, 0);\n\n    //@ts-ignore\n    const valData1 = this.data.slice([this.state.numTrainSamples/2],[numSamples/2],[0]);\n    //@ts-ignore\n    const valData2 = this.data.slice([numSamples/2 + this.state.numTrainSamples/2],[numSamples],[0]);\n    let valData = valData1.concat(valData2, 0);\n\n    trainData = await this.normalizeResults(trainData);\n\n    const shiftedValData = valData.subtract(this.mean as any);\n    const normalizedValData = shiftedValData.divide(this.varSqrt as any);\n    valData.delete();\n    shiftedValData.delete();\n    valData = normalizedValData;\n\n    const trainX = new Variable(trainData, {noGrad: true});\n    const trainYs = [...new Array(this.state.numTrainSamples/2).fill(0),...new Array(this.state.numTrainSamples/2).fill(1)]\n    const trainY = Variable.create([this.state.numTrainSamples, 1], trainYs, 'GPU', {noGrad: true});\n\n    const valX = new Variable(valData, {noGrad: true});\n    const valYs = [...new Array(this.state.numValidationSamples/2).fill(0),...new Array(this.state.numValidationSamples/2).fill(1)]\n\n    return {trainX, trainYs, trainY, valX, valYs};\n  }\n\n  async trainClassifier() {\n    this.classifier = new Linear(featureDim, 1, true);\n    await this.classifier.toGPU();\n\n    const {trainX, trainYs, trainY, valX, valYs} = await this.prepareData();\n\n    const optimizer = new Adam(this.classifier);\n\n    const numIts = 200;\n\n    for (let i = 0; i < numIts; i++) {\n      this.setState({\n        ...this.state,\n        progress: i/numIts*100\n      });\n\n      await wait(10);\n\n      const pred = (await this.classifier.forward([trainX]))[0];\n      const sigmoid = pred.sigmoid();\n\n      const loss = bce(sigmoid, trainY).reduceMean() as Variable;\n\n      if (i % 50 === 0) {\n        console.log(i, (await loss.getValues())[0]);\n\n        const predTrain: number[] = Array.from(await sigmoid.getValues());\n        let correctTrain = this.getCorrect(predTrain, trainYs);\n        console.log(`${correctTrain} of ${predTrain.length} predicted correctly`);\n\n        const predVal = (await this.classifier.forward([valX]))[0];\n        const sigmoidVal = predVal.sigmoid();\n        const predValArr: number[] = Array.from(await sigmoidVal.getValues());\n        let correctVal = this.getCorrect(predValArr, valYs);\n        console.log(`${correctVal} of ${predValArr.length} predicted correctly`);\n        sigmoidVal.delete();\n      }\n\n      loss.backward();\n      optimizer.step();\n\n      loss.delete();\n      optimizer.zeroGrads();\n    }\n\n    this.setState({\n      ...this.state,\n      stage: 'monitoring',\n      prediction: 0\n    });\n\n    await wait(1000);\n\n    this.saveModelSettings();\n\n    const reshaped = this.prepareVideo() as tjs.Tensor;\n\n    this.model?.forward([reshaped]).then(results => {\n      reshaped.delete();\n      this.process(results);\n    });\n  }\n\n  async saveModelSettings() {\n    const params = this.classifier?.getParameters() as Variable[];\n    const paramValues = [];\n    for (let param of params) {\n      paramValues.push(Array.from(await param.getValues()));\n    }\n\n    localStorage.setItem('classifierValues', JSON.stringify(paramValues));\n\n    const meanValues = Array.from(await this.mean?.getValues() as any);\n    localStorage.setItem('mean', JSON.stringify(meanValues));\n\n    const stdValues = Array.from(await this.varSqrt?.getValues() as any);\n    localStorage.setItem('varSqrt', JSON.stringify(stdValues));\n\n    localStorage.setItem('model', this.state.model);\n  }\n\n  toggleNotification(value: boolean) {\n    if (value) {\n      if (Notification.permission !== \"denied\" && Notification.permission !== \"granted\") {\n        Notification.requestPermission().then((permission) => {\n          if (permission === \"granted\") {\n            new Notification(\"I will notify you like this.\");\n            this.setState({\n              notification: true\n            });\n          }\n        });\n      } else if (Notification.permission === \"granted\") {\n        this.setState({\n          notification: true\n        });\n      }\n    } else {\n      this.setState({\n        notification: false\n      });\n    }\n  }\n\n  async process(tensors: tjs.Tensor[]) {\n    const processed = this.processResult(tensors);\n\n    const shifted = processed.subtract(this.mean as any);\n    processed.delete();\n    const normalized = new Variable(shifted.divide(this.varSqrt as any), {noGrad: true});\n    shifted.delete();\n\n    //@ts-ignore\n    const logits = (await this.classifier.forward([normalized]))[0];\n    normalized.delete();\n    const v = await logits.getValues();\n    logits.delete();\n\n    const sigmoid = 1/(1+Math.exp(-v[0]));\n\n    this.setState({\n      ...this.state,\n      prediction: sigmoid\n    });\n\n    if (sigmoid > this.state.threshold) {\n      this.numIts++;\n      if (this.numIts === this.state.numIterations) {\n        if (this.state.sound) {\n          this.audio?.play();\n        }\n        if (this.state.notification) {\n          new Notification('Stop touching your face');\n        }\n      }\n    } else {\n      this.numIts = 0;\n    }\n\n    if (this.state.stage === 'monitoring') {\n      await wait(1000);\n\n      const reshaped = this.prepareVideo() as tjs.Tensor;\n\n      this.model?.forward([reshaped]).then(results => {\n        reshaped.delete();\n        this.process(results);\n      });\n    }\n  }\n\n  getCorrect(predVals: number[], ys: number[]) {\n    let correct = 0;\n    for (let i = 0; i <predVals.length; i++) {\n      if ((ys[i] === 1 && predVals[i] > 0.5) || (ys[i] === 0 && predVals[i] < 0.5)) {\n        correct++;\n      }\n    }\n    return correct;\n  }\n\n  render() {\n    return (\n      <div className=\"App\">\n        <Paper elevation={3} style={{padding: '10px'}}>\n         {this.renderSettings()}\n\n          <Typography variant=\"h3\" component=\"h2\">\n            Dont touch your face\n          </Typography>\n          This app will teach you not to touch your face.<br/>\n\n          <div className=\"mainContent\">\n            <video autoPlay id=\"videoElement\"/><br/>\n            {this.renderState()}\n          </div>\n        </Paper>\n      </div>\n    );\n  }\n\n  renderSettings() {\n    return (\n      <div className=\"settings\">\n        {this.renderGeneralSettings()}\n        {this.renderModelSettings()}\n      </div>\n    );\n  }\n\n  renderGeneralSettings() {\n    return (\n      <Accordion>\n        <AccordionSummary\n          aria-controls=\"panel1a-content\"\n          id=\"panel1a-header\"\n        >\n          <Typography variant=\"h5\">Settings</Typography>\n        </AccordionSummary>\n        <AccordionDetails>\n          <Grid container spacing={1}>\n            <Grid item xs={12}>\n              <Typography id=\"slider-threshold-label\" gutterBottom>\n                Threshold\n              </Typography><div></div>\n            </Grid>\n            <Grid item xs={12}>\n              <Slider\n                aria-labelledby=\"slider-threshold-label\"\n                value={this.state.threshold}\n                min={0}\n                max={1}\n                step={0.1}\n                onChange={(ev, v) => this.setState({threshold: v as number})}\n                valueLabelDisplay=\"auto\"\n              />\n            </Grid>\n            <Grid item xs={12}>\n              <Typography id=\"slider-threshold-label\" gutterBottom>\n                Seconds until alert\n              </Typography><div></div>\n            </Grid>\n            <Grid item xs={12}>\n              <Slider\n                aria-labelledby=\"slider-threshold-label\"\n                value={this.state.numIterations}\n                min={1}\n                max={10}\n                step={1}\n                onChange={(ev, v) => this.setState({numIterations: v as number})}\n                valueLabelDisplay=\"auto\"\n              />\n            </Grid>\n            <Grid item xs={6}>\n              <FormControlLabel\n                control={\n                  <Checkbox\n                    checked={this.state.sound}\n                    onChange={(ev) => this.setState({sound: ev.target.checked})}\n                    name=\"checkedB\"\n                    color=\"primary\"\n                  />\n                }\n                label=\"Play sound\"\n              />\n            </Grid>\n            <Grid item xs={6}>\n              <FormControlLabel\n                control={\n                  <Checkbox\n                    checked={this.state.notification}\n                    onChange={(ev) => this.toggleNotification(ev.target.checked)}\n                    name=\"checkedB\"\n                    color=\"primary\"\n                  />\n                }\n                label=\"Show notification\"\n              />\n            </Grid>\n          </Grid>\n        </AccordionDetails>\n      </Accordion>\n    );\n  }\n\n  renderModelSettings() {\n    const marks = [\n      {\n        value: 16,\n        label: '16',\n      },\n      {\n        value: 32,\n        label: '32',\n      },\n      {\n        value: 64,\n        label: '64',\n      },\n      {\n        value: 128,\n        label: '128',\n      },\n      {\n        value: 256,\n        label: '256',\n      },\n    ];\n\n    return (\n      <Accordion>\n        <AccordionSummary\n          aria-controls=\"panel1a-content\"\n          id=\"panel1a-header\"\n        >\n          <Typography variant=\"h5\">Model</Typography>\n        </AccordionSummary>\n        <AccordionDetails>\n          <Grid container spacing={1}>\n            <Grid item xs={12}>\n              <Typography id=\"slider-threshold-label\" gutterBottom>\n                Model Size\n              </Typography><div></div>\n            </Grid>\n            <Grid item xs={12}>\n              <FormControl>\n                <InputLabel id=\"label-model-size\">Age</InputLabel>\n                <Select\n                  labelId=\"label-model-size\"\n                  id=\"model-size\"\n                  value={this.state.model}\n                  onChange={(event) => this.setModel(event.target.value as any)}\n                >\n                  {models.map(x => (\n                    <MenuItem value={x.name}>{x.size}</MenuItem>\n                  ))}\n                </Select>\n              </FormControl>\n            </Grid>\n            <Grid item xs={12}>\n              <Typography id=\"slider-train-samples-label\" gutterBottom>\n                Number of training samples\n              </Typography><div></div>\n            </Grid>\n            <Grid item xs={12}>\n              <Slider\n                aria-labelledby=\"slider-train-samples-label\"\n                value={this.state.numTrainSamples}\n                step={null}\n                valueLabelDisplay=\"auto\"\n                marks={marks}\n                min={16}\n                max={256}\n                onChange={(ev, v) => this.setState({numTrainSamples: v as number})}\n              />\n            </Grid>\n          </Grid>\n        </AccordionDetails>\n      </Accordion>\n    );\n  }\n\n  renderState() {\n    if (this.state.stage === 'getting-video') {\n      return (<> Please activate your webcam </>);\n    } else if (this.state.stage === 'warming-up') {\n      return (<> Warming up model. </>);\n    }\n    else if (this.state.stage === 'start') {\n      return (<Button variant=\"contained\" color=\"primary\" onClick={() => this.prepareTraining()}>Start</Button>);\n    } else if (this.state.stage === 'compiling') {\n      return (<> Dont touch your face until the bar is full. Starting in {this.state.countDown} seconds. </>);\n    } else if (this.state.stage === 'trainData-1' || this.state.stage === 'trainData-2') {\n      return (<LinearProgress variant=\"determinate\" value={this.state.progress} className=\"progress\"/>)\n    } else if (this.state.stage === 'between-stages') {\n      return (<> Next, touch your face until the bar is full. Starting in {this.state.countDown} seconds. </>);\n    } else if (this.state.stage === 'training') {\n      return (\n        <div> Done. Training face touch recognition model\n          <LinearProgress variant=\"indeterminate\" className=\"progress\"/>\n        </div>\n      );\n    } else if (this.state.stage === 'monitoring') {\n      const pred = (this.state.prediction as number);\n\n      const maxColor = 200;\n      const color = `rgb(${Math.round(pred*maxColor)},${maxColor-Math.round(pred*maxColor)},0)`;\n\n      return (\n        <div> Done. I will alert you when i think you are touching your face\n          <div style={{\n              width: 320,\n              height: 20,\n              backgroundColor: '#AAA',\n              borderRadius: 5,\n              overflow: 'hidden',\n              margin: 'auto'\n          }}>\n            <div style={{\n              width: pred*320,\n              height: 20,\n              backgroundColor: color\n            }}></div>\n          </div>\n        </div>\n      );\n    }\n  }\n}\n\nexport default App;\n","import { ReportHandler } from 'web-vitals';\n\nconst reportWebVitals = (onPerfEntry?: ReportHandler) => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}